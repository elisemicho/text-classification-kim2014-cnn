{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#from tensorflow.python.framework import ops\n",
    "#ops.reset_default_graph()\n",
    "#sess = tf.Session()\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_folder = \"../data/vardial2018-sample\"\n",
    "#data_files = [\"EGY\", \"GLF\", \"LAV\", \"MSA\", \"NOR\"]\n",
    "train_file = \"train.words\"\n",
    "dev_file = \"dev.words\"\n",
    "batch_size = 10\n",
    "\n",
    "# Model Hyperparameters\n",
    "# Embeddings\n",
    "vocab_size = 50000\n",
    "embed_size = 128\n",
    "\n",
    "# Convolutional layers\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 3\n",
    "dropout_keep_prob = 0.5\n",
    "l2_reg_lambda = 0.0\n",
    "l2_loss = 0.0\n",
    "\n",
    "# Training\n",
    "lr = 0.5\n",
    "training=False\n",
    "n_epochs = 3\n",
    "skip_step = 1\n",
    "global_step = tf.get_variable('global_step', initializer=tf.constant(0), trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loading data...\n",
      "Padding sentences...\n",
      "Building word vocabulary...\n",
      "Converting to ids...\n",
      "Train/Dev split: 10/10\n",
      "train shape: (10, 48)\n",
      "dev shape: (10, 48)\n",
      "vocab_size 131\n",
      "sentence max words 48\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('data'):\n",
    "    # Read in train and dev data (sentences as word ids and padded, labels as one-hot vectors)\n",
    "    #train_data, test_data = utils.get_vardial_dataset_from_many_files(batch_size, data_folder, data_files)\n",
    "    #x_train, x_dev, y_train, y_dev = utils.get_vardial_dataset_from_many_files(data_folder, data_files)\n",
    "    x_train, x_dev, y_train, y_dev = utils.get_vardial_dataset_from_train_dev_file(data_folder, train_file, dev_file)\n",
    "\n",
    "    #Create datasets\n",
    "    train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    train_data = train_data.shuffle(10000) # to shuffle your data\n",
    "    train_data = train_data.batch(batch_size)\n",
    "\n",
    "    test_data = tf.data.Dataset.from_tensor_slices((x_dev, y_dev))\n",
    "    test_data = test_data.batch(batch_size)\n",
    "\n",
    "    # Create iterators\n",
    "    iterator = tf.data.Iterator.from_structure(train_data.output_types, \n",
    "                                           train_data.output_shapes)\n",
    "    sentence, label = iterator.get_next()\n",
    "    # shape = [batch_size, sentence_length],[batch_size, num_classes]\n",
    "\n",
    "    sentence_length = sentence.shape[1].value\n",
    "    num_classes = label.shape[1].value\n",
    "    n_test = len(y_dev)\n",
    "\n",
    "    train_init = iterator.make_initializer(train_data)  # initializer for train_data\n",
    "    test_init = iterator.make_initializer(test_data)    # initializer for test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Variable.eval of <tf.Variable 'char_embed/embed_matrix:0' shape=(50000, 128) dtype=float32_ref>>\n",
      "<bound method Tensor.eval of <tf.Tensor 'char_embed/embedding:0' shape=(?, 48, 128) dtype=float32>>\n",
      "<bound method Tensor.eval of <tf.Tensor 'char_embed/ExpandDims:0' shape=(?, 48, 128, 1) dtype=float32>>\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Step 2 + 3: define weights and embedding lookup\n",
    "\"\"\"\n",
    "with tf.variable_scope('char_embed'):\n",
    "    embed_matrix = tf.get_variable('embed_matrix', \n",
    "                                        shape=[vocab_size, embed_size],\n",
    "                                        initializer=tf.random_uniform_initializer())\n",
    "    embedded_chars = tf.nn.embedding_lookup(embed_matrix, sentence, name='embedding')\n",
    "    embedded_chars_expanded = tf.expand_dims(embedded_chars, -1)\n",
    "    \"\"\"sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    sess.run(train_init)\"\"\"\n",
    "    print(embed_matrix.eval)\n",
    "    print(embedded_chars.eval)\n",
    "    print(embedded_chars_expanded.eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Tensor.eval of <tf.Tensor 'conv-maxpool-3/conv/Relu:0' shape=(?, 48, 128, 3) dtype=float32>>\n",
      "<bound method Tensor.eval of <tf.Tensor 'conv-maxpool-3/pool/MaxPool:0' shape=(?, 1, 1, 3) dtype=float32>>\n",
      "<bound method Tensor.eval of <tf.Tensor 'conv-maxpool-4/conv/Relu:0' shape=(?, 48, 128, 3) dtype=float32>>\n",
      "<bound method Tensor.eval of <tf.Tensor 'conv-maxpool-4/pool/MaxPool:0' shape=(?, 1, 1, 3) dtype=float32>>\n",
      "<bound method Tensor.eval of <tf.Tensor 'conv-maxpool-5/conv/Relu:0' shape=(?, 48, 128, 3) dtype=float32>>\n",
      "<bound method Tensor.eval of <tf.Tensor 'conv-maxpool-5/pool/MaxPool:0' shape=(?, 1, 1, 3) dtype=float32>>\n"
     ]
    }
   ],
   "source": [
    "# Create a convolution + maxpool layer for each filter size\n",
    "pooled_outputs = []\n",
    "for i, filter_size in enumerate(filter_sizes):\n",
    "    with tf.variable_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "\n",
    "        # Convolution layer\n",
    "        conv = tf.layers.conv2d(inputs=embedded_chars_expanded,\n",
    "                          filters=num_filters,\n",
    "                          kernel_size=[filter_size, embed_size],\n",
    "                          padding='SAME',\n",
    "                          activation=tf.nn.relu,\n",
    "                          name='conv')\n",
    "        print(conv.eval)\n",
    "        # shape = [batch_size, sequence_length, embed_size, num_filters]\n",
    "        \n",
    "        # Maxpooling over the outputs\n",
    "        pooled = tf.layers.max_pooling2d(inputs=conv, \n",
    "                                        pool_size=(sentence.shape[1].value - filter_size + 1, 1), \n",
    "                                        strides=[filter_size, embed_size],\n",
    "                                        name='pool')\n",
    "        print(pooled.eval)\n",
    "        pooled_outputs.append(pooled)\n",
    "         # shape =  [batch_size, 1, 1, num_filters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "<bound method Tensor.eval of <tf.Tensor 'concat:0' shape=(?, 1, 1, 9) dtype=float32>>\n",
      "<bound method Tensor.eval of <tf.Tensor 'Reshape:0' shape=(?, 9) dtype=float32>>\n"
     ]
    }
   ],
   "source": [
    "num_filters_total = num_filters * len(filter_sizes)\n",
    "print(num_filters_total)\n",
    "h_pool = tf.concat(pooled_outputs, 3)\n",
    "h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])\n",
    "print(h_pool.eval)\n",
    "print(h_pool_flat.eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Tensor.eval of <tf.Tensor 'dropout/dropout/mul:0' shape=(?, 9) dtype=float32>>\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"dropout\"):\n",
    "   h_drop = tf.nn.dropout(h_pool_flat, dropout_keep_prob)\n",
    "print(h_drop.eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Tensor.eval of <tf.Tensor 'fully-connected/dense/BiasAdd:0' shape=(?, 5) dtype=float32>>\n",
      "<bound method Tensor.eval of <tf.Tensor 'fully-connected/predictions:0' shape=(?,) dtype=int64>>\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"fully-connected\"):\n",
    "    scores = tf.layers.dense(h_drop, num_classes,\n",
    "                kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    predictions = tf.argmax(scores, 1, name=\"predictions\")\n",
    "print(scores.eval)\n",
    "print(predictions.eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Tensor.eval of <tf.Tensor 'loss/Reshape_2:0' shape=(?,) dtype=float32>>\n",
      "<bound method Tensor.eval of <tf.Tensor 'loss/add:0' shape=() dtype=float32>>\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('loss'):\n",
    "    losses = tf.nn.softmax_cross_entropy_with_logits(logits=scores, labels=label)\n",
    "    loss = tf.reduce_mean(losses) + l2_reg_lambda * 0.1\n",
    "print(losses.eval)\n",
    "print(loss.eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Tensor.eval of <tf.Tensor 'accuracy/Equal:0' shape=(?,) dtype=bool>>\n",
      "<bound method Tensor.eval of <tf.Tensor 'accuracy/accuracy:0' shape=() dtype=float32>>\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_predictions = tf.equal(predictions, tf.argmax(label, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "print(correct_predictions.eval)\n",
    "print(accuracy.eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('optimizer'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(lr).minimize(loss, \n",
    "                                                      global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('summaries'):\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    tf.summary.histogram('histogram_loss', loss)\n",
    "    summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(sess, saver, init, writer, epoch, step):\n",
    "    start_time = time.time()\n",
    "    sess.run(init) \n",
    "    training = True\n",
    "    total_loss = 0\n",
    "    n_batches = 0\n",
    "    try:\n",
    "        while True:\n",
    "            _, l, acc, summaries = sess.run([optimizer, loss, accuracy, summary_op])\n",
    "            writer.add_summary(summaries, global_step=step)\n",
    "            if (step + 1) % skip_step == 0:\n",
    "                print('step {0}, loss {1}'.format(step, l))\n",
    "            step += 1\n",
    "            total_loss += l\n",
    "            n_batches += 1\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        pass\n",
    "    saver.save(sess, 'checkpoints/char-cnn', step)\n",
    "    print('epoch {0}, loss {1}, took {2}'.format(epoch, total_loss/n_batches, time.time() - start_time))\n",
    "    return step\n",
    "\n",
    "def eval_once(sess, init, writer, epoch, step):\n",
    "    start_time = time.time()\n",
    "    sess.run(init)\n",
    "    training = False\n",
    "    total_correct_preds = 0\n",
    "    try:\n",
    "        while True:\n",
    "            accuracy_batch, summaries = sess.run([accuracy, summary_op])\n",
    "            writer.add_summary(summaries, global_step=step)\n",
    "            total_correct_preds += accuracy_batch\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        pass\n",
    "\n",
    "    print('epoch {0}, accuracy {1}, took {2}'.format(epoch, total_correct_preds, time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "step 0, loss 1.9731290340423584\n",
      "epoch 0, loss 1.9731290340423584, took 0.46268558502197266\n",
      "epoch 0, accuracy 0.20000000298023224, took 0.12839317321777344\n",
      "epoch 1\n",
      "step 1, loss 1.7951428890228271\n",
      "epoch 1, loss 1.7951428890228271, took 0.4263486862182617\n",
      "epoch 1, accuracy 0.4000000059604645, took 0.1185295581817627\n",
      "epoch 2\n",
      "step 2, loss 1.598291277885437\n",
      "epoch 2, loss 1.598291277885437, took 0.42763376235961914\n",
      "epoch 2, accuracy 0.6000000238418579, took 0.1194753646850586\n"
     ]
    }
   ],
   "source": [
    "utils.safe_mkdir('checkpoints')\n",
    "utils.safe_mkdir('checkpoints/char-cnn')\n",
    "writer = tf.summary.FileWriter('./graphs/char-cnn', tf.get_default_graph())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/char-cnn/checkpoint'))\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "    step = global_step.eval()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print('epoch {0}'.format(epoch))\n",
    "                step = train_one_epoch(sess, saver, train_init, writer, epoch, step)\n",
    "        eval_once(sess, test_init, writer, epoch, step)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
