{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import os\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data_and_labels_from_many_files(data_folder, data_files):\n",
    "    \"\"\"\n",
    "    Loads sentences from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    # Load data from files\n",
    "    print(\"Loading data...\")\n",
    "    x_text = []\n",
    "    y = []\n",
    "\n",
    "    for i, data_file in enumerate(data_files):\n",
    "\n",
    "        sentences = list(open(data_folder + \"/\" + data_file, \"r\").readlines())\n",
    "        sentences = [s.strip() for s in sentences]\n",
    "        # Split by words\n",
    "        # sentences = [clean_str(s) for s in sentences]\n",
    "        sentences = [s.split() for s in sentences]\n",
    "        x_text += sentences\n",
    "        # Labels as numbers\n",
    "        labels = [i for s in sentences]\n",
    "        y += labels\n",
    "\n",
    "    # Generate one-hot labels\n",
    "    y = to_categorical(y, num_classes=len(data_files))\n",
    "\n",
    "    return x_text, y\n",
    "\n",
    "def load_data_and_labels_from_one_file(data_folder, data_file):\n",
    "    \"\"\"\n",
    "    Loads sentences from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    # Load data from files\n",
    "    print(\"Loading data...\")\n",
    "    labels = [\"EGY\", \"GLF\", \"LAV\", \"MSA\", \"NOR\"]\n",
    "    x_text = []\n",
    "    y = []\n",
    "\n",
    "    with open(data_folder + \"/\" + data_file, \"r\") as f_in:\n",
    "        for line in f_in:\n",
    "            sentence, label = line.split(\"\\t\")\n",
    "            # Split by words\n",
    "            sentence = sentence.strip().split()\n",
    "            x_text.append(sentence)\n",
    "            # Labels as numbers\n",
    "            y.append(labels.index(label.strip(\"\\n\")))\n",
    "\n",
    "    # Generate one-hot labels\n",
    "    y = to_categorical(y, num_classes=len(labels))\n",
    "\n",
    "    return x_text, y\n",
    "\n",
    "def pad_sentences(sentences, padding_word=\"\"):\n",
    "    \"\"\"\n",
    "    Pads all sentences to be the length of the longest sentence.\n",
    "    Returns padded sentences.\n",
    "    \"\"\"\n",
    "    print(\"Padding sentences...\")\n",
    "    sequence_length = max(len(x) for x in sentences)\n",
    "    padded_sentences = []\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        num_padding = sequence_length - len(sentence)\n",
    "        new_sentence = sentence + [padding_word] * num_padding\n",
    "        padded_sentences.append(new_sentence)\n",
    "        \n",
    "    return padded_sentences\n",
    "\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary mapping from token to index based on the sentences.\n",
    "    Returns vocabulary mapping and inverse vocabulary mapping.\n",
    "    \"\"\"\n",
    "    # Build vocabulary\n",
    "    print(\"Building word vocabulary...\")\n",
    "    word_counts = Counter(itertools.chain(*sentences))\n",
    "    \n",
    "    # Mapping from index to word\n",
    "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "    \n",
    "    # Mapping from word to index\n",
    "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "    \n",
    "    return vocabulary, vocabulary_inv\n",
    "\n",
    "\n",
    "def build_input_data(sentences, labels, vocabulary):\n",
    "    \"\"\"\n",
    "    Maps sentences and labels to vectors based on a vocabulary.\n",
    "    \"\"\"\n",
    "    print(\"Converting to ids...\")\n",
    "    x = np.array([\n",
    "            [vocabulary[word] for word in sentence]\n",
    "            for sentence in sentences])\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "50\n",
      "['tthdm', 'AlmsAjd', 'fy', \"synA'\", 'wAl>bAt$y', 'sxryp', 'mn', 'Alhjrp', 'Alnbwyp', 'fy', 'Aljrydp', 'Alrsmyp', 'lmA', 'mAdty', 'Altrbyp', 'Al<slAmyp', 'mn', 'AlmdArs']\n"
     ]
    }
   ],
   "source": [
    "data_folder = \"../data/vardial2017-sample\"\n",
    "data_files = [\"EGY\", \"GLF\", \"LAV\", \"MSA\", \"NOR\"]\n",
    "sentences, labels = load_data_and_labels_from_many_files(data_folder, data_files)\n",
    "print(len(sentences))\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loading data...\n",
      "20\n",
      "['AlkAmyrwn', 'AlkAmlp']\n",
      "[[0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "data_folder = \"../data/vardial2018-sample\"\n",
    "train_file = \"train.words\"\n",
    "dev_file = \"dev.words\"\n",
    "# Step 1: Read in data\n",
    "sentences_train, labels_train = load_data_and_labels_from_one_file(data_folder, train_file)\n",
    "sentences_dev, labels_dev = load_data_and_labels_from_one_file(data_folder, train_file)\n",
    "sentences = sentences_train + sentences_dev\n",
    "labels = np.concatenate((labels_train,labels_dev))\n",
    "print(len(sentences))\n",
    "print(sentences[0])\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding sentences...\n",
      "Building word vocabulary...\n",
      "Converting to ids...\n",
      "10\n",
      "Train/Dev split: 10/10\n",
      "train shape: (10, 48)\n",
      "dev shape: (10, 48)\n",
      "vocab_size 131\n",
      "sentence max words 48\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Pad sentences and convert to ids\n",
    "sentences_padded = pad_sentences(sentences)\n",
    "vocabulary, vocabulary_inv = build_vocab(sentences_padded)\n",
    "x, y = build_input_data(sentences_padded, labels, vocabulary)\n",
    "\n",
    "# Step 3: Split train/test set\n",
    "dev_sample_index = len(sentences_train)\n",
    "print(dev_sample_index)\n",
    "x_train, x_dev = x[:dev_sample_index], x[dev_sample_index:]\n",
    "y_train, y_dev = y[:dev_sample_index], y[dev_sample_index:]\n",
    "\n",
    "vocab_size = len(vocabulary)\n",
    "sentence_size = x_train.shape[1]\n",
    "\n",
    "print('Train/Dev split: %d/%d' % (len(y_train), len(y_dev)))\n",
    "print('train shape:', x_train.shape)\n",
    "print('dev shape:', x_dev.shape)\n",
    "print('vocab_size', vocab_size)\n",
    "print('sentence max words', sentence_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorShape([Dimension(None), Dimension(48)]), TensorShape([Dimension(None), Dimension(5)]))\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Create datasets and iterator\n",
    "batch_size = 5\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_data = train_data.shuffle(10000) # to shuffle your data\n",
    "train_data = train_data.batch(batch_size)\n",
    "test_data = tf.data.Dataset.from_tensor_slices((x_dev, y_dev))\n",
    "test_data = test_data.batch(batch_size)\n",
    "print(test_data.output_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "iterator = tf.data.Iterator.from_structure(train_data.output_types, \n",
    "                                                   train_data.output_shapes)\n",
    "sentence, label = iterator.get_next()\n",
    "# shape = [batch_size, sentence_length],[batch_size, num_classes]\n",
    "\n",
    "sentence_length = sentence.shape[1].value\n",
    "num_classes = label.shape[1].value\n",
    "print(sentence_length)\n",
    "print(num_classes)\n",
    "train_init = iterator.make_initializer(train_data)  # initializer for train_data\n",
    "test_init = iterator.make_initializer(test_data)    # initializer for train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
