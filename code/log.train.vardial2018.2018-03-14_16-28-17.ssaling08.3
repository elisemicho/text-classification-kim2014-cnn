############# train @ Wed Mar 14 16:28:17 CET 2018 GPUS=3  HOST=ssaling08 PWD=/home/michon/projects/VarDial2018/work_dir/code
Loading data...
Loading data...
Padding sentences...
Building word vocabulary...
Converting to ids...
Train/Dev split: 14591/14591
train shape: (14591, 1220)
dev shape: (14591, 1220)
vocab_size 43150
sentence max words 1220
Epoch 1
step 629, loss 1.332897424697876
step 639, loss 1.8268102407455444
step 649, loss 2.082451820373535
step 659, loss 1.5698421001434326
step 669, loss 1.7724335193634033
step 679, loss 1.632421851158142
step 689, loss 1.5674129724502563
step 699, loss 1.6450132131576538
step 709, loss 1.6206616163253784
step 719, loss 1.5950820446014404
step 729, loss 1.5911946296691895
step 739, loss 3.6888556480407715
step 749, loss 1.6256603002548218
step 759, loss 1.5773036479949951
step 769, loss 1.5710455179214478
step 779, loss 1.6140904426574707
step 789, loss 1.6394104957580566
step 799, loss 1.625849723815918
step 809, loss 1.6306051015853882
step 819, loss 1.6270453929901123
step 829, loss 1.5957965850830078
step 839, loss 1.5193997621536255
Average loss 1.7866724374001486 for epoch 1, took 392.8893048763275 sec
Accuracy 0.0034331951202796244 for epoch 1, took 81.32971858978271 sec
Epoch 2
step 849, loss 1.5804862976074219
step 859, loss 1.374981164932251
step 869, loss 1.3783659934997559
step 879, loss 1.4741230010986328
step 889, loss 1.753441333770752
step 899, loss 1.6859440803527832
step 909, loss 1.6298015117645264
step 919, loss 1.6075941324234009
step 929, loss 1.5798567533493042
step 939, loss 1.6233201026916504
step 949, loss 1.6237330436706543
step 959, loss 1.5630764961242676
step 969, loss 1.556933045387268
step 979, loss 1.6376763582229614
step 989, loss 1.638890027999878
step 999, loss 1.634171962738037
step 1009, loss 1.638925313949585
step 1019, loss 1.6319226026535034
step 1029, loss 1.5975242853164673
step 1039, loss 1.5947775840759277
step 1049, loss 1.5866539478302002
step 1059, loss 1.623805284500122
step 1069, loss 1.6058235168457031
Average loss 1.6410076194687893 for epoch 2, took 387.7613642215729 sec
Accuracy 0.0034021400178192037 for epoch 2, took 80.41819453239441 sec
Epoch 3
step 1079, loss 1.384237289428711
step 1089, loss 1.429964542388916
step 1099, loss 1.3657872676849365
step 1109, loss 1.772630214691162
step 1119, loss 1.5816845893859863
step 1129, loss 1.5911997556686401
step 1139, loss 1.6019065380096436
step 1149, loss 1.6327786445617676
step 1159, loss 1.5832366943359375
step 1169, loss 1.6130542755126953
step 1179, loss 1.588075041770935
step 1189, loss 1.617432713508606
step 1199, loss 1.6566641330718994
step 1209, loss 1.6734856367111206
step 1219, loss 1.631962537765503
step 1229, loss 1.6103153228759766
step 1239, loss 1.640376091003418
step 1249, loss 1.6219797134399414
step 1259, loss 1.5317306518554688
step 1269, loss 1.6647698879241943
step 1279, loss 1.5118670463562012
step 1289, loss 1.5862555503845215
step 1299, loss 1.565000295639038
Average loss 1.6137994130452473 for epoch 3, took 387.81851172447205 sec
Accuracy 0.0034331951202796244 for epoch 3, took 80.3000431060791 sec
Epoch 4
step 1309, loss 1.3801437616348267
step 1319, loss 1.367250680923462
step 1329, loss 1.8683888912200928
step 1339, loss 1.8449673652648926
step 1349, loss 1.614264965057373
step 1359, loss 1.6309022903442383
step 1369, loss 1.7205572128295898
step 1379, loss 1.6001017093658447
step 1389, loss 1.5876423120498657
step 1399, loss 1.626939296722412
step 1409, loss 1.601995587348938
step 1419, loss 1.5795329809188843
step 1429, loss 1.5357104539871216
step 1439, loss 1.6629533767700195
step 1449, loss 1.5893381834030151
step 1459, loss 1.6015477180480957
step 1469, loss 1.6101312637329102
step 1479, loss 1.5977450609207153
step 1489, loss 1.613476276397705
step 1499, loss 1.5740396976470947
step 1509, loss 1.599141001701355
step 1519, loss 1.6361132860183716
step 1529, loss 1.567478895187378
Average loss 1.610856417501182 for epoch 4, took 387.8017132282257 sec
Accuracy 0.0034021400178192037 for epoch 4, took 80.1675214767456 sec
Epoch 5
step 1539, loss 1.3709821701049805
step 1549, loss 1.3444429636001587
step 1559, loss 1.5930960178375244
step 1569, loss 1.7410162687301636
step 1579, loss 1.639792799949646
step 1589, loss 1.6469480991363525
step 1599, loss 1.8423223495483398
step 1609, loss 1.7002842426300049
step 1619, loss 1.653770923614502
step 1629, loss 1.5471644401550293
step 1639, loss 1.5761840343475342
step 1649, loss 1.5988614559173584
step 1659, loss 1.58280348777771
step 1669, loss 1.6084048748016357
step 1679, loss 1.633345603942871
step 1689, loss 1.6367475986480713
step 1699, loss 1.606905460357666
step 1709, loss 1.609476089477539
step 1719, loss 1.6226228475570679
step 1729, loss 1.578116774559021
step 1739, loss 1.6724262237548828
step 1749, loss 1.6386425495147705
step 1759, loss 1.6300894021987915
Average loss 1.6098733335210567 for epoch 5, took 386.4064953327179 sec
Accuracy 0.0034021400178192037 for epoch 5, took 80.31332564353943 sec
Epoch 6
step 1769, loss 1.3922748565673828
step 1779, loss 1.4245320558547974
step 1789, loss 1.5651074647903442
step 1799, loss 1.5848567485809326
step 1809, loss 1.6567590236663818
step 1819, loss 1.6369130611419678
step 1829, loss 1.6875479221343994
step 1839, loss 1.58719801902771
step 1849, loss 1.593127727508545
step 1859, loss 1.6232960224151611
step 1869, loss 1.6505019664764404
step 1879, loss 1.6418834924697876
step 1889, loss 1.553466796875
step 1899, loss 1.6443228721618652
step 1909, loss 1.61915123462677
step 1919, loss 1.6133396625518799
step 1929, loss 1.5920052528381348
step 1939, loss 1.609596610069275
step 1949, loss 1.6208932399749756
step 1959, loss 1.5346918106079102
step 1969, loss 1.6075146198272705
step 1979, loss 1.5864698886871338
Average loss 1.613008621491884 for epoch 6, took 386.24330401420593 sec
Accuracy 0.003337888081694195 for epoch 6, took 80.24718642234802 sec
Epoch 7
step 1989, loss 1.5878067016601562
step 1999, loss 1.3806027173995972
step 2009, loss 1.3797094821929932
step 2019, loss 1.9863646030426025
step 2029, loss 1.627807855606079
step 2039, loss 1.657463550567627
step 2049, loss 1.590599536895752
step 2059, loss 1.5936927795410156
step 2069, loss 1.6381114721298218
step 2079, loss 1.6343567371368408
step 2089, loss 1.5901575088500977
step 2099, loss 1.6351947784423828
step 2109, loss 1.5837771892547607
step 2119, loss 1.5517539978027344
step 2129, loss 1.605003833770752
step 2139, loss 1.6016156673431396
step 2149, loss 1.5595353841781616
step 2159, loss 1.572387456893921
step 2169, loss 1.6085634231567383
step 2179, loss 1.6474738121032715
step 2189, loss 1.6315171718597412
step 2199, loss 1.6695003509521484
step 2209, loss 1.5771498680114746
Average loss 1.5968509185732456 for epoch 7, took 386.2292468547821 sec
Accuracy 0.0034331951202796244 for epoch 7, took 80.12648844718933 sec
Epoch 8
step 2219, loss 1.383328914642334
step 2229, loss 1.349717617034912
step 2239, loss 1.5295772552490234
step 2249, loss 2.032221794128418
step 2259, loss 1.5236790180206299
step 2269, loss 1.6080193519592285
step 2279, loss 1.5875258445739746
step 2289, loss 1.656525731086731
step 2299, loss 1.6391663551330566
step 2309, loss 1.5774686336517334
step 2319, loss 1.6236141920089722
step 2329, loss 1.7654229402542114
step 2339, loss 1.708848476409912
step 2349, loss 1.6628345251083374
step 2359, loss 1.629554271697998
step 2369, loss 1.6285912990570068
step 2379, loss 1.6231117248535156
step 2389, loss 1.6352159976959229
step 2399, loss 1.5922765731811523
step 2409, loss 1.711899757385254
step 2419, loss 1.6171307563781738
step 2429, loss 1.5851542949676514
step 2439, loss 1.6313464641571045
Average loss 1.6071294572269708 for epoch 8, took 386.14065289497375 sec
Accuracy 0.0034021400178192037 for epoch 8, took 80.12860822677612 sec
Epoch 9
step 2449, loss 1.3474876880645752
step 2459, loss 1.3501033782958984
step 2469, loss 1.5521032810211182
step 2479, loss 2.1676738262176514
step 2489, loss 1.663549542427063
step 2499, loss 1.55545973777771
step 2509, loss 1.699845552444458
step 2519, loss 1.6640393733978271
step 2529, loss 1.5529677867889404
step 2539, loss 1.5680346488952637
step 2549, loss 1.6742830276489258
step 2559, loss 1.6267530918121338
step 2569, loss 1.6180546283721924
step 2579, loss 1.6024985313415527
step 2589, loss 1.6171138286590576
step 2599, loss 1.6605204343795776
step 2609, loss 1.6827888488769531
step 2619, loss 1.6271839141845703
step 2629, loss 1.6208081245422363
step 2639, loss 1.6020716428756714
step 2649, loss 1.5849837064743042
step 2659, loss 1.6080851554870605
step 2669, loss 1.6106164455413818
Average loss 1.6093417191714572 for epoch 9, took 386.18143606185913 sec
Accuracy 0.0034021400178192037 for epoch 9, took 79.82869935035706 sec
Epoch 10
step 2679, loss 1.455026388168335
step 2689, loss 1.4009850025177002
step 2699, loss 1.8983241319656372
step 2709, loss 1.9267891645431519
step 2719, loss 1.6301491260528564
step 2729, loss 1.7688441276550293
step 2739, loss 1.5864558219909668
step 2749, loss 1.5387723445892334
step 2759, loss 1.6422653198242188
step 2769, loss 1.6059534549713135
step 2779, loss 1.5863630771636963
step 2789, loss 1.586637258529663
step 2799, loss 1.6379594802856445
step 2809, loss 1.5924396514892578
step 2819, loss 1.5658785104751587
step 2829, loss 1.5468988418579102
step 2839, loss 1.6215931177139282
step 2849, loss 1.637915849685669
step 2859, loss 1.5927364826202393
step 2869, loss 1.6165413856506348
step 2879, loss 1.5628771781921387
step 2889, loss 1.57606840133667
step 2899, loss 1.644572377204895
Average loss 1.6048828159507953 for epoch 10, took 386.10042572021484 sec
Accuracy 0.0034331951202796244 for epoch 10, took 80.34291863441467 sec
Epoch 11
step 2909, loss 1.3717478513717651
step 2919, loss 1.4197230339050293
step 2929, loss 1.4951095581054688
step 2939, loss 1.725451946258545
step 2949, loss 1.6942579746246338
step 2959, loss 1.6856296062469482
step 2969, loss 1.5515531301498413
step 2979, loss 1.5862152576446533
step 2989, loss 1.6152269840240479
step 2999, loss 1.5797674655914307
step 3009, loss 1.629390001296997
step 3019, loss 1.581942081451416
step 3029, loss 1.5986394882202148
step 3039, loss 1.6458463668823242
step 3049, loss 1.6314759254455566
step 3059, loss 1.5536010265350342
step 3069, loss 1.6453075408935547
step 3079, loss 1.6426591873168945
step 3089, loss 1.629349708557129
step 3099, loss 1.575798511505127
step 3109, loss 1.6149083375930786
step 3119, loss 1.6249358654022217
Average loss 1.6141587966366817 for epoch 11, took 386.22215700149536 sec
Accuracy 0.0034331951202796244 for epoch 11, took 80.21424341201782 sec
Epoch 12
step 3129, loss 1.6767314672470093
step 3139, loss 1.5889849662780762
step 3149, loss 1.4144787788391113
step 3159, loss 1.577157735824585
step 3169, loss 1.6422624588012695
step 3179, loss 1.8153932094573975
step 3189, loss 1.5953373908996582
step 3199, loss 1.6162292957305908
step 3209, loss 1.7485580444335938
step 3219, loss 1.6111295223236084
step 3229, loss 1.594351053237915
step 3239, loss 1.625166416168213
step 3249, loss 1.6427443027496338
step 3259, loss 1.5669174194335938
step 3269, loss 1.584224820137024
step 3279, loss 1.6358026266098022
step 3289, loss 1.6252496242523193
step 3299, loss 1.6008810997009277
step 3309, loss 1.6294023990631104
step 3319, loss 1.6776742935180664
step 3329, loss 1.6060374975204468
step 3339, loss 1.6988742351531982
step 3349, loss 1.6119641065597534
Average loss 1.609059325958553 for epoch 12, took 386.2313439846039 sec
Accuracy 0.0034331951202796244 for epoch 12, took 80.08444857597351 sec
Epoch 13
step 3359, loss 1.4370696544647217
step 3369, loss 1.4446547031402588
step 3379, loss 1.7380497455596924
step 3389, loss 1.7671239376068115
step 3399, loss 1.5837434530258179
step 3409, loss 1.619382381439209
step 3419, loss 1.6369324922561646
step 3429, loss 1.6050540208816528
step 3439, loss 1.5912137031555176
step 3449, loss 1.5913841724395752
step 3459, loss 1.583011269569397
step 3469, loss 1.5075345039367676
step 3479, loss 1.6347649097442627
step 3489, loss 1.6255443096160889
step 3499, loss 1.7824065685272217
step 3509, loss 1.6063024997711182
step 3519, loss 1.6164069175720215
step 3529, loss 1.6059837341308594
step 3539, loss 1.6537902355194092
step 3549, loss 1.5522263050079346
step 3559, loss 1.6902599334716797
step 3569, loss 1.5520410537719727
step 3579, loss 1.526977300643921
Average loss 1.6112628207917798 for epoch 13, took 386.25968885421753 sec
Accuracy 0.0034331951202796244 for epoch 13, took 80.4640417098999 sec
Epoch 14
step 3589, loss 1.3004558086395264
step 3599, loss 1.3851250410079956
step 3609, loss 1.451207160949707
step 3619, loss 2.094625949859619
step 3629, loss 1.5987197160720825
step 3639, loss 1.653328537940979
step 3649, loss 1.6430233716964722
step 3659, loss 1.6183419227600098
step 3669, loss 1.6224290132522583
step 3679, loss 1.5932687520980835
step 3689, loss 1.639281988143921
step 3699, loss 1.5936615467071533
step 3709, loss 1.6122641563415527
step 3719, loss 1.6640429496765137
step 3729, loss 1.6188887357711792
step 3739, loss 1.5916218757629395
step 3749, loss 1.6209726333618164
step 3759, loss 1.6481809616088867
step 3769, loss 1.6158020496368408
step 3779, loss 1.6685259342193604
step 3789, loss 1.6144945621490479
step 3799, loss 1.605083703994751
step 3809, loss 1.5825779438018799
Average loss 1.60966862502851 for epoch 14, took 386.2977249622345 sec
Accuracy 0.0030765968747858267 for epoch 14, took 80.13751411437988 sec
Epoch 15
step 3819, loss 1.4453506469726562
step 3829, loss 1.3854401111602783
step 3839, loss 1.6750026941299438
step 3849, loss 2.0351648330688477
step 3859, loss 1.6878163814544678
step 3869, loss 1.6873887777328491
step 3879, loss 1.6338481903076172
step 3889, loss 1.70111882686615
step 3899, loss 1.6281516551971436
step 3909, loss 1.5876764059066772
step 3919, loss 1.6775565147399902
step 3929, loss 1.5349640846252441
step 3939, loss 1.5098795890808105
step 3949, loss 1.5875974893569946
step 3959, loss 1.6400349140167236
step 3969, loss 1.6330842971801758
step 3979, loss 1.5968503952026367
step 3989, loss 1.588312029838562
step 3999, loss 1.5606180429458618
step 4009, loss 1.6082004308700562
step 4019, loss 1.6325383186340332
step 4029, loss 1.6280884742736816
step 4039, loss 1.5717198848724365
Average loss 1.6074267986573672 for epoch 15, took 386.2941777706146 sec
Accuracy 0.003337888081694195 for epoch 15, took 79.98943662643433 sec
Epoch 16
step 4049, loss 1.3953291177749634
step 4059, loss 1.4349578619003296
step 4069, loss 1.7917022705078125
step 4079, loss 1.9589533805847168
step 4089, loss 1.6472707986831665
step 4099, loss 1.6700743436813354
step 4109, loss 1.6141982078552246
step 4119, loss 1.6471147537231445
step 4129, loss 1.5914711952209473
step 4139, loss 1.6180953979492188
step 4149, loss 1.6400997638702393
step 4159, loss 1.670456886291504
step 4169, loss 1.5922292470932007
step 4179, loss 1.5810399055480957
step 4189, loss 1.6043680906295776
step 4199, loss 1.59269118309021
step 4209, loss 1.595731258392334
step 4219, loss 1.5453882217407227
step 4229, loss 1.6031289100646973
step 4239, loss 1.629716157913208
step 4249, loss 1.5717599391937256
step 4259, loss 1.6666549444198608
Average loss 1.604132520525079 for epoch 16, took 386.3179988861084 sec
Accuracy 0.003337888081694195 for epoch 16, took 80.15418744087219 sec
Epoch 17
step 4269, loss 1.6066720485687256
step 4279, loss 1.319975733757019
step 4289, loss 1.4062716960906982
step 4299, loss 1.7158061265945435
step 4309, loss 1.92899489402771
step 4319, loss 1.7151131629943848
step 4329, loss 1.6554409265518188
step 4339, loss 1.544042706489563
step 4349, loss 1.685300588607788
step 4359, loss 1.63828706741333
step 4369, loss 1.5553126335144043
step 4379, loss 1.6140737533569336
step 4389, loss 1.6053658723831177
step 4399, loss 1.613455057144165
step 4409, loss 1.5656622648239136
step 4419, loss 1.7370595932006836
step 4429, loss 1.6451494693756104
step 4439, loss 1.5893741846084595
step 4449, loss 1.6042101383209229
step 4459, loss 1.624984860420227
step 4469, loss 1.625262975692749
step 4479, loss 1.5962154865264893
step 4489, loss 1.7298858165740967
Average loss 1.6012447884208278 for epoch 17, took 386.1967122554779 sec
Accuracy 0.0034331951202796244 for epoch 17, took 80.09493970870972 sec
Epoch 18
step 4499, loss 1.4812610149383545
step 4509, loss 1.446255087852478
step 4519, loss 1.5485570430755615
step 4529, loss 1.7407159805297852
step 4539, loss 1.8210821151733398
step 4549, loss 1.7420579195022583
step 4559, loss 1.5544233322143555
step 4569, loss 1.6129791736602783
step 4579, loss 1.6036014556884766
step 4589, loss 1.552507996559143
step 4599, loss 1.628690481185913
step 4609, loss 1.6591107845306396
step 4619, loss 1.5888309478759766
step 4629, loss 1.560139536857605
step 4639, loss 1.5103446245193481
step 4649, loss 1.600221872329712
step 4659, loss 1.6329103708267212
step 4669, loss 1.633556604385376
step 4679, loss 1.5685675144195557
step 4689, loss 1.601699948310852
step 4699, loss 1.6349995136260986
step 4709, loss 1.6706148386001587
step 4719, loss 1.5368131399154663
Average loss 1.6093928270172655 for epoch 18, took 385.966427564621 sec
Accuracy 0.0034331951202796244 for epoch 18, took 80.1692156791687 sec
Epoch 19
step 4729, loss 1.303431510925293
step 4739, loss 1.3794445991516113
step 4749, loss 1.606810212135315
step 4759, loss 1.8802704811096191
step 4769, loss 1.6088327169418335
step 4779, loss 1.6957621574401855
step 4789, loss 1.6059138774871826
step 4799, loss 1.579272747039795
step 4809, loss 1.6487774848937988
step 4819, loss 1.553459644317627
step 4829, loss 1.625356674194336
step 4839, loss 1.6135714054107666
step 4849, loss 1.644851803779602
step 4859, loss 1.574704885482788
step 4869, loss 1.5722625255584717
step 4879, loss 1.6198488473892212
step 4889, loss 1.637101173400879
step 4899, loss 1.67140531539917
step 4909, loss 1.5839784145355225
step 4919, loss 1.583940029144287
step 4929, loss 1.6394147872924805
step 4939, loss 1.6162476539611816
step 4949, loss 1.5970124006271362
Average loss 1.6178461674012636 for epoch 19, took 381.5156981945038 sec
Accuracy 0.0034331951202796244 for epoch 19, took 80.30078172683716 sec
Epoch 20
step 4959, loss 1.3936493396759033
step 4969, loss 1.3989235162734985
step 4979, loss 1.8203299045562744
step 4989, loss 1.916089415550232
step 4999, loss 1.6635782718658447
step 5009, loss 1.7024633884429932
step 5019, loss 1.5651984214782715
step 5029, loss 1.5474773645401
step 5039, loss 1.5865166187286377
step 5049, loss 1.6191643476486206
step 5059, loss 1.5552538633346558
step 5069, loss 1.6129974126815796
step 5079, loss 1.5996848344802856
step 5089, loss 1.597000002861023
step 5099, loss 1.5706511735916138
step 5109, loss 1.5920372009277344
step 5119, loss 1.6339730024337769
step 5129, loss 1.538230061531067
step 5139, loss 1.610809564590454
step 5149, loss 1.6092209815979004
step 5159, loss 1.6232903003692627
step 5169, loss 1.5940830707550049
step 5179, loss 1.6341891288757324
Average loss 1.6102087534310525 for epoch 20, took 381.54732179641724 sec
Accuracy 0.003337888081694195 for epoch 20, took 80.14691734313965 sec
Epoch 21
step 5189, loss 1.3349997997283936
step 5199, loss 1.3767387866973877
step 5209, loss 1.4702523946762085
step 5219, loss 1.8167495727539062
step 5229, loss 1.6528680324554443
step 5239, loss 1.6633672714233398
step 5249, loss 1.6131830215454102
step 5259, loss 1.6246140003204346
step 5269, loss 1.5579380989074707
step 5279, loss 1.628778100013733
step 5289, loss 1.5672948360443115
step 5299, loss 1.5872554779052734
step 5309, loss 1.5126476287841797
step 5319, loss 1.6472363471984863
step 5329, loss 1.6566251516342163
step 5339, loss 1.6262016296386719
step 5349, loss 1.5929133892059326
step 5359, loss 1.645547866821289
step 5369, loss 1.5658633708953857
step 5379, loss 1.5663537979125977
step 5389, loss 1.6432695388793945
step 5399, loss 1.5950446128845215
Average loss 1.5982570109660166 for epoch 21, took 381.52052307128906 sec
Accuracy 0.0034331951202796244 for epoch 21, took 80.27287149429321 sec
Epoch 22
step 5409, loss 1.7193844318389893
step 5419, loss 1.3254389762878418
step 5429, loss 1.3815698623657227
step 5439, loss 1.4930996894836426
step 5449, loss 1.9144179821014404
step 5459, loss 1.7184526920318604
step 5469, loss 1.7103874683380127
step 5479, loss 1.6267545223236084
step 5489, loss 1.5326251983642578
step 5499, loss 1.5839918851852417
step 5509, loss 1.6359668970108032
step 5519, loss 1.6284650564193726
step 5529, loss 1.6143264770507812
step 5539, loss 1.6007425785064697
step 5549, loss 1.6093566417694092
step 5559, loss 1.6248856782913208
step 5569, loss 1.5482845306396484
step 5579, loss 1.5831170082092285
step 5589, loss 1.5246992111206055
step 5599, loss 1.6165120601654053
step 5609, loss 1.6687151193618774
step 5619, loss 1.6688354015350342
step 5629, loss 1.632108449935913
Average loss 1.6143236066165723 for epoch 22, took 381.7654778957367 sec
Accuracy 0.0034331951202796244 for epoch 22, took 79.99750542640686 sec
Epoch 23
step 5639, loss 1.5356916189193726
step 5649, loss 1.476751446723938
step 5659, loss 1.6589851379394531
step 5669, loss 2.2564945220947266
step 5679, loss 1.993079423904419
step 5689, loss 1.6224899291992188
step 5699, loss 1.7233467102050781
step 5709, loss 1.6238504648208618
step 5719, loss 1.5422042608261108
step 5729, loss 1.6622669696807861
step 5739, loss 1.5901005268096924
step 5749, loss 1.6292929649353027
step 5759, loss 1.5734065771102905
step 5769, loss 1.519093632698059
step 5779, loss 1.6875591278076172
step 5789, loss 1.7355999946594238
step 5799, loss 1.596916675567627
step 5809, loss 1.6184495687484741
step 5819, loss 1.6124800443649292
step 5829, loss 1.6513229608535767
step 5839, loss 1.6107937097549438
step 5849, loss 1.6339025497436523
step 5859, loss 1.6178746223449707
Average loss 1.6359038928098846 for epoch 23, took 381.73005199432373 sec
Accuracy 0.0034331951202796244 for epoch 23, took 79.83783340454102 sec
Epoch 24
step 5869, loss 1.410005807876587
step 5879, loss 1.4083795547485352
step 5889, loss 1.4949859380722046
step 5899, loss 1.7805917263031006
step 5909, loss 1.7040901184082031
step 5919, loss 1.5801597833633423
step 5929, loss 1.64652681350708
step 5939, loss 1.607818603515625
step 5949, loss 1.6232872009277344
step 5959, loss 1.5930581092834473
step 5969, loss 1.6768277883529663
step 5979, loss 1.5881764888763428
step 5989, loss 1.6144742965698242
step 5999, loss 1.6318371295928955
step 6009, loss 1.6414693593978882
step 6019, loss 1.626957654953003
step 6029, loss 1.5597171783447266
step 6039, loss 1.6791666746139526
step 6049, loss 1.5595746040344238
step 6059, loss 1.568950891494751
step 6069, loss 1.5929296016693115
step 6079, loss 1.6695857048034668
step 6089, loss 1.5686569213867188
Average loss 1.6070965115438427 for epoch 24, took 381.69888520240784 sec
Accuracy 0.0034331951202796244 for epoch 24, took 80.24925565719604 sec
Epoch 25
step 6099, loss 1.3984768390655518
step 6109, loss 1.3942286968231201
step 6119, loss 1.6399409770965576
step 6129, loss 1.5504735708236694
step 6139, loss 1.605696439743042
step 6149, loss 1.6762163639068604
step 6159, loss 1.635212779045105
step 6169, loss 1.6144647598266602
step 6179, loss 1.5846197605133057
step 6189, loss 1.7480502128601074
step 6199, loss 1.6091585159301758
step 6209, loss 1.5576000213623047
step 6219, loss 1.6342005729675293
step 6229, loss 1.556438684463501
step 6239, loss 1.6460840702056885
step 6249, loss 1.5421096086502075
step 6259, loss 1.5863142013549805
step 6269, loss 1.628603458404541
step 6279, loss 1.6360580921173096
step 6289, loss 1.60475492477417
step 6299, loss 1.6209074258804321
step 6309, loss 1.595961093902588
step 6319, loss 1.6450015306472778
Average loss 1.6101820583929096 for epoch 25, took 381.64724588394165 sec
Accuracy 0.0034331951202796244 for epoch 25, took 80.13235855102539 sec
Epoch 26
step 6329, loss 1.3480567932128906
step 6339, loss 1.3804130554199219
step 6349, loss 1.6507582664489746
step 6359, loss 1.6395717859268188
step 6369, loss 1.6502039432525635
step 6379, loss 1.6686251163482666
step 6389, loss 1.6427104473114014
step 6399, loss 1.6671216487884521
step 6409, loss 1.6055865287780762
step 6419, loss 1.5845680236816406
step 6429, loss 1.6372020244598389
step 6439, loss 1.6478779315948486
step 6449, loss 1.5961536169052124
step 6459, loss 1.6044092178344727
step 6469, loss 1.6405810117721558
step 6479, loss 1.666117787361145
step 6489, loss 1.6205096244812012
step 6499, loss 1.5967719554901123
step 6509, loss 1.6573435068130493
step 6519, loss 1.6674333810806274
step 6529, loss 1.7170605659484863
step 6539, loss 1.6182034015655518
Average loss 1.6124547526501773 for epoch 26, took 381.5792531967163 sec
Accuracy 0.0034331951202796244 for epoch 26, took 80.12561297416687 sec
Epoch 27
step 6549, loss 1.633864164352417
step 6559, loss 1.370915174484253
step 6569, loss 1.389169454574585
step 6579, loss 2.2114319801330566
step 6589, loss 1.8999075889587402
step 6599, loss 1.6175434589385986
step 6609, loss 1.6260006427764893
step 6619, loss 1.624143362045288
step 6629, loss 1.6151485443115234
step 6639, loss 1.645182728767395
step 6649, loss 1.5875012874603271
step 6659, loss 1.630031704902649
step 6669, loss 1.5465269088745117
step 6679, loss 1.596258282661438
step 6689, loss 1.586609125137329
step 6699, loss 1.6310515403747559
step 6709, loss 1.6657824516296387
step 6719, loss 1.6165423393249512
step 6729, loss 1.5982486009597778
step 6739, loss 1.6009037494659424
step 6749, loss 1.5759742259979248
step 6759, loss 1.569380760192871
step 6769, loss 1.6187045574188232
Average loss 1.6048809603640908 for epoch 27, took 381.68503069877625 sec
Accuracy 0.0034331951202796244 for epoch 27, took 80.20540118217468 sec
Epoch 28
step 6779, loss 1.4241762161254883
step 6789, loss 1.3634172677993774
step 6799, loss 1.4887458086013794
step 6809, loss 1.737464427947998
step 6819, loss 1.7523506879806519
step 6829, loss 1.6264210939407349
step 6839, loss 1.5875004529953003
step 6849, loss 1.6121941804885864
step 6859, loss 1.5966975688934326
step 6869, loss 1.677281379699707
step 6879, loss 1.6020429134368896
step 6889, loss 1.5985121726989746
step 6899, loss 1.6273279190063477
step 6909, loss 1.6454943418502808
step 6919, loss 1.6093766689300537
step 6929, loss 1.6501226425170898
step 6939, loss 1.5906860828399658
step 6949, loss 1.571718692779541
step 6959, loss 1.5525671243667603
step 6969, loss 1.588637351989746
step 6979, loss 1.6150662899017334
step 6989, loss 1.6999897956848145
step 6999, loss 1.6599817276000977
Average loss 1.601620086975265 for epoch 28, took 381.4983651638031 sec
Accuracy 0.0034021400178192037 for epoch 28, took 80.05252432823181 sec
Epoch 29
step 7009, loss 1.3462070226669312
step 7019, loss 1.3693699836730957
step 7029, loss 1.5610662698745728
step 7039, loss 1.8895542621612549
step 7049, loss 1.5996570587158203
step 7059, loss 1.6472430229187012
step 7069, loss 1.705296516418457
step 7079, loss 1.642329454421997
step 7089, loss 1.5756535530090332
step 7099, loss 1.5879952907562256
step 7109, loss 1.6192824840545654
step 7119, loss 1.72287917137146
step 7129, loss 1.6083494424819946
step 7139, loss 1.6337621212005615
step 7149, loss 1.6458156108856201
step 7159, loss 1.6461684703826904
step 7169, loss 1.6441724300384521
step 7179, loss 1.6619298458099365
step 7189, loss 1.599092960357666
step 7199, loss 1.6141107082366943
step 7209, loss 1.5788419246673584
step 7219, loss 1.508662462234497
step 7229, loss 1.6468491554260254
Average loss 1.6018671158112978 for epoch 29, took 381.59967494010925 sec
Accuracy 0.0034331951202796244 for epoch 29, took 80.1039686203003 sec
Epoch 30
step 7239, loss 1.3511970043182373
step 7249, loss 1.3927276134490967
step 7259, loss 1.539792776107788
step 7269, loss 1.634598731994629
step 7279, loss 1.6202738285064697
step 7289, loss 1.6754343509674072
step 7299, loss 1.6142466068267822
step 7309, loss 1.596131443977356
step 7319, loss 1.684744119644165
step 7329, loss 1.604894757270813
step 7339, loss 1.6361826658248901
step 7349, loss 1.6201136112213135
step 7359, loss 1.6603610515594482
step 7369, loss 1.543681025505066
step 7379, loss 1.6464869976043701
step 7389, loss 1.555713415145874
step 7399, loss 1.6398122310638428
step 7409, loss 1.6605134010314941
step 7419, loss 1.6179111003875732
step 7429, loss 1.5914500951766968
step 7439, loss 1.6712311506271362
step 7449, loss 1.5481791496276855
step 7459, loss 1.6254664659500122
Average loss 1.6081521516306359 for epoch 30, took 381.6450209617615 sec
Accuracy 0.003337888081694195 for epoch 30, took 80.39896941184998 sec
Epoch 31
step 7469, loss 1.4070366621017456
step 7479, loss 1.4355082511901855
step 7489, loss 1.557252287864685
step 7499, loss 1.7965734004974365
step 7509, loss 1.6578603982925415
step 7519, loss 1.6333582401275635
step 7529, loss 1.6452336311340332
step 7539, loss 1.6590076684951782
step 7549, loss 1.6212513446807861
step 7559, loss 1.5547574758529663
step 7569, loss 1.6360743045806885
step 7579, loss 1.6256916522979736
step 7589, loss 1.5959327220916748
step 7599, loss 1.6418272256851196
step 7609, loss 1.72288179397583
step 7619, loss 1.5666084289550781
step 7629, loss 1.6579655408859253
step 7639, loss 1.5848944187164307
step 7649, loss 1.639129400253296
step 7659, loss 1.6426219940185547
step 7669, loss 1.6012656688690186
step 7679, loss 1.6146750450134277
Average loss 1.6035764405601902 for epoch 31, took 381.56394028663635 sec
Accuracy 0.0034021400178192037 for epoch 31, took 79.98635125160217 sec
Epoch 32
step 7689, loss 1.556837558746338
step 7699, loss 1.4574031829833984
step 7709, loss 1.4032032489776611
step 7719, loss 1.5891344547271729
step 7729, loss 1.6166024208068848
step 7739, loss 1.6127231121063232
step 7749, loss 1.6172363758087158
step 7759, loss 1.6325737237930298
step 7769, loss 1.5791645050048828
step 7779, loss 1.627597689628601
step 7789, loss 1.6104094982147217
step 7799, loss 1.5893974304199219
step 7809, loss 1.604818344116211
step 7819, loss 1.682015299797058
step 7829, loss 1.6329554319381714
step 7839, loss 1.5851564407348633
step 7849, loss 1.6078159809112549
step 7859, loss 1.686189889907837
step 7869, loss 1.6121615171432495
step 7879, loss 1.6003756523132324
step 7889, loss 1.5510406494140625
step 7899, loss 1.614910364151001
step 7909, loss 1.6041756868362427
Average loss 1.6078000032065207 for epoch 32, took 381.5106289386749 sec
Accuracy 0.0034331951202796244 for epoch 32, took 80.22062087059021 sec
Epoch 33
step 7919, loss 1.4998868703842163
step 7929, loss 1.3430159091949463
step 7939, loss 1.678454875946045
step 7949, loss 1.8308749198913574
step 7959, loss 1.5475096702575684
step 7969, loss 1.6218738555908203
step 7979, loss 1.6595933437347412
step 7989, loss 1.6435565948486328
step 7999, loss 1.623265266418457
step 8009, loss 1.6095290184020996
step 8019, loss 1.6271097660064697
step 8029, loss 1.6065030097961426
step 8039, loss 1.6099674701690674
step 8049, loss 1.5535283088684082
step 8059, loss 1.6201517581939697
step 8069, loss 1.5693155527114868
step 8079, loss 1.6377241611480713
step 8089, loss 1.619626760482788
step 8099, loss 1.5938825607299805
step 8109, loss 1.6317728757858276
step 8119, loss 1.5675259828567505
step 8129, loss 1.6366677284240723
step 8139, loss 1.6163434982299805
Average loss 1.6049247347472007 for epoch 33, took 381.6931097507477 sec
Accuracy 0.0034331951202796244 for epoch 33, took 80.20194554328918 sec
Epoch 34
step 8149, loss 1.4352107048034668
step 8159, loss 1.4510853290557861
step 8169, loss 1.6540287733078003
step 8179, loss 1.977381944656372
step 8189, loss 1.5979723930358887
step 8199, loss 1.508043885231018
step 8209, loss 1.5271230936050415
step 8219, loss 1.6359933614730835
step 8229, loss 1.6320725679397583
step 8239, loss 1.609859824180603
step 8249, loss 1.6018784046173096
step 8259, loss 1.5920748710632324
step 8269, loss 1.5670047998428345
step 8279, loss 1.630523920059204
step 8289, loss 1.5872738361358643
step 8299, loss 1.5665603876113892
step 8309, loss 1.5840870141983032
step 8319, loss 1.6219468116760254
step 8329, loss 1.5907468795776367
step 8339, loss 1.5838531255722046
step 8349, loss 1.5749831199645996
step 8359, loss 1.6193631887435913
step 8369, loss 1.6972379684448242
Average loss 1.6113144906989314 for epoch 34, took 381.57632637023926 sec
Accuracy 0.0034331951202796244 for epoch 34, took 80.2403450012207 sec
Epoch 35
step 8379, loss 1.3513902425765991
step 8389, loss 1.4516353607177734
step 8399, loss 1.4613449573516846
step 8409, loss 1.7223472595214844
step 8419, loss 1.5438921451568604
step 8429, loss 1.6537706851959229
step 8439, loss 1.5793559551239014
step 8449, loss 1.6219582557678223
step 8459, loss 1.6574664115905762
step 8469, loss 1.5326863527297974
step 8479, loss 1.618114709854126
step 8489, loss 1.5791940689086914
step 8499, loss 1.5957627296447754
step 8509, loss 1.6221294403076172
step 8519, loss 1.549095630645752
step 8529, loss 1.609391212463379
step 8539, loss 1.6178966760635376
step 8549, loss 1.6041160821914673
step 8559, loss 1.5611109733581543
step 8569, loss 1.6016885042190552
step 8579, loss 1.5976300239562988
step 8589, loss 1.5986952781677246
step 8599, loss 1.6079164743423462
Average loss 1.6070303279056883 for epoch 35, took 381.61106038093567 sec
Accuracy 0.0034021400178192037 for epoch 35, took 80.29405879974365 sec
Epoch 36
step 8609, loss 1.294800043106079
step 8619, loss 1.3885223865509033
step 8629, loss 1.7455079555511475
step 8639, loss 1.5690810680389404
step 8649, loss 1.7497596740722656
step 8659, loss 1.6633155345916748
step 8669, loss 1.5914218425750732
step 8679, loss 1.6808650493621826
step 8689, loss 1.538830041885376
step 8699, loss 1.593801498413086
step 8709, loss 1.6898590326309204
step 8719, loss 1.6057984828948975
step 8729, loss 1.6117732524871826
step 8739, loss 1.5980701446533203
step 8749, loss 1.6227129697799683
step 8759, loss 1.7038683891296387
step 8769, loss 1.6340157985687256
step 8779, loss 1.6244981288909912
step 8789, loss 1.5669314861297607
step 8799, loss 1.6178548336029053
step 8809, loss 1.6386253833770752
step 8819, loss 1.5642738342285156
Average loss 1.6025452399462985 for epoch 36, took 381.68112802505493 sec
Accuracy 0.0034331951202796244 for epoch 36, took 80.190190076828 sec
Epoch 37
step 8829, loss 1.6543433666229248
step 8839, loss 1.4500126838684082
step 8849, loss 1.4314746856689453
step 8859, loss 1.9615733623504639
step 8869, loss 1.6663419008255005
step 8879, loss 1.6783077716827393
step 8889, loss 1.6643059253692627
step 8899, loss 1.6808340549468994
step 8909, loss 1.6070348024368286
step 8919, loss 1.600667119026184
step 8929, loss 1.650913953781128
step 8939, loss 1.576153039932251
step 8949, loss 1.550002098083496
step 8959, loss 1.5825519561767578
step 8969, loss 1.6086170673370361
step 8979, loss 1.583008050918579
step 8989, loss 1.620766282081604
step 8999, loss 1.5821402072906494
step 9009, loss 1.6206717491149902
step 9019, loss 1.6421945095062256
step 9029, loss 1.6101574897766113
step 9039, loss 1.6102778911590576
step 9049, loss 1.6109528541564941
Average loss 1.61246384131281 for epoch 37, took 381.5994040966034 sec
Accuracy 0.0034331951202796244 for epoch 37, took 80.42025637626648 sec
Epoch 38
step 9059, loss 1.4129033088684082
step 9069, loss 1.37864089012146
step 9079, loss 1.3239665031433105
step 9089, loss 1.735541582107544
step 9099, loss 1.7439961433410645
step 9109, loss 1.6466610431671143
step 9119, loss 1.714237928390503
step 9129, loss 1.654740571975708
step 9139, loss 1.6685274839401245
step 9149, loss 1.6043221950531006
step 9159, loss 1.602038860321045
step 9169, loss 1.6157289743423462
step 9179, loss 1.5533781051635742
step 9189, loss 1.5909345149993896
step 9199, loss 1.6096235513687134
step 9209, loss 1.5480940341949463
step 9219, loss 1.6281962394714355
step 9229, loss 1.649627923965454
step 9239, loss 1.6595098972320557
step 9249, loss 1.6220301389694214
step 9259, loss 1.565121054649353
step 9269, loss 1.5850119590759277
step 9279, loss 1.5475037097930908
Average loss 1.612808631177534 for epoch 38, took 386.28690791130066 sec
Accuracy 0.0034331951202796244 for epoch 38, took 80.1044454574585 sec
Epoch 39
step 9289, loss 1.4271161556243896
step 9299, loss 1.396767258644104
step 9309, loss 1.8274873495101929
step 9319, loss 1.9247853755950928
step 9329, loss 1.620131015777588
step 9339, loss 1.656536340713501
step 9349, loss 1.5965468883514404
step 9359, loss 1.6218032836914062
step 9369, loss 1.6072711944580078
step 9379, loss 1.5881829261779785
step 9389, loss 1.6120667457580566
step 9399, loss 1.5962265729904175
step 9409, loss 1.7076045274734497
step 9419, loss 1.5604145526885986
step 9429, loss 1.5856404304504395
step 9439, loss 1.580744981765747
step 9449, loss 1.6204659938812256
step 9459, loss 1.6243371963500977
step 9469, loss 1.5920844078063965
step 9479, loss 1.600827693939209
step 9489, loss 1.626842975616455
step 9499, loss 1.5873479843139648
step 9509, loss 1.540723443031311
Average loss 1.61100134462641 for epoch 39, took 386.23540687561035 sec
Accuracy 0.0034331951202796244 for epoch 39, took 80.12684202194214 sec
Epoch 40
step 9519, loss 1.4852585792541504
step 9529, loss 1.4293997287750244
step 9539, loss 1.3748903274536133
step 9549, loss 1.6954679489135742
step 9559, loss 1.6241676807403564
step 9569, loss 1.5963995456695557
step 9579, loss 1.6603446006774902
step 9589, loss 1.5363259315490723
step 9599, loss 1.5897071361541748
step 9609, loss 1.6558669805526733
step 9619, loss 1.64664626121521
step 9629, loss 1.647475004196167
step 9639, loss 1.598426342010498
step 9649, loss 1.6054226160049438
step 9659, loss 1.6040667295455933
step 9669, loss 1.5103408098220825
step 9679, loss 1.653200387954712
step 9689, loss 1.8291915655136108
step 9699, loss 1.8575470447540283
step 9709, loss 1.696929693222046
step 9719, loss 1.5893175601959229
step 9729, loss 1.6538443565368652
step 9739, loss 1.6230117082595825
Average loss 1.6160795652029807 for epoch 40, took 386.29501008987427 sec
Accuracy 0.0034331951202796244 for epoch 40, took 80.25032591819763 sec
Epoch 41
step 9749, loss 1.427115559577942
step 9759, loss 1.4083774089813232
step 9769, loss 1.8537945747375488
step 9779, loss 1.7285207509994507
step 9789, loss 1.7889004945755005
step 9799, loss 1.6867910623550415
step 9809, loss 1.6177729368209839
step 9819, loss 1.6114215850830078
step 9829, loss 1.6436209678649902
step 9839, loss 1.5909054279327393
step 9849, loss 1.5864437818527222
step 9859, loss 1.683315634727478
step 9869, loss 1.6454224586486816
step 9879, loss 1.624406099319458
step 9889, loss 1.6971553564071655
step 9899, loss 1.6750426292419434
step 9909, loss 1.6274592876434326
step 9919, loss 1.5675603151321411
step 9929, loss 1.575573205947876
step 9939, loss 1.6148276329040527
step 9949, loss 1.5951979160308838
step 9959, loss 1.6112968921661377
Average loss 1.6128507392448292 for epoch 41, took 386.1896698474884 sec
Accuracy 0.0034331951202796244 for epoch 41, took 80.33177518844604 sec
Epoch 42
step 9969, loss 1.6411447525024414
step 9979, loss 1.5100092887878418
step 9989, loss 1.4107414484024048
step 9999, loss 1.9676964282989502
step 10009, loss 1.777639627456665
step 10019, loss 1.7090140581130981
step 10029, loss 1.6494569778442383
step 10039, loss 1.6481654644012451
step 10049, loss 1.6134698390960693
step 10059, loss 1.6242258548736572
step 10069, loss 1.6703526973724365
step 10079, loss 1.634972333908081
step 10089, loss 1.6321442127227783
step 10099, loss 1.5987123250961304
step 10109, loss 1.5726368427276611
step 10119, loss 1.5983376502990723
step 10129, loss 1.5936343669891357
step 10139, loss 1.6141610145568848
step 10149, loss 1.6904963254928589
step 10159, loss 1.5841660499572754
step 10169, loss 1.572278618812561
step 10179, loss 1.5908656120300293
step 10189, loss 1.6080032587051392
Average loss 1.6117047322423834 for epoch 42, took 386.25254583358765 sec
Accuracy 0.0034331951202796244 for epoch 42, took 80.42128109931946 sec
Epoch 43
step 10199, loss 1.49573814868927
step 10209, loss 1.3529818058013916
step 10219, loss 1.3708220720291138
step 10229, loss 1.5680642127990723
step 10239, loss 1.705200433731079
step 10249, loss 1.5525593757629395
step 10259, loss 1.6233561038970947
step 10269, loss 1.6057227849960327
step 10279, loss 1.5817794799804688
step 10289, loss 1.588454246520996
step 10299, loss 1.5717507600784302
step 10309, loss 1.5910687446594238
step 10319, loss 1.5768604278564453
step 10329, loss 1.574693202972412
step 10339, loss 1.5674643516540527
step 10349, loss 1.621720790863037
step 10359, loss 1.5707887411117554
step 10369, loss 1.546493649482727
step 10379, loss 1.6341924667358398
step 10389, loss 1.5613809823989868
step 10399, loss 1.5883662700653076
step 10409, loss 1.7107658386230469
step 10419, loss 1.6286689043045044
Average loss 1.6068171811731238 for epoch 43, took 386.22321105003357 sec
Accuracy 0.0034331951202796244 for epoch 43, took 80.10044813156128 sec
Epoch 44
step 10429, loss 1.3819985389709473
step 10439, loss 1.3808555603027344
step 10449, loss 1.38649320602417
step 10459, loss 1.5977699756622314
step 10469, loss 1.6170731782913208
step 10479, loss 1.5989569425582886
step 10489, loss 1.6015703678131104
step 10499, loss 1.5902557373046875
step 10509, loss 1.6626158952713013
step 10519, loss 1.6658509969711304
step 10529, loss 1.5865349769592285
step 10539, loss 1.5883053541183472
step 10549, loss 1.610288143157959
step 10559, loss 1.6126419305801392
step 10569, loss 1.6221460103988647
step 10579, loss 1.6040165424346924
step 10589, loss 1.5954108238220215
step 10599, loss 1.6098875999450684
step 10609, loss 1.7627778053283691
step 10619, loss 1.595884084701538
step 10629, loss 1.620443344116211
step 10639, loss 1.536270022392273
step 10649, loss 1.6780959367752075
Average loss 1.6064080951506632 for epoch 44, took 386.2134883403778 sec
Accuracy 0.0034331951202796244 for epoch 44, took 80.06892466545105 sec
Epoch 45
step 10659, loss 1.4200663566589355
step 10669, loss 1.4233319759368896
step 10679, loss 1.6927244663238525
step 10689, loss 1.9284982681274414
step 10699, loss 1.664315104484558
step 10709, loss 1.6024577617645264
step 10719, loss 1.574469804763794
step 10729, loss 1.650044322013855
step 10739, loss 1.7705544233322144
step 10749, loss 1.5255818367004395
step 10759, loss 1.607446312904358
step 10769, loss 1.7804738283157349
step 10779, loss 1.601940631866455
step 10789, loss 1.6300280094146729
step 10799, loss 1.6372504234313965
step 10809, loss 1.6148922443389893
step 10819, loss 1.7162210941314697
step 10829, loss 1.5487616062164307
step 10839, loss 1.5782216787338257
step 10849, loss 1.6256225109100342
step 10859, loss 1.6251606941223145
step 10869, loss 1.5713502168655396
step 10879, loss 1.5898056030273438
Average loss 1.618313269657001 for epoch 45, took 386.23697781562805 sec
Accuracy 0.0034331951202796244 for epoch 45, took 80.37684440612793 sec
Epoch 46
step 10889, loss 1.4979130029678345
step 10899, loss 1.4014124870300293
step 10909, loss 1.6423040628433228
step 10919, loss 2.1248655319213867
step 10929, loss 1.6550263166427612
step 10939, loss 1.6363319158554077
step 10949, loss 1.563103437423706
step 10959, loss 1.6462271213531494
step 10969, loss 1.6016747951507568
step 10979, loss 1.6216968297958374
step 10989, loss 1.607640027999878
step 10999, loss 1.580366611480713
step 11009, loss 1.611135482788086
step 11019, loss 1.6371660232543945
step 11029, loss 1.6524797677993774
step 11039, loss 1.63031804561615
step 11049, loss 1.652162790298462
step 11059, loss 1.5518206357955933
step 11069, loss 1.6584463119506836
step 11079, loss 1.6619278192520142
step 11089, loss 1.5842258930206299
step 11099, loss 1.6157457828521729
Average loss 1.599009438564903 for epoch 46, took 386.24112939834595 sec
Accuracy 0.0034021400178192037 for epoch 46, took 80.00220823287964 sec
Epoch 47
step 11109, loss 1.4912950992584229
step 11119, loss 1.4246156215667725
step 11129, loss 1.4058058261871338
step 11139, loss 1.8163843154907227
step 11149, loss 1.728921890258789
step 11159, loss 1.6333394050598145
step 11169, loss 1.6233471632003784
step 11179, loss 1.6158149242401123
step 11189, loss 1.6015708446502686
step 11199, loss 1.5560181140899658
step 11209, loss 1.6236122846603394
step 11219, loss 1.563320279121399
step 11229, loss 1.6883132457733154
step 11239, loss 1.618852972984314
step 11249, loss 1.6525709629058838
step 11259, loss 1.6658518314361572
step 11269, loss 1.6569722890853882
step 11279, loss 1.6002531051635742
step 11289, loss 1.6199722290039062
step 11299, loss 1.588791847229004
step 11309, loss 1.598318338394165
step 11319, loss 1.5877540111541748
step 11329, loss 1.569615125656128
Average loss 1.6016327628963871 for epoch 47, took 386.29794478416443 sec
Accuracy 0.0034331951202796244 for epoch 47, took 79.94219040870667 sec
Epoch 48
step 11339, loss 1.374475121498108
step 11349, loss 1.3565046787261963
step 11359, loss 1.5173659324645996
step 11369, loss 1.9343984127044678
step 11379, loss 1.6318705081939697
step 11389, loss 1.7123918533325195
step 11399, loss 1.6512951850891113
step 11409, loss 1.600437045097351
step 11419, loss 1.6102632284164429
step 11429, loss 1.6321539878845215
step 11439, loss 1.6176478862762451
step 11449, loss 1.7340104579925537
step 11459, loss 1.6153016090393066
step 11469, loss 1.5617626905441284
step 11479, loss 1.6182255744934082
step 11489, loss 1.614253044128418
step 11499, loss 1.5890532732009888
step 11509, loss 1.5695135593414307
step 11519, loss 1.544933557510376
step 11529, loss 1.6193270683288574
step 11539, loss 1.603206992149353
step 11549, loss 1.5638936758041382
step 11559, loss 1.564289927482605
Average loss 1.6168205236133777 for epoch 48, took 386.11332082748413 sec
Accuracy 0.0034331951202796244 for epoch 48, took 80.2264084815979 sec
Epoch 49
step 11569, loss 1.483591079711914
step 11579, loss 1.4246747493743896
step 11589, loss 1.6193411350250244
step 11599, loss 1.7725484371185303
step 11609, loss 1.6092544794082642
step 11619, loss 1.5914337635040283
step 11629, loss 1.6346839666366577
step 11639, loss 1.6064929962158203
step 11649, loss 1.552466869354248
step 11659, loss 1.628922700881958
step 11669, loss 1.5530292987823486
step 11679, loss 1.5904864072799683
step 11689, loss 1.6099967956542969
step 11699, loss 1.587651252746582
step 11709, loss 1.6143794059753418
step 11719, loss 1.625477910041809
step 11729, loss 1.6585679054260254
step 11739, loss 1.6853402853012085
step 11749, loss 1.6925203800201416
step 11759, loss 1.595925211906433
step 11769, loss 1.5920383930206299
step 11779, loss 1.6157782077789307
step 11789, loss 1.5875601768493652
Average loss 1.6123765210310619 for epoch 49, took 386.2351179122925 sec
Accuracy 0.0034331951202796244 for epoch 49, took 80.41328263282776 sec
Epoch 50
step 11799, loss 1.3629432916641235
step 11809, loss 1.3575462102890015
step 11819, loss 1.3519229888916016
step 11829, loss 1.5528006553649902
step 11839, loss 1.6429646015167236
step 11849, loss 1.6163074970245361
step 11859, loss 1.6603343486785889
step 11869, loss 1.584064245223999
step 11879, loss 1.5902684926986694
step 11889, loss 1.6165494918823242
step 11899, loss 1.6353423595428467
step 11909, loss 1.5862559080123901
step 11919, loss 1.5860648155212402
step 11929, loss 1.6404472589492798
step 11939, loss 1.5807499885559082
step 11949, loss 1.5917456150054932
step 11959, loss 1.6182560920715332
step 11969, loss 1.5742125511169434
step 11979, loss 1.6326696872711182
step 11989, loss 1.5726045370101929
step 11999, loss 1.6237456798553467
step 12009, loss 1.6423269510269165
step 12019, loss 1.5681424140930176
Average loss 1.6016678459811629 for epoch 50, took 386.18110036849976 sec
Accuracy 0.0034021400178192037 for epoch 50, took 80.16673707962036 sec
Epoch 51
step 12029, loss 1.428762435913086
step 12039, loss 1.3949551582336426
step 12049, loss 1.9529690742492676
step 12059, loss 1.6279290914535522
step 12069, loss 1.7580548524856567
step 12079, loss 1.6836379766464233
step 12089, loss 1.6413689851760864
step 12099, loss 1.6751742362976074
step 12109, loss 1.6232941150665283
step 12119, loss 1.5617679357528687
step 12129, loss 1.6001451015472412
step 12139, loss 1.6781654357910156
step 12149, loss 1.6600935459136963
step 12159, loss 1.5897908210754395
step 12169, loss 1.6014081239700317
step 12179, loss 1.5188407897949219
step 12189, loss 1.5994935035705566
step 12199, loss 1.6381052732467651
step 12209, loss 1.5206029415130615
step 12219, loss 1.6305921077728271
step 12229, loss 1.5724878311157227
step 12239, loss 1.648307204246521
Average loss 1.609820586547517 for epoch 51, took 386.2823405265808 sec
Accuracy 0.0034021400178192037 for epoch 51, took 80.18983125686646 sec
Epoch 52
step 12249, loss 1.4371672868728638
step 12259, loss 1.3526002168655396
step 12269, loss 1.3898022174835205
step 12279, loss 1.7087599039077759
step 12289, loss 1.6258985996246338
step 12299, loss 1.71072518825531
step 12309, loss 1.6602933406829834
step 12319, loss 1.5559449195861816
step 12329, loss 1.6164746284484863
step 12339, loss 1.6140522956848145
step 12349, loss 1.6067960262298584
step 12359, loss 1.5893179178237915
step 12369, loss 1.575770616531372
step 12379, loss 1.6958622932434082
step 12389, loss 1.6132760047912598
step 12399, loss 1.6565526723861694
step 12409, loss 1.5805723667144775
step 12419, loss 1.5940135717391968
step 12429, loss 1.6024413108825684
step 12439, loss 1.582024097442627
step 12449, loss 1.4091055393218994
step 12459, loss 1.631974697113037
step 12469, loss 1.5964481830596924
Average loss 1.591879561282041 for epoch 52, took 386.26002621650696 sec
Accuracy 0.0034021400178192037 for epoch 52, took 79.9881842136383 sec
Epoch 53
step 12479, loss 1.5222859382629395
step 12489, loss 1.3090406656265259
step 12499, loss 1.5603017807006836
step 12509, loss 1.7072806358337402
step 12519, loss 1.8030602931976318
step 12529, loss 1.6343618631362915
step 12539, loss 1.6134158372879028
step 12549, loss 1.610069751739502
step 12559, loss 1.6125378608703613
step 12569, loss 1.5916852951049805
step 12579, loss 1.6014059782028198
step 12589, loss 1.5931588411331177
step 12599, loss 1.5968043804168701
step 12609, loss 1.5879942178726196
step 12619, loss 1.5988171100616455
step 12629, loss 1.5642648935317993
step 12639, loss 1.5970940589904785
step 12649, loss 1.6558713912963867
step 12659, loss 1.6245142221450806
step 12669, loss 1.6823605298995972
step 12679, loss 1.6290428638458252
step 12689, loss 1.6496704816818237
step 12699, loss 1.6256650686264038
Average loss 1.6058854518229502 for epoch 53, took 386.22424244880676 sec
Accuracy 0.0034021400178192037 for epoch 53, took 80.02075242996216 sec
Epoch 54
step 12709, loss 1.4522428512573242
step 12719, loss 1.4726486206054688
step 12729, loss 1.593095302581787
step 12739, loss 1.9408140182495117
step 12749, loss 1.6184594631195068
step 12759, loss 1.6583964824676514
step 12769, loss 1.5570974349975586
step 12779, loss 1.5916268825531006
step 12789, loss 1.6795704364776611
step 12799, loss 1.600436806678772
step 12809, loss 1.6589559316635132
step 12819, loss 1.5238475799560547
step 12829, loss 1.5740554332733154
step 12839, loss 1.6201934814453125
step 12849, loss 1.5956631898880005
step 12859, loss 1.5778040885925293
step 12869, loss 1.624173879623413
step 12879, loss 1.6101347208023071
step 12889, loss 1.6233148574829102
step 12899, loss 1.6022061109542847
step 12909, loss 1.5786657333374023
step 12919, loss 1.553415298461914
step 12929, loss 1.5748066902160645
Average loss 1.6117268868705683 for epoch 54, took 386.32037258148193 sec
Accuracy 0.0034331951202796244 for epoch 54, took 80.05243301391602 sec
Epoch 55
step 12939, loss 1.3471823930740356
step 12949, loss 1.3897260427474976
step 12959, loss 1.4700677394866943
step 12969, loss 2.184342384338379
step 12979, loss 1.6101536750793457
step 12989, loss 1.638826608657837
step 12999, loss 1.7589775323867798
step 13009, loss 1.5872130393981934
step 13019, loss 1.5820372104644775
step 13029, loss 1.619520902633667
step 13039, loss 1.5668706893920898
step 13049, loss 1.6237106323242188
step 13059, loss 1.6518234014511108
step 13069, loss 1.596298336982727
step 13079, loss 1.5395538806915283
step 13089, loss 1.6449384689331055
step 13099, loss 1.5525262355804443
step 13109, loss 1.59873366355896
step 13119, loss 1.6017796993255615
step 13129, loss 1.62291419506073
step 13139, loss 1.6575958728790283
step 13149, loss 1.6747057437896729
step 13159, loss 1.6045609712600708
Average loss 1.6142012597176068 for epoch 55, took 386.0406906604767 sec
Accuracy 0.0034331951202796244 for epoch 55, took 80.29796576499939 sec
Epoch 56
step 13169, loss 1.4636634588241577
step 13179, loss 1.418095588684082
step 13189, loss 1.776931881904602
step 13199, loss 2.096284866333008
step 13209, loss 1.728452444076538
step 13219, loss 1.6105577945709229
step 13229, loss 1.5995628833770752
step 13239, loss 1.6246380805969238
step 13249, loss 1.5941752195358276
step 13259, loss 1.6354763507843018
step 13269, loss 1.5668625831604004
step 13279, loss 1.5725994110107422
step 13289, loss 1.5738517045974731
step 13299, loss 1.6446502208709717
step 13309, loss 1.6061689853668213
step 13319, loss 1.6009292602539062
step 13329, loss 1.5837258100509644
step 13339, loss 1.6104027032852173
step 13349, loss 1.6119651794433594
step 13359, loss 1.5796853303909302
step 13369, loss 1.6070330142974854
step 13379, loss 1.5852153301239014
Average loss 1.6076672338602835 for epoch 56, took 386.5317859649658 sec
Accuracy 0.0034021400178192037 for epoch 56, took 80.29413866996765 sec
Epoch 57
step 13389, loss 1.591742753982544
step 13399, loss 1.3879534006118774
step 13409, loss 1.4088947772979736
step 13419, loss 1.9382399320602417
step 13429, loss 1.4892854690551758
step 13439, loss 1.6565284729003906
step 13449, loss 1.5951075553894043
step 13459, loss 1.577440619468689
step 13469, loss 1.6050233840942383
step 13479, loss 1.671884536743164
step 13489, loss 1.6793912649154663
step 13499, loss 1.6468862295150757
step 13509, loss 1.5868099927902222
step 13519, loss 1.5953190326690674
step 13529, loss 1.616245985031128
step 13539, loss 1.649086833000183
step 13549, loss 1.7054035663604736
step 13559, loss 1.6727286577224731
step 13569, loss 1.5791795253753662
step 13579, loss 1.628678560256958
step 13589, loss 1.5393282175064087
step 13599, loss 1.5696392059326172
step 13609, loss 1.54762864112854
Average loss 1.6056484846692336 for epoch 57, took 387.5899214744568 sec
Accuracy 0.0034331951202796244 for epoch 57, took 80.18536376953125 sec
Epoch 58
step 13619, loss 1.4776830673217773
step 13629, loss 1.3298940658569336
step 13639, loss 1.6747397184371948
step 13649, loss 1.8378885984420776
step 13659, loss 1.6595394611358643
step 13669, loss 1.6046900749206543
step 13679, loss 1.6323204040527344
step 13689, loss 1.6284584999084473
step 13699, loss 1.6222232580184937
step 13709, loss 1.610417127609253
step 13719, loss 1.5744130611419678
step 13729, loss 1.5727007389068604
step 13739, loss 1.6046851873397827
step 13749, loss 1.6380690336227417
step 13759, loss 1.5829582214355469
step 13769, loss 1.6915165185928345
step 13779, loss 1.6551158428192139
step 13789, loss 1.6406919956207275
step 13799, loss 1.6258833408355713
step 13809, loss 1.5758103132247925
step 13819, loss 1.658602237701416
step 13829, loss 1.6436636447906494
step 13839, loss 1.6147756576538086
Average loss 1.6089214557095577 for epoch 58, took 387.3989109992981 sec
Accuracy 0.0034331951202796244 for epoch 58, took 80.25513553619385 sec
Epoch 59
step 13849, loss 1.3702754974365234
step 13859, loss 1.3862181901931763
step 13869, loss 1.6586745977401733
step 13879, loss 2.0029304027557373
step 13889, loss 1.6053088903427124
step 13899, loss 1.6554274559020996
step 13909, loss 1.6862120628356934
step 13919, loss 1.7169795036315918
step 13929, loss 1.6493937969207764
step 13939, loss 1.6430370807647705
step 13949, loss 1.600495457649231
step 13959, loss 1.6068564653396606
step 13969, loss 1.6012980937957764
step 13979, loss 1.6501054763793945
step 13989, loss 1.5119850635528564
step 13999, loss 1.6090080738067627
step 14009, loss 1.615231990814209
step 14019, loss 1.5912134647369385
step 14029, loss 1.621847152709961
step 14039, loss 1.5787873268127441
step 14049, loss 1.5998809337615967
step 14059, loss 1.5712320804595947
step 14069, loss 1.6057164669036865
Average loss 1.606464105739928 for epoch 59, took 387.6253626346588 sec
Accuracy 0.003337888081694195 for epoch 59, took 80.01283288002014 sec
Epoch 60
step 14079, loss 1.454906702041626
step 14089, loss 1.4049913883209229
step 14099, loss 1.6298003196716309
step 14109, loss 1.9662432670593262
step 14119, loss 1.6684424877166748
step 14129, loss 1.764350175857544
step 14139, loss 1.7186740636825562
step 14149, loss 1.6315146684646606
step 14159, loss 1.6587274074554443
step 14169, loss 1.629185676574707
step 14179, loss 1.591278314590454
step 14189, loss 1.5580098628997803
step 14199, loss 1.6015849113464355
step 14209, loss 1.608931303024292
step 14219, loss 1.6129931211471558
step 14229, loss 1.5986249446868896
step 14239, loss 1.635329008102417
step 14249, loss 1.6226125955581665
step 14259, loss 1.5439932346343994
step 14269, loss 1.5846896171569824
step 14279, loss 1.6455967426300049
step 14289, loss 1.61763334274292
step 14299, loss 1.5672866106033325
Average loss 1.605989746880113 for epoch 60, took 387.7239410877228 sec
Accuracy 0.0034331951202796244 for epoch 60, took 79.95632886886597 sec
Epoch 61
step 14309, loss 1.4872465133666992
step 14319, loss 1.3903682231903076
step 14329, loss 1.6415302753448486
step 14339, loss 1.924870252609253
step 14349, loss 1.7441892623901367
step 14359, loss 1.5820492506027222
step 14369, loss 1.615734577178955
step 14379, loss 1.5645657777786255
step 14389, loss 1.6235508918762207
step 14399, loss 1.5380198955535889
step 14409, loss 1.6746752262115479
step 14419, loss 1.6156595945358276
step 14429, loss 1.6408178806304932
step 14439, loss 1.6299254894256592
step 14449, loss 1.5503432750701904
step 14459, loss 1.6269562244415283
step 14469, loss 1.630277156829834
step 14479, loss 1.5814716815948486
step 14489, loss 1.6372201442718506
step 14499, loss 1.6742881536483765
step 14509, loss 1.6241282224655151
step 14519, loss 1.5951268672943115
Average loss 1.6067316605333697 for epoch 61, took 387.5777094364166 sec
Accuracy 0.0034331951202796244 for epoch 61, took 80.04636406898499 sec
Epoch 62
step 14529, loss 1.6497300863265991
step 14539, loss 1.3946094512939453
step 14549, loss 1.3459646701812744
step 14559, loss 1.593667984008789
step 14569, loss 1.7503000497817993
step 14579, loss 1.6840485334396362
step 14589, loss 1.7357630729675293
step 14599, loss 1.721573829650879
step 14609, loss 1.5674800872802734
step 14619, loss 1.611534833908081
step 14629, loss 1.5975463390350342
step 14639, loss 1.629941701889038
step 14649, loss 1.569121241569519
step 14659, loss 1.5937939882278442
step 14669, loss 1.6177120208740234
step 14679, loss 1.6689250469207764
step 14689, loss 1.6388263702392578
step 14699, loss 1.6523571014404297
step 14709, loss 1.6347733736038208
step 14719, loss 1.685253620147705
step 14729, loss 1.6693286895751953
step 14739, loss 1.615262746810913
step 14749, loss 1.6348750591278076
Average loss 1.614882984182291 for epoch 62, took 387.6163647174835 sec
Accuracy 0.0034331951202796244 for epoch 62, took 80.03561544418335 sec
Epoch 63
step 14759, loss 1.532716989517212
step 14769, loss 1.3467847108840942
step 14779, loss 1.4978619813919067
step 14789, loss 2.276386022567749
step 14799, loss 2.1653666496276855
step 14809, loss 1.7826650142669678
step 14819, loss 1.704343557357788
step 14829, loss 1.7486823797225952
step 14839, loss 1.6441442966461182
step 14849, loss 1.6067081689834595
step 14859, loss 1.5582332611083984
step 14869, loss 1.6704185009002686
step 14879, loss 1.5641335248947144
step 14889, loss 1.6149985790252686
step 14899, loss 1.732062816619873
step 14909, loss 1.5582506656646729
step 14919, loss 1.6083377599716187
step 14929, loss 1.6695469617843628
step 14939, loss 1.6047632694244385
step 14949, loss 1.5588901042938232
step 14959, loss 1.5915833711624146
step 14969, loss 1.6260931491851807
step 14979, loss 1.6868090629577637
Average loss 1.6230309412144779 for epoch 63, took 387.4920082092285 sec
Accuracy 0.003337888081694195 for epoch 63, took 80.2310221195221 sec
Epoch 64
step 14989, loss 1.3978335857391357
step 14999, loss 1.42886483669281
step 15009, loss 1.4596203565597534
step 15019, loss 1.8126003742218018
step 15029, loss 1.65885591506958
step 15039, loss 1.643758773803711
step 15049, loss 1.6100609302520752
step 15059, loss 1.5454117059707642
step 15069, loss 1.6525709629058838
step 15079, loss 1.663156270980835
step 15089, loss 1.6337559223175049
step 15099, loss 1.6489574909210205
step 15109, loss 1.5629596710205078
step 15119, loss 1.6194868087768555
step 15129, loss 1.5955476760864258
step 15139, loss 1.598473310470581
step 15149, loss 1.5976414680480957
step 15159, loss 1.622331976890564
step 15169, loss 1.6305699348449707
step 15179, loss 1.6695215702056885
step 15189, loss 1.5919901132583618
step 15199, loss 1.6472547054290771
step 15209, loss 1.6334789991378784
Average loss 1.6103746781223698 for epoch 64, took 387.56609582901 sec
Accuracy 0.0034021400178192037 for epoch 64, took 80.01443028450012 sec
Epoch 65
step 15219, loss 1.393585205078125
step 15229, loss 1.4052951335906982
step 15239, loss 1.6824698448181152
step 15249, loss 1.8255772590637207
step 15259, loss 1.7561964988708496
step 15269, loss 1.6813572645187378
step 15279, loss 1.6524738073349
step 15289, loss 1.6172387599945068
step 15299, loss 1.6500890254974365
step 15309, loss 1.4957826137542725
step 15319, loss 1.6300735473632812
step 15329, loss 1.6654911041259766
step 15339, loss 1.6189393997192383
step 15349, loss 1.6614916324615479
step 15359, loss 1.6248418092727661
step 15369, loss 1.6060564517974854
step 15379, loss 1.6038682460784912
step 15389, loss 1.6538622379302979
step 15399, loss 1.6247652769088745
step 15409, loss 1.5914127826690674
step 15419, loss 1.5787543058395386
step 15429, loss 1.6053907871246338
step 15439, loss 1.5825546979904175
Average loss 1.602973292794144 for epoch 65, took 387.66015791893005 sec
Accuracy 0.0034331951202796244 for epoch 65, took 80.12763905525208 sec
Epoch 66
step 15449, loss 1.322439193725586
step 15459, loss 1.3790608644485474
step 15469, loss 1.8625694513320923
step 15479, loss 1.57837975025177
step 15489, loss 1.7198359966278076
step 15499, loss 1.7216764688491821
step 15509, loss 1.6207107305526733
step 15519, loss 1.59914231300354
step 15529, loss 1.597777247428894
step 15539, loss 1.6603106260299683
step 15549, loss 1.6691228151321411
step 15559, loss 1.63795006275177
step 15569, loss 1.6235668659210205
step 15579, loss 1.6147953271865845
step 15589, loss 1.558760643005371
step 15599, loss 1.5312917232513428
step 15609, loss 1.546893835067749
step 15619, loss 1.5724531412124634
step 15629, loss 1.5961687564849854
step 15639, loss 1.5833146572113037
step 15649, loss 1.6239380836486816
step 15659, loss 1.526688575744629
Average loss 1.6078455463836068 for epoch 66, took 386.28155755996704 sec
Accuracy 0.0034331951202796244 for epoch 66, took 80.1754240989685 sec
Epoch 67
step 15669, loss 1.6222126483917236
step 15679, loss 1.4510759115219116
step 15689, loss 1.3978955745697021
step 15699, loss 1.785179615020752
step 15709, loss 1.8013899326324463
step 15719, loss 1.6438804864883423
step 15729, loss 1.635117769241333
step 15739, loss 1.581843614578247
step 15749, loss 1.586423397064209
step 15759, loss 1.6340365409851074
step 15769, loss 1.6327342987060547
step 15779, loss 1.6187227964401245
step 15789, loss 1.6116210222244263
step 15799, loss 1.5772640705108643
step 15809, loss 1.6275187730789185
step 15819, loss 1.5778007507324219
step 15829, loss 1.6085622310638428
step 15839, loss 1.6005913019180298
step 15849, loss 1.6041570901870728
step 15859, loss 1.5976574420928955
step 15869, loss 1.5917298793792725
step 15879, loss 1.643616795539856
step 15889, loss 1.6243102550506592
Average loss 1.6144070102457415 for epoch 67, took 386.0561521053314 sec
Accuracy 0.0034021400178192037 for epoch 67, took 80.18809580802917 sec
Epoch 68
step 15899, loss 1.4059447050094604
step 15909, loss 1.4090075492858887
step 15919, loss 1.385509967803955
step 15929, loss 1.8613619804382324
step 15939, loss 1.6373714208602905
step 15949, loss 1.6332430839538574
step 15959, loss 1.6158925294876099
step 15969, loss 1.5943686962127686
step 15979, loss 1.6094872951507568
step 15989, loss 1.5865503549575806
step 15999, loss 1.5480326414108276
step 16009, loss 1.6106088161468506
step 16019, loss 1.5703277587890625
step 16029, loss 1.5871257781982422
step 16039, loss 1.5820809602737427
step 16049, loss 1.5577656030654907
step 16059, loss 1.6004951000213623
step 16069, loss 1.5654362440109253
step 16079, loss 1.6047146320343018
step 16089, loss 1.5962687730789185
step 16099, loss 1.6264851093292236
step 16109, loss 1.5950098037719727
step 16119, loss 1.6093450784683228
Average loss 1.6082464252647601 for epoch 68, took 386.0700957775116 sec
Accuracy 0.0034331951202796244 for epoch 68, took 80.11441230773926 sec
Epoch 69
step 16129, loss 1.3801392316818237
step 16139, loss 1.376099944114685
step 16149, loss 1.503618597984314
step 16159, loss 1.6192495822906494
step 16169, loss 1.6096833944320679
step 16179, loss 1.712924599647522
step 16189, loss 1.6139166355133057
step 16199, loss 1.6196048259735107
step 16209, loss 1.6426087617874146
step 16219, loss 1.6318247318267822
step 16229, loss 1.6081018447875977
step 16239, loss 1.6283583641052246
step 16249, loss 1.630065679550171
step 16259, loss 1.6380391120910645
step 16269, loss 1.576053500175476
step 16279, loss 1.5745800733566284
step 16289, loss 1.618615746498108
step 16299, loss 1.5975823402404785
step 16309, loss 1.60524582862854
step 16319, loss 1.588148832321167
step 16329, loss 1.6769996881484985
step 16339, loss 1.5388355255126953
step 16349, loss 1.630079746246338
Average loss 1.6090551468363978 for epoch 69, took 386.0374720096588 sec
Accuracy 0.0034331951202796244 for epoch 69, took 80.18137335777283 sec
Epoch 70
step 16359, loss 1.5329344272613525
step 16369, loss 1.4072043895721436
step 16379, loss 1.6468348503112793
step 16389, loss 1.8622925281524658
step 16399, loss 1.6525907516479492
step 16409, loss 1.810581922531128
step 16419, loss 1.6188833713531494
step 16429, loss 1.5784621238708496
step 16439, loss 1.6127153635025024
step 16449, loss 1.5650792121887207
step 16459, loss 1.6202270984649658
step 16469, loss 1.6198530197143555
step 16479, loss 1.564767599105835
step 16489, loss 1.587448239326477
step 16499, loss 1.6049782037734985
step 16509, loss 1.5804147720336914
step 16519, loss 1.5766894817352295
step 16529, loss 1.6408873796463013
step 16539, loss 1.619842529296875
step 16549, loss 1.619633436203003
step 16559, loss 1.6158833503723145
step 16569, loss 1.567392110824585
step 16579, loss 1.7031054496765137
Average loss 1.608958670444656 for epoch 70, took 386.09717655181885 sec
Accuracy 0.0034331951202796244 for epoch 70, took 80.19237279891968 sec
Epoch 71
step 16589, loss 1.4030603170394897
step 16599, loss 1.4316883087158203
step 16609, loss 1.5055402517318726
step 16619, loss 1.7962501049041748
step 16629, loss 1.7304095029830933
step 16639, loss 1.7799046039581299
step 16649, loss 1.6157094240188599
step 16659, loss 1.5551788806915283
step 16669, loss 1.642228364944458
step 16679, loss 1.6114387512207031
step 16689, loss 1.660983920097351
step 16699, loss 1.567905068397522
step 16709, loss 1.6110386848449707
step 16719, loss 1.6299188137054443
step 16729, loss 1.594599723815918
step 16739, loss 1.611783504486084
step 16749, loss 1.650932788848877
step 16759, loss 1.5589265823364258
step 16769, loss 1.6208797693252563
step 16779, loss 1.6224608421325684
step 16789, loss 1.5991485118865967
step 16799, loss 1.5867242813110352
Average loss 1.6107279815171893 for epoch 71, took 386.0857422351837 sec
Accuracy 0.0034331951202796244 for epoch 71, took 80.32015132904053 sec
Epoch 72
step 16809, loss 1.5695335865020752
step 16819, loss 1.5334765911102295
step 16829, loss 1.3396533727645874
step 16839, loss 1.8837542533874512
step 16849, loss 1.8784428834915161
step 16859, loss 1.6811754703521729
step 16869, loss 1.6487081050872803
step 16879, loss 1.5523405075073242
step 16889, loss 1.6350802183151245
step 16899, loss 1.6060267686843872
step 16909, loss 1.5767508745193481
step 16919, loss 1.64230215549469
step 16929, loss 1.5630624294281006
step 16939, loss 1.5964531898498535
step 16949, loss 1.5606739521026611
step 16959, loss 1.5930390357971191
step 16969, loss 1.6161085367202759
step 16979, loss 1.6425480842590332
step 16989, loss 1.596538782119751
step 16999, loss 1.57663893699646
step 17009, loss 1.6060426235198975
step 17019, loss 1.5705349445343018
step 17029, loss 1.6056156158447266
Average loss 1.6090651734879142 for epoch 72, took 385.98265528678894 sec
Accuracy 0.0034331951202796244 for epoch 72, took 80.15518641471863 sec
Epoch 73
step 17039, loss 1.5178028345108032
step 17049, loss 1.5195763111114502
step 17059, loss 1.4842417240142822
step 17069, loss 2.1555886268615723
step 17079, loss 1.485753059387207
step 17089, loss 1.6051520109176636
step 17099, loss 1.6519521474838257
step 17109, loss 1.6237363815307617
step 17119, loss 1.6529006958007812
step 17129, loss 1.587117075920105
step 17139, loss 1.6424691677093506
step 17149, loss 1.6422319412231445
step 17159, loss 1.599163293838501
step 17169, loss 1.6613918542861938
step 17179, loss 1.6709895133972168
step 17189, loss 1.6965968608856201
step 17199, loss 1.5530122518539429
step 17209, loss 1.5805320739746094
step 17219, loss 1.6055011749267578
step 17229, loss 1.6344701051712036
step 17239, loss 1.5953142642974854
step 17249, loss 1.581687331199646
step 17259, loss 1.6972215175628662
Average loss 1.6207633431543385 for epoch 73, took 386.0698449611664 sec
Accuracy 0.0034331951202796244 for epoch 73, took 79.96789312362671 sec
Epoch 74
step 17269, loss 1.4859769344329834
step 17279, loss 1.410771369934082
step 17289, loss 1.744234323501587
step 17299, loss 1.9632052183151245
step 17309, loss 1.5874114036560059
step 17319, loss 1.6290888786315918
step 17329, loss 1.6466623544692993
step 17339, loss 1.6257827281951904
step 17349, loss 1.6275262832641602
step 17359, loss 1.6810417175292969
step 17369, loss 1.577620029449463
step 17379, loss 1.6817364692687988
step 17389, loss 1.6046816110610962
step 17399, loss 1.5894956588745117
step 17409, loss 1.7398617267608643
step 17419, loss 1.7225346565246582
step 17429, loss 1.6184160709381104
step 17439, loss 1.5544153451919556
step 17449, loss 1.6009995937347412
step 17459, loss 1.6276025772094727
step 17469, loss 1.580777645111084
step 17479, loss 1.5761876106262207
step 17489, loss 1.632460355758667
Average loss 1.6180281372446763 for epoch 74, took 386.06483006477356 sec
Accuracy 0.0030765968747858267 for epoch 74, took 80.1595938205719 sec
Epoch 75
step 17499, loss 1.3776845932006836
step 17509, loss 1.392155408859253
step 17519, loss 1.5850567817687988
step 17529, loss 1.8951432704925537
step 17539, loss 1.611517310142517
step 17549, loss 1.6274479627609253
step 17559, loss 1.656454086303711
step 17569, loss 1.6048426628112793
step 17579, loss 1.6138478517532349
step 17589, loss 1.5533626079559326
step 17599, loss 1.5714112520217896
step 17609, loss 1.5721893310546875
step 17619, loss 1.5996425151824951
step 17629, loss 1.611276388168335
step 17639, loss 1.5352940559387207
step 17649, loss 1.633354902267456
step 17659, loss 1.6507941484451294
step 17669, loss 1.603057861328125
step 17679, loss 1.6306452751159668
step 17689, loss 1.559934139251709
step 17699, loss 1.6371127367019653
step 17709, loss 1.5881037712097168
step 17719, loss 1.5462695360183716
Average loss 1.6070918200308817 for epoch 75, took 386.20687341690063 sec
Accuracy 0.0034331951202796244 for epoch 75, took 80.20062160491943 sec
Epoch 76
step 17729, loss 1.3618561029434204
step 17739, loss 1.3311609029769897
step 17749, loss 1.6018049716949463
step 17759, loss 2.2384092807769775
step 17769, loss 1.7896418571472168
step 17779, loss 1.7243140935897827
step 17789, loss 1.6761658191680908
step 17799, loss 1.603881597518921
step 17809, loss 1.509063482284546
step 17819, loss 1.5804070234298706
step 17829, loss 1.5539311170578003
step 17839, loss 1.6042487621307373
step 17849, loss 1.549058198928833
step 17859, loss 1.615689992904663
step 17869, loss 1.614726185798645
step 17879, loss 1.648299217224121
step 17889, loss 1.6168155670166016
step 17899, loss 1.679661512374878
step 17909, loss 1.6107678413391113
step 17919, loss 1.5940842628479004
step 17929, loss 1.6350089311599731
step 17939, loss 1.6374406814575195
Average loss 1.6200535051655351 for epoch 76, took 386.3532512187958 sec
Accuracy 0.0034331951202796244 for epoch 76, took 79.94754266738892 sec
Epoch 77
step 17949, loss 1.6193827390670776
step 17959, loss 1.3373234272003174
step 17969, loss 1.35433030128479
step 17979, loss 1.871640682220459
step 17989, loss 1.5829346179962158
step 17999, loss 1.7163734436035156
step 18009, loss 1.6220836639404297
step 18019, loss 1.5868797302246094
step 18029, loss 1.6420105695724487
step 18039, loss 1.7069742679595947
step 18049, loss 1.5623527765274048
step 18059, loss 1.5811524391174316
step 18069, loss 1.5986568927764893
step 18079, loss 1.5885415077209473
step 18089, loss 1.6555049419403076
step 18099, loss 1.6451160907745361
step 18109, loss 1.643540620803833
step 18119, loss 1.6077274084091187
step 18129, loss 1.632927417755127
step 18139, loss 1.605945348739624
step 18149, loss 1.615032434463501
step 18159, loss 1.6352537870407104
step 18169, loss 1.6053743362426758
Average loss 1.6129844167776275 for epoch 77, took 386.1535620689392 sec
Accuracy 0.0034331951202796244 for epoch 77, took 80.13403058052063 sec
Epoch 78
step 18179, loss 1.5426552295684814
step 18189, loss 1.3598910570144653
step 18199, loss 1.650868535041809
step 18209, loss 1.8023130893707275
step 18219, loss 1.7737889289855957
step 18229, loss 1.7677628993988037
step 18239, loss 1.8196271657943726
step 18249, loss 1.6227279901504517
step 18259, loss 1.6662192344665527
step 18269, loss 1.6394362449645996
step 18279, loss 1.5764217376708984
step 18289, loss 1.6318031549453735
step 18299, loss 1.6105031967163086
step 18309, loss 1.5757801532745361
step 18319, loss 1.61838698387146
step 18329, loss 1.5831191539764404
step 18339, loss 1.6505635976791382
step 18349, loss 1.6484160423278809
step 18359, loss 1.682633399963379
step 18369, loss 1.5753601789474487
step 18379, loss 1.6252188682556152
step 18389, loss 1.707195520401001
step 18399, loss 1.6285673379898071
Average loss 1.6192936034579026 for epoch 78, took 386.2117991447449 sec
Accuracy 0.0034331951202796244 for epoch 78, took 79.94394087791443 sec
Epoch 79
step 18409, loss 1.4475815296173096
step 18419, loss 1.4168007373809814
step 18429, loss 1.669420838356018
step 18439, loss 1.559118390083313
step 18449, loss 1.5772573947906494
step 18459, loss 1.6781147718429565
step 18469, loss 1.6676470041275024
step 18479, loss 1.4978859424591064
step 18489, loss 1.5993272066116333
step 18499, loss 1.5143280029296875
step 18509, loss 1.5863707065582275
step 18519, loss 1.629560947418213
step 18529, loss 1.578582525253296
step 18539, loss 1.5644705295562744
step 18549, loss 1.6737675666809082
step 18559, loss 1.5576984882354736
step 18569, loss 1.5938084125518799
step 18579, loss 1.5902748107910156
step 18589, loss 1.5745865106582642
step 18599, loss 1.6503452062606812
step 18609, loss 1.5521749258041382
step 18619, loss 1.6287291049957275
step 18629, loss 1.6877551078796387
Average loss 1.6196405552981192 for epoch 79, took 386.115647315979 sec
Accuracy 0.003337888081694195 for epoch 79, took 80.13255620002747 sec
Epoch 80
step 18639, loss 1.446010947227478
step 18649, loss 1.4674596786499023
step 18659, loss 1.765312671661377
step 18669, loss 1.7413184642791748
step 18679, loss 1.6299173831939697
step 18689, loss 1.6693673133850098
step 18699, loss 1.5788805484771729
step 18709, loss 1.5872658491134644
step 18719, loss 1.5924640893936157
step 18729, loss 1.6073189973831177
step 18739, loss 1.6740574836730957
step 18749, loss 1.5576810836791992
step 18759, loss 1.6172235012054443
step 18769, loss 1.534300446510315
step 18779, loss 1.5605943202972412
step 18789, loss 1.6057751178741455
step 18799, loss 1.6808533668518066
step 18809, loss 1.5834951400756836
step 18819, loss 1.621385097503662
step 18829, loss 1.5641050338745117
step 18839, loss 1.5658233165740967
step 18849, loss 1.5663361549377441
step 18859, loss 1.5900663137435913
Average loss 1.601007248225965 for epoch 80, took 386.23864245414734 sec
Accuracy 0.0034331951202796244 for epoch 80, took 80.13211631774902 sec
Epoch 81
step 18869, loss 1.4899827241897583
step 18879, loss 1.402127742767334
step 18889, loss 1.5914491415023804
step 18899, loss 2.0943965911865234
step 18909, loss 1.6270580291748047
step 18919, loss 1.633805751800537
step 18929, loss 1.6048364639282227
step 18939, loss 1.6130926609039307
step 18949, loss 1.6614353656768799
step 18959, loss 1.5518755912780762
step 18969, loss 1.6014248132705688
step 18979, loss 1.6163123846054077
step 18989, loss 1.6223387718200684
step 18999, loss 1.6847493648529053
step 19009, loss 1.5779144763946533
step 19019, loss 1.6315596103668213
step 19029, loss 1.5114244222640991
step 19039, loss 1.5890696048736572
step 19049, loss 1.6705255508422852
step 19059, loss 1.6228208541870117
step 19069, loss 1.6162245273590088
step 19079, loss 1.615997552871704
Average loss 1.611935628610745 for epoch 81, took 386.0563545227051 sec
Accuracy 0.0034331951202796244 for epoch 81, took 80.08675265312195 sec
Epoch 82
step 19089, loss 1.6925933361053467
step 19099, loss 1.3040910959243774
step 19109, loss 1.4236538410186768
step 19119, loss 1.6227412223815918
step 19129, loss 1.8990769386291504
step 19139, loss 1.7428193092346191
step 19149, loss 1.702070951461792
step 19159, loss 1.6895201206207275
step 19169, loss 1.5328212976455688
step 19179, loss 1.624784231185913
step 19189, loss 1.6182972192764282
step 19199, loss 1.6018669605255127
step 19209, loss 1.6124279499053955
step 19219, loss 1.617452621459961
step 19229, loss 1.6197125911712646
step 19239, loss 1.6081904172897339
step 19249, loss 1.6332811117172241
step 19259, loss 1.619943380355835
step 19269, loss 1.5571799278259277
step 19279, loss 1.6318163871765137
step 19289, loss 1.5670443773269653
step 19299, loss 1.7041995525360107
step 19309, loss 1.6369953155517578
Average loss 1.6160389524802827 for epoch 82, took 386.13618659973145 sec
Accuracy 0.0034331951202796244 for epoch 82, took 80.1031608581543 sec
Epoch 83
step 19319, loss 1.5338566303253174
step 19329, loss 1.3569650650024414
step 19339, loss 1.371654987335205
step 19349, loss 1.5978469848632812
step 19359, loss 1.7550958395004272
step 19369, loss 1.7436692714691162
step 19379, loss 1.647350788116455
step 19389, loss 1.5739282369613647
step 19399, loss 1.5600190162658691
step 19409, loss 1.6161584854125977
step 19419, loss 1.6010923385620117
step 19429, loss 1.7015433311462402
step 19439, loss 1.5865302085876465
step 19449, loss 1.6065073013305664
step 19459, loss 1.609649419784546
step 19469, loss 1.582167148590088
step 19479, loss 1.5916651487350464
step 19489, loss 1.581437587738037
step 19499, loss 1.6108307838439941
step 19509, loss 1.676605463027954
step 19519, loss 1.625902771949768
step 19529, loss 1.6470067501068115
step 19539, loss 1.5653541088104248
Average loss 1.6177305579185486 for epoch 83, took 386.27084398269653 sec
Accuracy 0.0030765968747858267 for epoch 83, took 79.98340129852295 sec
Epoch 84
step 19549, loss 1.3828966617584229
step 19559, loss 1.3663785457611084
step 19569, loss 1.4976632595062256
step 19579, loss 2.087994337081909
step 19589, loss 1.5427323579788208
step 19599, loss 1.5968828201293945
step 19609, loss 1.612873911857605
step 19619, loss 1.5799418687820435
step 19629, loss 1.6194486618041992
step 19639, loss 1.6408771276474
step 19649, loss 1.5920010805130005
step 19659, loss 1.6100059747695923
step 19669, loss 1.7214454412460327
step 19679, loss 1.5676475763320923
step 19689, loss 1.6720924377441406
step 19699, loss 1.6044559478759766
step 19709, loss 1.720524549484253
step 19719, loss 1.6251559257507324
step 19729, loss 1.6075748205184937
step 19739, loss 1.6217011213302612
step 19749, loss 1.6364338397979736
step 19759, loss 1.6107454299926758
step 19769, loss 1.6318913698196411
Average loss 1.6049919970202864 for epoch 84, took 386.30235958099365 sec
Accuracy 0.0034331951202796244 for epoch 84, took 79.94746541976929 sec
Epoch 85
step 19779, loss 1.3841055631637573
step 19789, loss 1.3687692880630493
step 19799, loss 1.7104960680007935
step 19809, loss 1.9385842084884644
step 19819, loss 1.670780897140503
step 19829, loss 1.6085858345031738
step 19839, loss 1.659299373626709
step 19849, loss 1.6167051792144775
step 19859, loss 1.6015756130218506
step 19869, loss 1.5813193321228027
step 19879, loss 1.5948604345321655
step 19889, loss 1.561935544013977
step 19899, loss 1.637631893157959
step 19909, loss 1.6246671676635742
step 19919, loss 1.5646779537200928
step 19929, loss 1.598273754119873
step 19939, loss 1.609394907951355
step 19949, loss 1.6605366468429565
step 19959, loss 1.5778371095657349
step 19969, loss 1.6351618766784668
step 19979, loss 1.7199419736862183
step 19989, loss 1.6281200647354126
step 19999, loss 1.5746408700942993
Average loss 1.6110463921438183 for epoch 85, took 381.35800528526306 sec
Accuracy 0.0030765968747858267 for epoch 85, took 80.15895128250122 sec
Epoch 86
step 20009, loss 1.3480677604675293
step 20019, loss 1.3580706119537354
step 20029, loss 1.677431344985962
step 20039, loss 1.649144172668457
step 20049, loss 1.8251924514770508
step 20059, loss 1.844842791557312
step 20069, loss 1.6782065629959106
step 20079, loss 1.662827968597412
step 20089, loss 1.612380027770996
step 20099, loss 1.6080374717712402
step 20109, loss 1.6340922117233276
step 20119, loss 1.6194363832473755
step 20129, loss 1.6163228750228882
step 20139, loss 1.5703203678131104
step 20149, loss 1.6640782356262207
step 20159, loss 1.598356008529663
step 20169, loss 1.5387828350067139
step 20179, loss 1.5998313426971436
step 20189, loss 1.625823974609375
step 20199, loss 1.7187070846557617
step 20209, loss 1.6191602945327759
step 20219, loss 1.5899158716201782
Average loss 1.6099799442709537 for epoch 86, took 381.5123040676117 sec
Accuracy 0.0030765968747858267 for epoch 86, took 80.24314904212952 sec
Epoch 87
step 20229, loss 1.5098788738250732
step 20239, loss 1.3929252624511719
step 20249, loss 1.419774055480957
step 20259, loss 1.5674997568130493
step 20269, loss 1.6747894287109375
step 20279, loss 1.6176626682281494
step 20289, loss 1.6122010946273804
step 20299, loss 1.5926997661590576
step 20309, loss 1.6083853244781494
step 20319, loss 1.5695865154266357
step 20329, loss 1.5971174240112305
step 20339, loss 1.5508569478988647
step 20349, loss 1.5789364576339722
step 20359, loss 1.6235677003860474
step 20369, loss 1.6495707035064697
step 20379, loss 1.711449384689331
step 20389, loss 1.6437543630599976
step 20399, loss 1.6493723392486572
step 20409, loss 1.6271644830703735
step 20419, loss 1.5505976676940918
step 20429, loss 1.6812348365783691
step 20439, loss 1.6051661968231201
step 20449, loss 1.676820993423462
Average loss 1.6021724034819687 for epoch 87, took 381.45105051994324 sec
Accuracy 0.0030765968747858267 for epoch 87, took 80.26670527458191 sec
Epoch 88
step 20459, loss 1.39801025390625
step 20469, loss 1.3589715957641602
step 20479, loss 1.6322650909423828
step 20489, loss 2.2997260093688965
step 20499, loss 1.6567871570587158
step 20509, loss 1.6371345520019531
step 20519, loss 1.61433744430542
step 20529, loss 1.5886040925979614
step 20539, loss 1.6676104068756104
step 20549, loss 1.6038599014282227
step 20559, loss 1.5462102890014648
step 20569, loss 1.5789769887924194
step 20579, loss 1.6322519779205322
step 20589, loss 1.6112531423568726
step 20599, loss 1.5358984470367432
step 20609, loss 1.5923449993133545
step 20619, loss 1.6415144205093384
step 20629, loss 1.639992117881775
step 20639, loss 1.6142947673797607
step 20649, loss 1.7221213579177856
step 20659, loss 1.6149359941482544
step 20669, loss 1.5648094415664673
step 20679, loss 1.6246428489685059
Average loss 1.6032688256941343 for epoch 88, took 381.47511172294617 sec
Accuracy 0.0034331951202796244 for epoch 88, took 80.21484756469727 sec
Epoch 89
step 20689, loss 1.30131995677948
step 20699, loss 1.3706612586975098
step 20709, loss 1.5610852241516113
step 20719, loss 1.8350889682769775
step 20729, loss 1.69370436668396
step 20739, loss 1.6281845569610596
step 20749, loss 1.6342236995697021
step 20759, loss 1.6452580690383911
step 20769, loss 1.610009789466858
step 20779, loss 1.6296031475067139
step 20789, loss 1.5774247646331787
step 20799, loss 1.648813247680664
step 20809, loss 1.5829412937164307
step 20819, loss 1.557563304901123
step 20829, loss 1.6541774272918701
step 20839, loss 1.611876368522644
step 20849, loss 1.5916980504989624
step 20859, loss 1.5363237857818604
step 20869, loss 1.6015046834945679
step 20879, loss 1.6257615089416504
step 20889, loss 1.5341455936431885
step 20899, loss 1.631683349609375
step 20909, loss 1.6218814849853516
Average loss 1.608611756249478 for epoch 89, took 381.51198625564575 sec
Accuracy 0.0034331951202796244 for epoch 89, took 80.33344030380249 sec
Epoch 90
step 20919, loss 1.4608972072601318
step 20929, loss 1.4011428356170654
step 20939, loss 1.7227953672409058
step 20949, loss 1.9549689292907715
step 20959, loss 1.778348684310913
step 20969, loss 1.6153085231781006
step 20979, loss 1.6001238822937012
step 20989, loss 1.614448070526123
step 20999, loss 1.6573596000671387
step 21009, loss 1.6586546897888184
step 21019, loss 1.6253759860992432
step 21029, loss 1.6348912715911865
step 21039, loss 1.630465030670166
step 21049, loss 1.6378121376037598
step 21059, loss 1.6056606769561768
step 21069, loss 1.5861735343933105
step 21079, loss 1.6870269775390625
step 21089, loss 1.6772412061691284
step 21099, loss 1.6002057790756226
step 21109, loss 1.6281044483184814
step 21119, loss 1.5485131740570068
step 21129, loss 1.631174921989441
step 21139, loss 1.6150548458099365
Average loss 1.6126225554106528 for epoch 90, took 381.41984581947327 sec
Accuracy 0.0034021400178192037 for epoch 90, took 80.03761100769043 sec
Epoch 91
step 21149, loss 1.4559240341186523
step 21159, loss 1.3491026163101196
step 21169, loss 1.5851385593414307
step 21179, loss 1.6671578884124756
step 21189, loss 1.7799184322357178
step 21199, loss 1.6888635158538818
step 21209, loss 1.627343773841858
step 21219, loss 1.5790183544158936
step 21229, loss 1.5688443183898926
step 21239, loss 1.6096923351287842
step 21249, loss 1.5405769348144531
step 21259, loss 1.625145673751831
step 21269, loss 1.5387673377990723
step 21279, loss 1.6032869815826416
step 21289, loss 1.5920360088348389
step 21299, loss 1.5800001621246338
step 21309, loss 1.6902759075164795
step 21319, loss 1.7044050693511963
step 21329, loss 1.5109379291534424
step 21339, loss 1.7115193605422974
step 21349, loss 1.5978041887283325
step 21359, loss 1.6013965606689453
Average loss 1.6084492849676233 for epoch 91, took 381.3799569606781 sec
Accuracy 0.0034331951202796244 for epoch 91, took 80.10966300964355 sec
Epoch 92
step 21369, loss 1.7950242757797241
step 21379, loss 1.4742640256881714
step 21389, loss 1.4013028144836426
step 21399, loss 1.7949453592300415
step 21409, loss 1.688420057296753
step 21419, loss 1.712144136428833
step 21429, loss 1.7054829597473145
step 21439, loss 1.6904617547988892
step 21449, loss 1.610280156135559
step 21459, loss 1.5164985656738281
step 21469, loss 1.625011920928955
step 21479, loss 1.609975814819336
step 21489, loss 1.5704345703125
step 21499, loss 1.6156506538391113
step 21509, loss 1.647024154663086
step 21519, loss 1.6687700748443604
step 21529, loss 1.5464835166931152
step 21539, loss 1.6794500350952148
step 21549, loss 1.5976841449737549
step 21559, loss 1.5790960788726807
step 21569, loss 1.6372578144073486
step 21579, loss 1.638991355895996
step 21589, loss 1.6109658479690552
Average loss 1.6196418728744775 for epoch 92, took 381.3834900856018 sec
Accuracy 0.0034331951202796244 for epoch 92, took 80.36313033103943 sec
Epoch 93
step 21599, loss 1.4775023460388184
step 21609, loss 1.403135061264038
step 21619, loss 1.3866963386535645
step 21629, loss 2.1144814491271973
step 21639, loss 1.750600814819336
step 21649, loss 1.6743788719177246
step 21659, loss 1.5834896564483643
step 21669, loss 1.5752085447311401
step 21679, loss 1.6242566108703613
step 21689, loss 1.6045911312103271
step 21699, loss 1.575615644454956
step 21709, loss 1.586533784866333
step 21719, loss 1.6321778297424316
step 21729, loss 1.6070938110351562
step 21739, loss 1.5877697467803955
step 21749, loss 1.5247719287872314
step 21759, loss 1.6148629188537598
step 21769, loss 1.6474425792694092
step 21779, loss 1.6095974445343018
step 21789, loss 1.627680778503418
step 21799, loss 1.6065722703933716
step 21809, loss 1.6015552282333374
step 21819, loss 1.6245520114898682
Average loss 1.622544505094227 for epoch 93, took 381.4315559864044 sec
Accuracy 0.0034331951202796244 for epoch 93, took 80.286625623703 sec
Epoch 94
step 21829, loss 1.3468797206878662
step 21839, loss 1.3998053073883057
step 21849, loss 1.6052583456039429
step 21859, loss 2.03169846534729
step 21869, loss 1.5236120223999023
step 21879, loss 1.6428742408752441
step 21889, loss 1.6081894636154175
step 21899, loss 1.6527032852172852
step 21909, loss 1.6632028818130493
step 21919, loss 1.5904146432876587
step 21929, loss 1.561476707458496
step 21939, loss 1.599047303199768
step 21949, loss 1.5615787506103516
step 21959, loss 1.6194138526916504
step 21969, loss 1.5920615196228027
step 21979, loss 1.6281079053878784
step 21989, loss 1.5560836791992188
step 21999, loss 1.6167404651641846
step 22009, loss 1.6509050130844116
step 22019, loss 1.6112327575683594
step 22029, loss 1.598941683769226
step 22039, loss 1.5975775718688965
step 22049, loss 1.6207334995269775
Average loss 1.6009887681718458 for epoch 94, took 383.55614161491394 sec
Accuracy 0.0034331951202796244 for epoch 94, took 79.93557167053223 sec
Epoch 95
step 22059, loss 1.4609334468841553
step 22069, loss 1.3867347240447998
step 22079, loss 1.6877634525299072
step 22089, loss 1.7467576265335083
step 22099, loss 1.682161569595337
step 22109, loss 1.612007975578308
step 22119, loss 1.6203820705413818
step 22129, loss 1.5446796417236328
step 22139, loss 1.591313123703003
step 22149, loss 1.6441606283187866
step 22159, loss 1.5595492124557495
step 22169, loss 1.614844560623169
step 22179, loss 1.6127805709838867
step 22189, loss 1.5766923427581787
step 22199, loss 1.634041666984558
step 22209, loss 1.5740597248077393
step 22219, loss 1.579852819442749
step 22229, loss 1.5650997161865234
step 22239, loss 1.5600976943969727
step 22249, loss 1.549934983253479
step 22259, loss 1.6335701942443848
step 22269, loss 1.5795512199401855
step 22279, loss 1.6210798025131226
Average loss 1.6090373961549056 for epoch 95, took 386.1890594959259 sec
Accuracy 0.0034331951202796244 for epoch 95, took 80.08276629447937 sec
Epoch 96
step 22289, loss 1.3508800268173218
step 22299, loss 1.5709537267684937
step 22309, loss 1.7584000825881958
step 22319, loss 1.667473316192627
step 22329, loss 1.6358487606048584
step 22339, loss 1.6262128353118896
step 22349, loss 1.6175113916397095
step 22359, loss 1.560683250427246
step 22369, loss 1.5948619842529297
step 22379, loss 1.7509434223175049
step 22389, loss 1.5997817516326904
step 22399, loss 1.6266567707061768
step 22409, loss 1.6197338104248047
step 22419, loss 1.5727249383926392
step 22429, loss 1.5933589935302734
step 22439, loss 1.657936692237854
step 22449, loss 1.6586997509002686
step 22459, loss 1.6134631633758545
step 22469, loss 1.5709640979766846
step 22479, loss 1.614527702331543
step 22489, loss 1.5388513803482056
step 22499, loss 1.6723127365112305
Average loss 1.6188746910346181 for epoch 96, took 386.26496505737305 sec
Accuracy 0.0034331951202796244 for epoch 96, took 80.26647996902466 sec
Epoch 97
step 22509, loss 1.7062442302703857
step 22519, loss 1.3731979131698608
step 22529, loss 1.3943431377410889
step 22539, loss 1.6700778007507324
step 22549, loss 2.0677361488342285
step 22559, loss 1.8574507236480713
step 22569, loss 1.6984202861785889
step 22579, loss 1.634153127670288
step 22589, loss 1.6239477396011353
step 22599, loss 1.6666098833084106
step 22609, loss 1.5893300771713257
step 22619, loss 1.6069496870040894
step 22629, loss 1.6037263870239258
step 22639, loss 1.6699614524841309
step 22649, loss 1.6047470569610596
step 22659, loss 1.5327281951904297
step 22669, loss 1.5975507497787476
step 22679, loss 1.6081316471099854
step 22689, loss 1.6465636491775513
step 22699, loss 1.6209111213684082
step 22709, loss 1.635899305343628
step 22719, loss 1.680006742477417
step 22729, loss 1.6169848442077637
Average loss 1.611007613048219 for epoch 97, took 386.1076979637146 sec
Accuracy 0.0034021400178192037 for epoch 97, took 80.20789504051208 sec
Epoch 98
step 22739, loss 1.6324751377105713
step 22749, loss 1.445235013961792
step 22759, loss 1.5092763900756836
step 22769, loss 1.4920079708099365
step 22779, loss 1.8781940937042236
step 22789, loss 1.6491448879241943
step 22799, loss 1.5777034759521484
step 22809, loss 1.5441137552261353
step 22819, loss 1.6714811325073242
step 22829, loss 1.6124897003173828
step 22839, loss 1.6225361824035645
step 22849, loss 1.6600728034973145
step 22859, loss 1.6051387786865234
step 22869, loss 1.6152918338775635
step 22879, loss 1.5921249389648438
step 22889, loss 1.6469829082489014
step 22899, loss 1.6032204627990723
step 22909, loss 1.5973844528198242
step 22919, loss 1.6096389293670654
step 22929, loss 1.6582062244415283
step 22939, loss 1.6240198612213135
step 22949, loss 1.6079332828521729
step 22959, loss 1.5949347019195557
Average loss 1.6116204497061277 for epoch 98, took 385.84026050567627 sec
Accuracy 0.0034331951202796244 for epoch 98, took 80.1865611076355 sec
Epoch 99
step 22969, loss 1.3668181896209717
step 22979, loss 1.3734734058380127
step 22989, loss 1.5805798768997192
step 22999, loss 2.3379745483398438
step 23009, loss 1.697129487991333
step 23019, loss 1.664576768875122
step 23029, loss 1.6129547357559204
step 23039, loss 1.5462706089019775
step 23049, loss 1.5731136798858643
step 23059, loss 1.567186713218689
step 23069, loss 1.5528085231781006
step 23079, loss 1.6142144203186035
step 23089, loss 1.5812174081802368
step 23099, loss 1.7145130634307861
step 23109, loss 1.5738193988800049
step 23119, loss 1.6077048778533936
step 23129, loss 1.6100380420684814
step 23139, loss 1.6241865158081055
step 23149, loss 1.6575651168823242
step 23159, loss 1.6026346683502197
step 23169, loss 1.6116318702697754
step 23179, loss 1.5840836763381958
step 23189, loss 1.6313247680664062
Average loss 1.6076256146556454 for epoch 99, took 386.14174938201904 sec
Accuracy 0.0034021400178192037 for epoch 99, took 80.25400686264038 sec
Epoch 100
step 23199, loss 1.3951971530914307
step 23209, loss 1.4059302806854248
step 23219, loss 1.558408498764038
step 23229, loss 1.9576114416122437
step 23239, loss 1.7890625
step 23249, loss 1.7144746780395508
step 23259, loss 1.663581371307373
step 23269, loss 1.6200625896453857
step 23279, loss 1.5889283418655396
step 23289, loss 1.5919017791748047
step 23299, loss 1.634461760520935
step 23309, loss 1.601233959197998
step 23319, loss 1.6095576286315918
step 23329, loss 1.6285932064056396
step 23339, loss 1.5978448390960693
step 23349, loss 1.5825625658035278
step 23359, loss 1.580506443977356
step 23369, loss 1.6129955053329468
step 23379, loss 1.5746983289718628
step 23389, loss 1.6390795707702637
step 23399, loss 1.6428030729293823
step 23409, loss 1.5216686725616455
step 23419, loss 1.6381744146347046
Average loss 1.6055067913574086 for epoch 100, took 386.1417589187622 sec
Accuracy 0.0034331951202796244 for epoch 100, took 80.19644713401794 sec
Epoch 101
step 23429, loss 1.5401761531829834
step 23439, loss 1.413834810256958
step 23449, loss 1.5897101163864136
step 23459, loss 1.7764077186584473
step 23469, loss 1.6246521472930908
step 23479, loss 1.5771853923797607
step 23489, loss 1.6295069456100464
step 23499, loss 1.5918266773223877
step 23509, loss 1.6793266534805298
step 23519, loss 1.7035856246948242
step 23529, loss 1.6123217344284058
step 23539, loss 1.6406846046447754
step 23549, loss 1.5995912551879883
step 23559, loss 1.6802027225494385
step 23569, loss 1.6021642684936523
step 23579, loss 1.525926947593689
step 23589, loss 1.5912511348724365
step 23599, loss 1.6513583660125732
step 23609, loss 1.608919620513916
step 23619, loss 1.6000230312347412
step 23629, loss 1.6081514358520508
step 23639, loss 1.6261374950408936
Average loss 1.611673061784945 for epoch 101, took 386.09700417518616 sec
Accuracy 0.0034021400178192037 for epoch 101, took 80.13671469688416 sec
Epoch 102
step 23649, loss 1.5543864965438843
step 23659, loss 1.3874216079711914
step 23669, loss 1.4595530033111572
step 23679, loss 1.5221853256225586
step 23689, loss 1.6768718957901
step 23699, loss 1.6008057594299316
step 23709, loss 1.6694308519363403
step 23719, loss 1.6322097778320312
step 23729, loss 1.5662171840667725
step 23739, loss 1.5827723741531372
step 23749, loss 1.5972092151641846
step 23759, loss 1.597517490386963
step 23769, loss 1.5978326797485352
step 23779, loss 1.634736180305481
step 23789, loss 1.5941176414489746
step 23799, loss 1.5846582651138306
step 23809, loss 1.6264441013336182
step 23819, loss 1.6235824823379517
step 23829, loss 1.5092896223068237
step 23839, loss 1.6098506450653076
step 23849, loss 1.619960904121399
step 23859, loss 1.5894695520401
step 23869, loss 1.622746467590332
Average loss 1.6018817309747662 for epoch 102, took 385.98868441581726 sec
Accuracy 0.0034331951202796244 for epoch 102, took 80.06710052490234 sec
Epoch 103
step 23879, loss 1.5364413261413574
step 23889, loss 1.280968189239502
step 23899, loss 1.4947316646575928
step 23909, loss 1.9804408550262451
step 23919, loss 1.8130358457565308
step 23929, loss 1.6026623249053955
step 23939, loss 1.5726127624511719
step 23949, loss 1.62532377243042
step 23959, loss 1.6438395977020264
step 23969, loss 1.59877610206604
step 23979, loss 1.576564073562622
step 23989, loss 1.586837887763977
step 23999, loss 1.5912938117980957
step 24009, loss 1.600938081741333
step 24019, loss 1.5904349088668823
step 24029, loss 1.6136691570281982
step 24039, loss 1.5691354274749756
step 24049, loss 1.7262825965881348
step 24059, loss 1.6819344758987427
step 24069, loss 1.6577708721160889
step 24079, loss 1.5967607498168945
step 24089, loss 1.6940277814865112
step 24099, loss 1.6312828063964844
Average loss 1.6181091540738155 for epoch 103, took 385.8417730331421 sec
Accuracy 0.003337888081694195 for epoch 103, took 80.0122652053833 sec
Epoch 104
step 24109, loss 1.344543695449829
step 24119, loss 1.4219565391540527
step 24129, loss 1.7216525077819824
step 24139, loss 1.725684642791748
step 24149, loss 1.7034708261489868
step 24159, loss 1.606682538986206
step 24169, loss 1.578099250793457
step 24179, loss 1.5729504823684692
step 24189, loss 1.588052749633789
step 24199, loss 1.5960888862609863
step 24209, loss 1.6433172225952148
step 24219, loss 1.5880937576293945
step 24229, loss 1.5650635957717896
step 24239, loss 1.646963357925415
step 24249, loss 1.5575370788574219
step 24259, loss 1.644866943359375
step 24269, loss 1.6594469547271729
step 24279, loss 1.5567007064819336
step 24289, loss 1.5775468349456787
step 24299, loss 1.6079962253570557
step 24309, loss 1.6046438217163086
step 24319, loss 1.6192548274993896
step 24329, loss 1.592907190322876
Average loss 1.593301118465892 for epoch 104, took 385.8785719871521 sec
Accuracy 0.0034331951202796244 for epoch 104, took 80.23073649406433 sec
Epoch 105
step 24339, loss 1.5198001861572266
step 24349, loss 1.456024169921875
step 24359, loss 1.6780235767364502
step 24369, loss 1.9075884819030762
step 24379, loss 1.6204822063446045
step 24389, loss 1.72196364402771
step 24399, loss 1.5291892290115356
step 24409, loss 1.6551947593688965
step 24419, loss 1.5225287675857544
step 24429, loss 1.6225216388702393
step 24439, loss 1.7240725755691528
step 24449, loss 1.6228086948394775
step 24459, loss 1.5800578594207764
step 24469, loss 1.624230146408081
step 24479, loss 1.5836008787155151
step 24489, loss 1.6363215446472168
step 24499, loss 1.6297855377197266
step 24509, loss 1.6507840156555176
step 24519, loss 1.67477285861969
step 24529, loss 1.628483533859253
step 24539, loss 1.6356974840164185
step 24549, loss 1.530318021774292
step 24559, loss 1.6472991704940796
Average loss 1.611685438637148 for epoch 105, took 386.1449201107025 sec
Accuracy 0.0034331951202796244 for epoch 105, took 80.00063610076904 sec
Epoch 106
step 24569, loss 1.511561632156372
step 24579, loss 1.4389550685882568
step 24589, loss 1.7155312299728394
step 24599, loss 1.658748984336853
step 24609, loss 1.7438218593597412
step 24619, loss 1.6695871353149414
step 24629, loss 1.6071808338165283
step 24639, loss 1.6920499801635742
step 24649, loss 1.5929627418518066
step 24659, loss 1.5846960544586182
step 24669, loss 1.6021780967712402
step 24679, loss 1.5477291345596313
step 24689, loss 1.638321042060852
step 24699, loss 1.6221275329589844
step 24709, loss 1.5640780925750732
step 24719, loss 1.6244045495986938
step 24729, loss 1.5752296447753906
step 24739, loss 1.5906085968017578
step 24749, loss 1.6301541328430176
step 24759, loss 1.704542875289917
step 24769, loss 1.5931406021118164
step 24779, loss 1.6860013008117676
Average loss 1.616326781218512 for epoch 106, took 386.0031955242157 sec
Accuracy 0.0034331951202796244 for epoch 106, took 80.16921544075012 sec
Epoch 107
step 24789, loss 1.5405491590499878
step 24799, loss 1.4625341892242432
step 24809, loss 1.4204988479614258
step 24819, loss 1.4773063659667969
step 24829, loss 1.9051659107208252
step 24839, loss 1.8169913291931152
step 24849, loss 1.666071891784668
step 24859, loss 1.70332932472229
step 24869, loss 1.5687975883483887
step 24879, loss 1.627288818359375
step 24889, loss 1.6129918098449707
step 24899, loss 1.569218397140503
step 24909, loss 1.6427850723266602
step 24919, loss 1.601920247077942
step 24929, loss 1.6575978994369507
step 24939, loss 1.6466583013534546
step 24949, loss 1.6288059949874878
step 24959, loss 1.6377649307250977
step 24969, loss 1.5713179111480713
step 24979, loss 1.5706360340118408
step 24989, loss 1.5888829231262207
step 24999, loss 1.5679481029510498
step 25009, loss 1.5492607355117798
Average loss 1.6155176847650294 for epoch 107, took 386.0627615451813 sec
Accuracy 0.0034331951202796244 for epoch 107, took 80.0734658241272 sec
Epoch 108
step 25019, loss 1.377641201019287
step 25029, loss 1.3834261894226074
step 25039, loss 1.669944405555725
step 25049, loss 1.6449332237243652
step 25059, loss 1.7239835262298584
step 25069, loss 1.5532948970794678
step 25079, loss 1.6873197555541992
step 25089, loss 1.5709068775177002
step 25099, loss 1.5451176166534424
step 25109, loss 1.6166753768920898
step 25119, loss 1.6284269094467163
step 25129, loss 1.5640873908996582
step 25139, loss 1.5687010288238525
step 25149, loss 1.6073598861694336
step 25159, loss 1.6444971561431885
step 25169, loss 1.6258604526519775
step 25179, loss 1.6862549781799316
step 25189, loss 1.6479679346084595
step 25199, loss 1.6236621141433716
step 25209, loss 1.613912582397461
step 25219, loss 1.6517434120178223
step 25229, loss 1.5877808332443237
step 25239, loss 1.749819278717041
Average loss 1.6112097276930224 for epoch 108, took 386.1304121017456 sec
Accuracy 0.0034331951202796244 for epoch 108, took 80.39131259918213 sec
Epoch 109
step 25249, loss 1.3289333581924438
step 25259, loss 1.4567803144454956
step 25269, loss 1.5147356986999512
step 25279, loss 1.6539056301116943
step 25289, loss 1.6121492385864258
step 25299, loss 1.552348256111145
step 25309, loss 1.6205360889434814
step 25319, loss 1.6109509468078613
step 25329, loss 1.7167420387268066
step 25339, loss 1.5809082984924316
step 25349, loss 1.6766793727874756
step 25359, loss 1.6198201179504395
step 25369, loss 1.623134970664978
step 25379, loss 1.5565708875656128
step 25389, loss 1.6130566596984863
step 25399, loss 1.622889518737793
step 25409, loss 1.759673833847046
step 25419, loss 1.5919824838638306
step 25429, loss 1.5833016633987427
step 25439, loss 1.6168285608291626
step 25449, loss 1.5409257411956787
step 25459, loss 1.564708948135376
step 25469, loss 1.5802569389343262
Average loss 1.6108283353479285 for epoch 109, took 386.05365562438965 sec
Accuracy 0.0034331951202796244 for epoch 109, took 79.980872631073 sec
Epoch 110
step 25479, loss 1.4462029933929443
step 25489, loss 1.366002082824707
step 25499, loss 1.6946048736572266
step 25509, loss 1.721572756767273
step 25519, loss 1.651555061340332
step 25529, loss 1.6291290521621704
step 25539, loss 1.6333986520767212
step 25549, loss 1.652622938156128
step 25559, loss 1.6399785280227661
step 25569, loss 1.6368746757507324
step 25579, loss 1.5867058038711548
step 25589, loss 1.5859240293502808
step 25599, loss 1.584254264831543
step 25609, loss 1.610966682434082
step 25619, loss 1.598310947418213
step 25629, loss 1.6354249715805054
step 25639, loss 1.681307315826416
step 25649, loss 1.4905579090118408
step 25659, loss 1.676356315612793
step 25669, loss 1.609752893447876
step 25679, loss 1.6687661409378052
step 25689, loss 1.564984917640686
step 25699, loss 1.636421799659729
Average loss 1.616396131745556 for epoch 110, took 386.09898829460144 sec
Accuracy 0.003337888081694195 for epoch 110, took 80.30794739723206 sec
Epoch 111
step 25709, loss 1.3592000007629395
step 25719, loss 1.391148328781128
step 25729, loss 1.8558318614959717
step 25739, loss 1.8379141092300415
step 25749, loss 1.8463586568832397
step 25759, loss 1.6797549724578857
step 25769, loss 1.5826473236083984
step 25779, loss 1.6090270280838013
step 25789, loss 1.5990231037139893
step 25799, loss 1.6013810634613037
step 25809, loss 1.593695044517517
step 25819, loss 1.64265775680542
step 25829, loss 1.5874954462051392
step 25839, loss 1.6411775350570679
step 25849, loss 1.6400108337402344
step 25859, loss 1.5533725023269653
step 25869, loss 1.6154322624206543
step 25879, loss 1.5012421607971191
step 25889, loss 1.619259238243103
step 25899, loss 1.6612783670425415
step 25909, loss 1.6556098461151123
step 25919, loss 1.7353274822235107
Average loss 1.609879708080961 for epoch 111, took 386.1107089519501 sec
Accuracy 0.0034331951202796244 for epoch 111, took 80.00610542297363 sec
Epoch 112
step 25929, loss 1.683569312095642
step 25939, loss 1.4068695306777954
step 25949, loss 1.4036149978637695
step 25959, loss 1.6244211196899414
step 25969, loss 1.8845043182373047
step 25979, loss 1.6972370147705078
step 25989, loss 1.6545192003250122
step 25999, loss 1.6608848571777344
step 26009, loss 1.5825402736663818
step 26019, loss 1.6276501417160034
step 26029, loss 1.6568715572357178
step 26039, loss 1.5803723335266113
step 26049, loss 1.6057922840118408
step 26059, loss 1.6279877424240112
step 26069, loss 1.6446011066436768
step 26079, loss 1.6809067726135254
step 26089, loss 1.6125048398971558
step 26099, loss 1.585789680480957
step 26109, loss 1.5994820594787598
step 26119, loss 1.6141808032989502
step 26129, loss 1.5866169929504395
step 26139, loss 1.6446937322616577
step 26149, loss 1.5994441509246826
Average loss 1.6068526407082875 for epoch 112, took 385.98705673217773 sec
Accuracy 0.0034331951202796244 for epoch 112, took 80.03863573074341 sec
Epoch 113
step 26159, loss 1.430381178855896
step 26169, loss 1.4180632829666138
step 26179, loss 1.677257776260376
step 26189, loss 1.8151310682296753
step 26199, loss 1.5971287488937378
step 26209, loss 1.6058762073516846
step 26219, loss 1.615157127380371
step 26229, loss 1.6408092975616455
step 26239, loss 1.5597057342529297
step 26249, loss 1.6728674173355103
step 26259, loss 1.588942289352417
step 26269, loss 1.610166311264038
step 26279, loss 1.606533408164978
step 26289, loss 1.6156657934188843
step 26299, loss 1.5551787614822388
step 26309, loss 1.601723313331604
step 26319, loss 1.5403485298156738
step 26329, loss 1.550858974456787
step 26339, loss 1.5634181499481201
step 26349, loss 1.7010674476623535
step 26359, loss 1.594558596611023
step 26369, loss 1.6156179904937744
step 26379, loss 1.6095941066741943
Average loss 1.6048279713120377 for epoch 113, took 386.26529335975647 sec
Accuracy 0.0034331951202796244 for epoch 113, took 80.14235877990723 sec
Epoch 114
step 26389, loss 1.3477717638015747
step 26399, loss 1.4158375263214111
step 26409, loss 1.5648504495620728
step 26419, loss 2.1839470863342285
step 26429, loss 1.6992093324661255
step 26439, loss 1.6694507598876953
step 26449, loss 1.6728605031967163
step 26459, loss 1.5716428756713867
step 26469, loss 1.6309030055999756
step 26479, loss 1.6088285446166992
step 26489, loss 1.6137487888336182
step 26499, loss 1.5963317155838013
step 26509, loss 1.6382896900177002
step 26519, loss 1.598052740097046
step 26529, loss 1.6440165042877197
step 26539, loss 1.60952889919281
step 26549, loss 1.579127550125122
step 26559, loss 1.6310404539108276
step 26569, loss 1.614623785018921
step 26579, loss 1.6364564895629883
step 26589, loss 1.5697476863861084
step 26599, loss 1.6336584091186523
step 26609, loss 1.5965876579284668
Average loss 1.6043163679148023 for epoch 114, took 386.3309426307678 sec
Accuracy 0.0034331951202796244 for epoch 114, took 80.14748287200928 sec
Epoch 115
step 26619, loss 1.372767686843872
step 26629, loss 1.393513560295105
step 26639, loss 1.6784026622772217
step 26649, loss 1.8539265394210815
step 26659, loss 1.7156107425689697
step 26669, loss 1.7031259536743164
step 26679, loss 1.6987138986587524
step 26689, loss 1.6257715225219727
step 26699, loss 1.5922000408172607
step 26709, loss 1.5486953258514404
step 26719, loss 1.6308012008666992
step 26729, loss 1.5646772384643555
step 26739, loss 1.597102165222168
step 26749, loss 1.6281614303588867
step 26759, loss 1.5827689170837402
step 26769, loss 1.626153588294983
step 26779, loss 1.5873773097991943
step 26789, loss 1.6278395652770996
step 26799, loss 1.6080865859985352
step 26809, loss 1.6278748512268066
step 26819, loss 1.619542121887207
step 26829, loss 1.6219139099121094
step 26839, loss 1.5316368341445923
Average loss 1.6136556241595954 for epoch 115, took 386.1393246650696 sec
Accuracy 0.0034331951202796244 for epoch 115, took 80.379469871521 sec
Epoch 116
step 26849, loss 1.4026095867156982
step 26859, loss 1.4009543657302856
step 26869, loss 1.5995688438415527
step 26879, loss 1.7432746887207031
step 26889, loss 1.7137596607208252
step 26899, loss 1.6167826652526855
step 26909, loss 1.617236614227295
step 26919, loss 1.5779552459716797
step 26929, loss 1.633026361465454
step 26939, loss 1.6324270963668823
step 26949, loss 1.6774070262908936
step 26959, loss 1.654423713684082
step 26969, loss 1.6418037414550781
step 26979, loss 1.655238389968872
step 26989, loss 1.5932962894439697
step 26999, loss 1.6407493352890015
step 27009, loss 1.6185122728347778
step 27019, loss 1.6330182552337646
step 27029, loss 1.621647834777832
step 27039, loss 1.6037588119506836
step 27049, loss 1.6126128435134888
step 27059, loss 1.6724262237548828
Average loss 1.6096001926221346 for epoch 116, took 386.1802382469177 sec
Accuracy 0.0030765968747858267 for epoch 116, took 80.00115633010864 sec
Epoch 117
step 27069, loss 1.5181236267089844
step 27079, loss 1.3149619102478027
step 27089, loss 1.3861024379730225
step 27099, loss 1.7937380075454712
step 27109, loss 1.9823451042175293
step 27119, loss 1.6801660060882568
step 27129, loss 1.6773488521575928
step 27139, loss 1.579352855682373
step 27149, loss 1.6171315908432007
step 27159, loss 1.580133318901062
step 27169, loss 1.6052700281143188
step 27179, loss 1.6658029556274414
step 27189, loss 1.6452720165252686
step 27199, loss 1.5938010215759277
step 27209, loss 1.646261215209961
step 27219, loss 1.654360294342041
step 27229, loss 1.5779892206192017
step 27239, loss 1.5937285423278809
step 27249, loss 1.549147129058838
step 27259, loss 1.6119903326034546
step 27269, loss 1.592174768447876
step 27279, loss 1.6244359016418457
step 27289, loss 1.5894804000854492
Average loss 1.6057388714531011 for epoch 117, took 386.1154019832611 sec
Accuracy 0.0034331951202796244 for epoch 117, took 80.21230125427246 sec
Epoch 118
step 27299, loss 1.508982539176941
step 27309, loss 1.3943607807159424
step 27319, loss 1.4984946250915527
step 27329, loss 2.0445685386657715
step 27339, loss 1.717937707901001
step 27349, loss 1.701482892036438
step 27359, loss 1.7029911279678345
step 27369, loss 1.5829278230667114
step 27379, loss 1.5440369844436646
step 27389, loss 1.5515508651733398
step 27399, loss 1.5917773246765137
step 27409, loss 1.6183602809906006
step 27419, loss 1.6319456100463867
step 27429, loss 1.6768577098846436
step 27439, loss 1.5939581394195557
step 27449, loss 1.5735714435577393
step 27459, loss 1.572960376739502
step 27469, loss 1.6537847518920898
step 27479, loss 1.6220530271530151
step 27489, loss 1.6003968715667725
step 27499, loss 1.596152424812317
step 27509, loss 1.595304250717163
step 27519, loss 1.6976158618927002
Average loss 1.6178951493480749 for epoch 118, took 386.1558926105499 sec
Accuracy 0.0030765968747858267 for epoch 118, took 80.21789336204529 sec
Epoch 119
step 27529, loss 1.37318754196167
step 27539, loss 1.4065593481063843
step 27549, loss 1.559563398361206
step 27559, loss 1.8940212726593018
step 27569, loss 1.5716649293899536
step 27579, loss 1.6645712852478027
step 27589, loss 1.6178388595581055
step 27599, loss 1.6564054489135742
step 27609, loss 1.6862523555755615
step 27619, loss 1.5939536094665527
step 27629, loss 1.6202030181884766
step 27639, loss 1.5748881101608276
step 27649, loss 1.6176626682281494
step 27659, loss 1.5680863857269287
step 27669, loss 1.6090548038482666
step 27679, loss 1.6890385150909424
step 27689, loss 1.6253397464752197
step 27699, loss 1.6256898641586304
step 27709, loss 1.6005833148956299
step 27719, loss 1.6372385025024414
step 27729, loss 1.6467533111572266
step 27739, loss 1.631088376045227
step 27749, loss 1.597611427307129
Average loss 1.598892037805758 for epoch 119, took 386.04203057289124 sec
Accuracy 0.0034331951202796244 for epoch 119, took 80.30653047561646 sec
Epoch 120
step 27759, loss 1.404298186302185
step 27769, loss 1.3798089027404785
step 27779, loss 1.5101287364959717
step 27789, loss 1.705204963684082
step 27799, loss 1.6106452941894531
step 27809, loss 1.6068718433380127
step 27819, loss 1.6130733489990234
step 27829, loss 1.5647978782653809
step 27839, loss 1.6610835790634155
step 27849, loss 1.6425178050994873
step 27859, loss 1.6207375526428223
step 27869, loss 1.6023955345153809
step 27879, loss 1.603622317314148
step 27889, loss 1.621265172958374
step 27899, loss 1.5826976299285889
step 27909, loss 1.5531208515167236
step 27919, loss 1.6235108375549316
step 27929, loss 1.6010410785675049
step 27939, loss 1.598618984222412
step 27949, loss 1.5670737028121948
step 27959, loss 1.6573216915130615
step 27969, loss 1.599195122718811
step 27979, loss 1.663213849067688
Average loss 1.6074110018579584 for epoch 120, took 386.0718615055084 sec
Accuracy 0.003337888081694195 for epoch 120, took 80.26871228218079 sec
Epoch 121
step 27989, loss 1.464425802230835
step 27999, loss 1.4343552589416504
step 28009, loss 1.8048491477966309
step 28019, loss 1.7591338157653809
step 28029, loss 1.7008572816848755
step 28039, loss 1.776982069015503
step 28049, loss 1.647325873374939
step 28059, loss 1.6044790744781494
step 28069, loss 1.617765188217163
step 28079, loss 1.6091477870941162
step 28089, loss 1.693497896194458
step 28099, loss 1.658846378326416
step 28109, loss 1.5815608501434326
step 28119, loss 1.6233325004577637
step 28129, loss 1.6742727756500244
step 28139, loss 1.5789742469787598
step 28149, loss 1.6298139095306396
step 28159, loss 1.595170259475708
step 28169, loss 1.6210750341415405
step 28179, loss 1.6044228076934814
step 28189, loss 1.5536677837371826
step 28199, loss 1.6304736137390137
Average loss 1.607377354513135 for epoch 121, took 386.0280885696411 sec
Accuracy 0.0034331951202796244 for epoch 121, took 80.06417655944824 sec
Epoch 122
step 28209, loss 1.6807677745819092
step 28219, loss 1.4364957809448242
step 28229, loss 1.340740442276001
step 28239, loss 1.3950471878051758
step 28249, loss 1.6563878059387207
step 28259, loss 1.7558648586273193
step 28269, loss 1.5949339866638184
step 28279, loss 1.6187978982925415
step 28289, loss 1.5605649948120117
step 28299, loss 1.6807507276535034
step 28309, loss 1.601845622062683
step 28319, loss 1.5957715511322021
step 28329, loss 1.6746699810028076
step 28339, loss 1.6241644620895386
step 28349, loss 1.6092751026153564
step 28359, loss 1.6107368469238281
step 28369, loss 1.592389702796936
step 28379, loss 1.6461230516433716
step 28389, loss 1.5805184841156006
step 28399, loss 1.5694694519042969
step 28409, loss 1.6100305318832397
step 28419, loss 1.6457959413528442
step 28429, loss 1.6408860683441162
Average loss 1.6182401237780588 for epoch 122, took 385.77337169647217 sec
Accuracy 0.0034331951202796244 for epoch 122, took 80.35400414466858 sec
Epoch 123
step 28439, loss 1.4797731637954712
step 28449, loss 1.3164241313934326
step 28459, loss 1.493351936340332
step 28469, loss 2.2510814666748047
step 28479, loss 1.6083414554595947
step 28489, loss 1.6062734127044678
step 28499, loss 1.6151297092437744
step 28509, loss 1.605409026145935
step 28519, loss 1.6064802408218384
step 28529, loss 1.6029610633850098
step 28539, loss 1.6234872341156006
step 28549, loss 1.6090682744979858
step 28559, loss 1.639923334121704
step 28569, loss 1.5894076824188232
step 28579, loss 1.589357852935791
step 28589, loss 1.5474474430084229
step 28599, loss 1.5798777341842651
step 28609, loss 1.6337881088256836
step 28619, loss 1.602707028388977
step 28629, loss 1.7102633714675903
step 28639, loss 1.6382689476013184
step 28649, loss 1.631182312965393
step 28659, loss 1.5935304164886475
Average loss 1.615293732860632 for epoch 123, took 381.5212256908417 sec
Accuracy 0.0034331951202796244 for epoch 123, took 80.20280504226685 sec
Epoch 124
step 28669, loss 1.359391212463379
step 28679, loss 1.3957750797271729
step 28689, loss 1.5092377662658691
step 28699, loss 1.7518919706344604
step 28709, loss 1.754598617553711
step 28719, loss 1.6393903493881226
step 28729, loss 1.6242091655731201
step 28739, loss 1.6212489604949951
step 28749, loss 1.594534158706665
step 28759, loss 1.6260768175125122
step 28769, loss 1.583209753036499
step 28779, loss 1.6154389381408691
step 28789, loss 1.597396969795227
step 28799, loss 1.6408107280731201
step 28809, loss 1.6261653900146484
step 28819, loss 1.6501729488372803
step 28829, loss 1.6553900241851807
step 28839, loss 1.6026854515075684
step 28849, loss 1.644338846206665
step 28859, loss 1.6036815643310547
step 28869, loss 1.6324368715286255
step 28879, loss 1.5623207092285156
step 28889, loss 1.6395455598831177
Average loss 1.60850884726173 for epoch 124, took 381.39421129226685 sec
Accuracy 0.0034331951202796244 for epoch 124, took 80.16977763175964 sec
Epoch 125
step 28899, loss 1.3713458776474
step 28909, loss 1.392411231994629
step 28919, loss 1.5477516651153564
step 28929, loss 1.908482551574707
step 28939, loss 1.6375412940979004
step 28949, loss 1.5732358694076538
step 28959, loss 1.7789933681488037
step 28969, loss 1.6202516555786133
step 28979, loss 1.5841035842895508
step 28989, loss 1.6115350723266602
step 28999, loss 1.6536704301834106
step 29009, loss 1.5602993965148926
step 29019, loss 1.6174468994140625
step 29029, loss 1.6198985576629639
step 29039, loss 1.6594736576080322
step 29049, loss 1.592860221862793
step 29059, loss 1.6169377565383911
step 29069, loss 1.6136999130249023
step 29079, loss 1.6716067790985107
step 29089, loss 1.709844708442688
step 29099, loss 1.5719175338745117
step 29109, loss 1.6170188188552856
step 29119, loss 1.6070159673690796
Average loss 1.6137811708868595 for epoch 125, took 381.48266339302063 sec
Accuracy 0.0034331951202796244 for epoch 125, took 80.28510737419128 sec
Epoch 126
step 29129, loss 1.4489402770996094
step 29139, loss 1.5303082466125488
step 29149, loss 1.671885371208191
step 29159, loss 1.5945152044296265
step 29169, loss 1.635219931602478
step 29179, loss 1.662989616394043
step 29189, loss 1.5650506019592285
step 29199, loss 1.6091500520706177
step 29209, loss 1.6330088376998901
step 29219, loss 1.546227216720581
step 29229, loss 1.594867467880249
step 29239, loss 1.5838215351104736
step 29249, loss 1.6470346450805664
step 29259, loss 1.5565599203109741
step 29269, loss 1.5761959552764893
step 29279, loss 1.6690982580184937
step 29289, loss 1.5697517395019531
step 29299, loss 1.5998629331588745
step 29309, loss 1.7204108238220215
step 29319, loss 1.6468085050582886
step 29329, loss 1.5906527042388916
step 29339, loss 1.4993443489074707
Average loss 1.6132636943407226 for epoch 126, took 381.6006050109863 sec
Accuracy 0.0034331951202796244 for epoch 126, took 80.30403590202332 sec
Epoch 127
step 29349, loss 1.7125122547149658
step 29359, loss 1.4535441398620605
step 29369, loss 1.485184669494629
step 29379, loss 1.784511685371399
step 29389, loss 1.7999053001403809
step 29399, loss 1.7136521339416504
step 29409, loss 1.714119553565979
step 29419, loss 1.6511346101760864
step 29429, loss 1.6353853940963745
step 29439, loss 1.5509130954742432
step 29449, loss 1.5808316469192505
step 29459, loss 1.6031713485717773
step 29469, loss 1.6693204641342163
step 29479, loss 1.55059814453125
step 29489, loss 1.606498122215271
step 29499, loss 1.595651626586914
step 29509, loss 1.7082270383834839
step 29519, loss 1.6227744817733765
step 29529, loss 1.5952826738357544
step 29539, loss 1.646923303604126
step 29549, loss 1.5777060985565186
step 29559, loss 1.677215576171875
step 29569, loss 1.6057240962982178
Average loss 1.6135895837817276 for epoch 127, took 381.4129972457886 sec
Accuracy 0.0034331951202796244 for epoch 127, took 80.29143381118774 sec
Epoch 128
step 29579, loss 1.491658091545105
step 29589, loss 1.404999852180481
step 29599, loss 1.5449309349060059
step 29609, loss 1.8483939170837402
step 29619, loss 1.6471601724624634
step 29629, loss 1.727601170539856
step 29639, loss 1.6013388633728027
step 29649, loss 1.6189857721328735
step 29659, loss 1.6643840074539185
step 29669, loss 1.6343607902526855
step 29679, loss 1.6702723503112793
step 29689, loss 1.5893945693969727
step 29699, loss 1.6233854293823242
step 29709, loss 1.5904760360717773
step 29719, loss 1.5850200653076172
step 29729, loss 1.6178607940673828
step 29739, loss 1.647883415222168
step 29749, loss 1.548917531967163
step 29759, loss 1.6002109050750732
step 29769, loss 1.6704272031784058
step 29779, loss 1.5632507801055908
step 29789, loss 1.6374907493591309
step 29799, loss 1.5662356615066528
Average loss 1.610786167153141 for epoch 128, took 381.5260331630707 sec
Accuracy 0.0034331951202796244 for epoch 128, took 80.22183585166931 sec
Epoch 129
step 29809, loss 1.387131690979004
step 29819, loss 1.4133415222167969
step 29829, loss 1.4757579565048218
step 29839, loss 2.1205861568450928
step 29849, loss 1.5564789772033691
step 29859, loss 1.634228229522705
step 29869, loss 1.6109392642974854
step 29879, loss 1.6409738063812256
step 29889, loss 1.5475614070892334
step 29899, loss 1.6389710903167725
step 29909, loss 1.5955345630645752
step 29919, loss 1.6324366331100464
step 29929, loss 1.5994913578033447
step 29939, loss 1.6066315174102783
step 29949, loss 1.548965573310852
step 29959, loss 1.57326078414917
step 29969, loss 1.5496199131011963
step 29979, loss 1.6098215579986572
step 29989, loss 1.5604045391082764
step 29999, loss 1.5842666625976562
step 30009, loss 1.6118534803390503
step 30019, loss 1.6306884288787842
step 30029, loss 1.6123595237731934
Average loss 1.602898190941727 for epoch 129, took 381.50207328796387 sec
Accuracy 0.0034331951202796244 for epoch 129, took 80.24068880081177 sec
Epoch 130
step 30039, loss 1.486220121383667
step 30049, loss 1.4645991325378418
step 30059, loss 1.6503127813339233
step 30069, loss 2.1240427494049072
step 30079, loss 1.6659917831420898
step 30089, loss 1.6171842813491821
step 30099, loss 1.6112849712371826
step 30109, loss 1.651888132095337
step 30119, loss 1.6183604001998901
step 30129, loss 1.60329270362854
step 30139, loss 1.6004045009613037
step 30149, loss 1.611812710762024
step 30159, loss 1.6200577020645142
step 30169, loss 1.6071689128875732
step 30179, loss 1.641958236694336
step 30189, loss 1.6335372924804688
step 30199, loss 1.6062448024749756
step 30209, loss 1.5777037143707275
step 30219, loss 1.5827182531356812
step 30229, loss 1.6183092594146729
step 30239, loss 1.63145112991333
step 30249, loss 1.5346585512161255
step 30259, loss 1.6334547996520996
Average loss 1.6057857008356797 for epoch 130, took 381.5272762775421 sec
Accuracy 0.0034021400178192037 for epoch 130, took 80.0722725391388 sec
Epoch 131
step 30269, loss 1.228989601135254
step 30279, loss 1.4684618711471558
step 30289, loss 1.5824178457260132
step 30299, loss 1.660050392150879
step 30309, loss 1.639957308769226
step 30319, loss 1.6011539697647095
step 30329, loss 1.5805076360702515
step 30339, loss 1.742600440979004
step 30349, loss 1.6337034702301025
step 30359, loss 1.5916069746017456
step 30369, loss 1.5814839601516724
step 30379, loss 1.5062413215637207
step 30389, loss 1.6320958137512207
step 30399, loss 1.5724046230316162
step 30409, loss 1.5561046600341797
step 30419, loss 1.6228561401367188
step 30429, loss 1.6299371719360352
step 30439, loss 1.646304726600647
step 30449, loss 1.6572556495666504
step 30459, loss 1.6007194519042969
step 30469, loss 1.570504903793335
step 30479, loss 1.6203076839447021
Average loss 1.599891277781704 for epoch 131, took 381.55443263053894 sec
Accuracy 0.0034331951202796244 for epoch 131, took 80.15488386154175 sec
Epoch 132
step 30489, loss 1.6921226978302002
step 30499, loss 1.5172004699707031
step 30509, loss 1.3861823081970215
step 30519, loss 1.7860203981399536
step 30529, loss 1.9548876285552979
step 30539, loss 1.7745628356933594
step 30549, loss 1.6645092964172363
step 30559, loss 1.6769458055496216
step 30569, loss 1.6872329711914062
step 30579, loss 1.610836386680603
step 30589, loss 1.5788750648498535
step 30599, loss 1.567967176437378
step 30609, loss 1.6062040328979492
step 30619, loss 1.6127336025238037
step 30629, loss 1.529965877532959
step 30639, loss 1.628232717514038
step 30649, loss 1.6353933811187744
step 30659, loss 1.6558713912963867
step 30669, loss 1.6169023513793945
step 30679, loss 1.5699142217636108
step 30689, loss 1.6626462936401367
step 30699, loss 1.61592698097229
step 30709, loss 1.619210958480835
Average loss 1.6212023235203927 for epoch 132, took 384.4106388092041 sec
Accuracy 0.0030765968747858267 for epoch 132, took 80.28515625 sec
Epoch 133
step 30719, loss 1.3907251358032227
step 30729, loss 1.3329007625579834
step 30739, loss 1.3897017240524292
step 30749, loss 1.6083933115005493
step 30759, loss 1.6129932403564453
step 30769, loss 1.5896861553192139
step 30779, loss 1.625893235206604
step 30789, loss 1.6259040832519531
step 30799, loss 1.6275213956832886
step 30809, loss 1.6116487979888916
step 30819, loss 1.7098623514175415
step 30829, loss 1.6148567199707031
step 30839, loss 1.552435278892517
step 30849, loss 1.544679880142212
step 30859, loss 1.5679614543914795
step 30869, loss 1.6874899864196777
step 30879, loss 1.5927588939666748
step 30889, loss 1.6561808586120605
step 30899, loss 1.6224899291992188
step 30909, loss 1.6275722980499268
step 30919, loss 1.537017822265625
step 30929, loss 1.5970258712768555
step 30939, loss 1.5608699321746826
Average loss 1.6046012885737837 for epoch 133, took 386.03235387802124 sec
Accuracy 0.0034331951202796244 for epoch 133, took 80.2754635810852 sec
Epoch 134
step 30949, loss 1.4240827560424805
step 30959, loss 1.3954894542694092
step 30969, loss 1.3839730024337769
step 30979, loss 1.5560120344161987
step 30989, loss 1.5933445692062378
step 30999, loss 1.6780917644500732
step 31009, loss 1.6236951351165771
step 31019, loss 1.5995571613311768
step 31029, loss 1.6023294925689697
step 31039, loss 1.5902080535888672
step 31049, loss 1.6266727447509766
step 31059, loss 1.6518774032592773
step 31069, loss 1.6216061115264893
step 31079, loss 1.6257840394973755
step 31089, loss 1.6640982627868652
step 31099, loss 1.6780433654785156
step 31109, loss 1.6313831806182861
step 31119, loss 1.6126163005828857
step 31129, loss 1.568277359008789
step 31139, loss 1.6190457344055176
step 31149, loss 1.5426506996154785
step 31159, loss 1.6065404415130615
step 31169, loss 1.6691828966140747
Average loss 1.6110663257147138 for epoch 134, took 385.987263917923 sec
Accuracy 0.0034331951202796244 for epoch 134, took 80.13330960273743 sec
Epoch 135
step 31179, loss 1.4786790609359741
step 31189, loss 1.4358094930648804
step 31199, loss 1.4145032167434692
step 31209, loss 1.625265121459961
step 31219, loss 1.609487771987915
step 31229, loss 1.649202585220337
step 31239, loss 1.630476951599121
step 31249, loss 1.5916261672973633
step 31259, loss 1.6114470958709717
step 31269, loss 1.6680958271026611
step 31279, loss 1.5829631090164185
step 31289, loss 1.53536856174469
step 31299, loss 1.6293178796768188
step 31309, loss 1.5880730152130127
step 31319, loss 1.6253910064697266
step 31329, loss 1.6031652688980103
step 31339, loss 1.6082074642181396
step 31349, loss 1.5745434761047363
step 31359, loss 1.6164271831512451
step 31369, loss 1.584594964981079
step 31379, loss 1.6044244766235352
step 31389, loss 1.5851651430130005
step 31399, loss 1.593192458152771
Average loss 1.6041189034779866 for epoch 135, took 386.00912952423096 sec
Accuracy 0.0034331951202796244 for epoch 135, took 80.22244930267334 sec
Epoch 136
step 31409, loss 1.4133663177490234
step 31419, loss 1.4014604091644287
step 31429, loss 1.5960886478424072
step 31439, loss 1.6646498441696167
step 31449, loss 1.6939451694488525
step 31459, loss 1.6284289360046387
step 31469, loss 1.6577470302581787
step 31479, loss 1.6407084465026855
step 31489, loss 1.6007912158966064
step 31499, loss 1.5642000436782837
step 31509, loss 1.6097028255462646
step 31519, loss 1.6083756685256958
step 31529, loss 1.586505651473999
step 31539, loss 1.6175143718719482
step 31549, loss 1.6098575592041016
step 31559, loss 1.5471577644348145
step 31569, loss 1.585669755935669
step 31579, loss 1.5720531940460205
step 31589, loss 1.5544602870941162
step 31599, loss 1.5593191385269165
step 31609, loss 1.6055052280426025
step 31619, loss 1.6044529676437378
Average loss 1.607948914431689 for epoch 136, took 386.0703101158142 sec
Accuracy 0.003337888081694195 for epoch 136, took 80.15535593032837 sec
Epoch 137
step 31629, loss 1.502469539642334
step 31639, loss 1.3642981052398682
step 31649, loss 1.3936578035354614
step 31659, loss 1.6525167226791382
step 31669, loss 1.824414610862732
step 31679, loss 1.607774257659912
step 31689, loss 1.6089866161346436
step 31699, loss 1.6708486080169678
step 31709, loss 1.7201433181762695
step 31719, loss 1.5954487323760986
step 31729, loss 1.573655128479004
step 31739, loss 1.6854143142700195
step 31749, loss 1.5743645429611206
step 31759, loss 1.5888068675994873
step 31769, loss 1.5640168190002441
step 31779, loss 1.607728362083435
step 31789, loss 1.6368706226348877
step 31799, loss 1.581636905670166
step 31809, loss 1.6010762453079224
step 31819, loss 1.5803821086883545
step 31829, loss 1.6077189445495605
step 31839, loss 1.5608657598495483
step 31849, loss 1.5793465375900269
Average loss 1.6154735569368328 for epoch 137, took 386.09986996650696 sec
Accuracy 0.0034021400178192037 for epoch 137, took 80.21673703193665 sec
Epoch 138
step 31859, loss 1.4922890663146973
step 31869, loss 1.43576979637146
step 31879, loss 1.38999605178833
step 31889, loss 2.1802926063537598
step 31899, loss 1.6287342309951782
step 31909, loss 1.628081202507019
step 31919, loss 1.61260187625885
step 31929, loss 1.5896515846252441
step 31939, loss 1.594257116317749
step 31949, loss 1.59324312210083
step 31959, loss 1.5928640365600586
step 31969, loss 1.6329479217529297
step 31979, loss 1.6046936511993408
step 31989, loss 1.6247079372406006
step 31999, loss 1.648878574371338
step 32009, loss 1.5635472536087036
step 32019, loss 1.6306370496749878
step 32029, loss 1.5928293466567993
step 32039, loss 1.5327274799346924
step 32049, loss 1.5936634540557861
step 32059, loss 1.6050865650177002
step 32069, loss 1.5546966791152954
step 32079, loss 1.5954921245574951
Average loss 1.6000022009799355 for epoch 138, took 386.0743877887726 sec
Accuracy 0.0034331951202796244 for epoch 138, took 80.34787273406982 sec
Epoch 139
step 32089, loss 1.3638211488723755
step 32099, loss 1.3950831890106201
step 32109, loss 1.7125556468963623
step 32119, loss 1.842353105545044
step 32129, loss 1.5835809707641602
step 32139, loss 1.6331326961517334
step 32149, loss 1.6017320156097412
step 32159, loss 1.5702612400054932
step 32169, loss 1.5937621593475342
step 32179, loss 1.5960302352905273
step 32189, loss 1.5387555360794067
step 32199, loss 1.6077699661254883
step 32209, loss 1.643520712852478
step 32219, loss 1.6006269454956055
step 32229, loss 1.608130931854248
step 32239, loss 1.6203184127807617
step 32249, loss 1.662123203277588
step 32259, loss 1.6415401697158813
step 32269, loss 1.61806058883667
step 32279, loss 1.5767195224761963
step 32289, loss 1.5923316478729248
step 32299, loss 1.6108653545379639
step 32309, loss 1.5742435455322266
Average loss 1.605827280826736 for epoch 139, took 386.0361096858978 sec
Accuracy 0.0030765968747858267 for epoch 139, took 80.41012024879456 sec
Epoch 140
step 32319, loss 1.434043288230896
step 32329, loss 1.3767406940460205
step 32339, loss 1.5892741680145264
step 32349, loss 1.7706775665283203
step 32359, loss 1.6297755241394043
step 32369, loss 1.657029151916504
step 32379, loss 1.6432996988296509
step 32389, loss 1.5967731475830078
step 32399, loss 1.5885634422302246
step 32409, loss 1.6220641136169434
step 32419, loss 1.6053651571273804
step 32429, loss 1.642198920249939
step 32439, loss 1.6248595714569092
step 32449, loss 1.6070067882537842
step 32459, loss 1.6043481826782227
step 32469, loss 1.5842219591140747
step 32479, loss 1.7029272317886353
step 32489, loss 1.6112587451934814
step 32499, loss 1.6102845668792725
step 32509, loss 1.6427710056304932
step 32519, loss 1.571936845779419
step 32529, loss 1.5771435499191284
step 32539, loss 1.650346040725708
Average loss 1.6138690014680226 for epoch 140, took 386.2176733016968 sec
Accuracy 0.0034021400178192037 for epoch 140, took 80.04105401039124 sec
Epoch 141
step 32549, loss 1.3265365362167358
step 32559, loss 1.382672905921936
step 32569, loss 1.8383970260620117
step 32579, loss 1.890043020248413
step 32589, loss 1.692595362663269
step 32599, loss 1.7061558961868286
step 32609, loss 1.6069324016571045
step 32619, loss 1.5555522441864014
step 32629, loss 1.629664421081543
step 32639, loss 1.6314845085144043
step 32649, loss 1.5922610759735107
step 32659, loss 1.5838347673416138
step 32669, loss 1.611561894416809
step 32679, loss 1.6349420547485352
step 32689, loss 1.6859707832336426
step 32699, loss 1.5534923076629639
step 32709, loss 1.6500942707061768
step 32719, loss 1.714364767074585
step 32729, loss 1.5337129831314087
step 32739, loss 1.615759253501892
step 32749, loss 1.619910478591919
step 32759, loss 1.7262873649597168
Average loss 1.601851415738725 for epoch 141, took 386.04914951324463 sec
Accuracy 0.0034021400178192037 for epoch 141, took 80.0592770576477 sec
Epoch 142
step 32769, loss 1.5337326526641846
step 32779, loss 1.3862507343292236
step 32789, loss 1.373941421508789
step 32799, loss 1.7882864475250244
step 32809, loss 1.8885002136230469
step 32819, loss 1.6201236248016357
step 32829, loss 1.5917609930038452
step 32839, loss 1.5952160358428955
step 32849, loss 1.6477668285369873
step 32859, loss 1.6225459575653076
step 32869, loss 1.5731070041656494
step 32879, loss 1.6107416152954102
step 32889, loss 1.4890241622924805
step 32899, loss 1.6041439771652222
step 32909, loss 1.6153616905212402
step 32919, loss 1.622119665145874
step 32929, loss 1.5858469009399414
step 32939, loss 1.579472303390503
step 32949, loss 1.6210273504257202
step 32959, loss 1.6224596500396729
step 32969, loss 1.634892463684082
step 32979, loss 1.6266582012176514
step 32989, loss 1.5969297885894775
Average loss 1.6057505989283847 for epoch 142, took 386.2284619808197 sec
Accuracy 0.0034331951202796244 for epoch 142, took 80.23744940757751 sec
Epoch 143
step 32999, loss 1.457895278930664
step 33009, loss 1.4186954498291016
step 33019, loss 1.4893161058425903
step 33029, loss 1.8628729581832886
step 33039, loss 1.6029590368270874
step 33049, loss 1.6018335819244385
step 33059, loss 1.633906602859497
step 33069, loss 1.616671085357666
step 33079, loss 1.6118152141571045
step 33089, loss 1.6109528541564941
step 33099, loss 1.590489149093628
step 33109, loss 1.604393720626831
step 33119, loss 1.590531587600708
step 33129, loss 1.6046136617660522
step 33139, loss 1.5791685581207275
step 33149, loss 1.5965408086776733
step 33159, loss 1.5725860595703125
step 33169, loss 1.5448131561279297
step 33179, loss 1.661428689956665
step 33189, loss 1.5600627660751343
step 33199, loss 1.6023383140563965
step 33209, loss 1.650922179222107
step 33219, loss 1.5969747304916382
Average loss 1.6015110648514932 for epoch 143, took 386.1519773006439 sec
Accuracy 0.0034331951202796244 for epoch 143, took 80.36645722389221 sec
Epoch 144
step 33229, loss 1.4170241355895996
step 33239, loss 1.3646039962768555
step 33249, loss 1.4880468845367432
step 33259, loss 2.313999652862549
step 33269, loss 1.6604658365249634
step 33279, loss 1.7423222064971924
step 33289, loss 1.6511733531951904
step 33299, loss 1.6160459518432617
step 33309, loss 1.6470305919647217
step 33319, loss 1.613389492034912
step 33329, loss 1.6108574867248535
step 33339, loss 1.606165885925293
step 33349, loss 1.6165227890014648
step 33359, loss 1.6174571514129639
step 33369, loss 1.6292991638183594
step 33379, loss 1.6513924598693848
step 33389, loss 1.656097173690796
step 33399, loss 1.6100386381149292
step 33409, loss 1.5742789506912231
step 33419, loss 1.6309685707092285
step 33429, loss 1.6621593236923218
step 33439, loss 1.5698845386505127
step 33449, loss 1.6044034957885742
Average loss 1.6105597343361169 for epoch 144, took 386.11995029449463 sec
Accuracy 0.003337888081694195 for epoch 144, took 79.86678147315979 sec
Epoch 145
step 33459, loss 1.3420047760009766
step 33469, loss 1.4599599838256836
step 33479, loss 1.7285288572311401
step 33489, loss 1.8252570629119873
step 33499, loss 1.8336628675460815
step 33509, loss 1.7226853370666504
step 33519, loss 1.7034118175506592
step 33529, loss 1.7100036144256592
step 33539, loss 1.6187127828598022
step 33549, loss 1.6296075582504272
step 33559, loss 1.6068679094314575
step 33569, loss 1.607865810394287
step 33579, loss 1.5965871810913086
step 33589, loss 1.5812220573425293
step 33599, loss 1.6175811290740967
step 33609, loss 1.602689504623413
step 33619, loss 1.6382758617401123
step 33629, loss 1.579479455947876
step 33639, loss 1.537083625793457
step 33649, loss 1.6543567180633545
step 33659, loss 1.6131210327148438
step 33669, loss 1.6162654161453247
step 33679, loss 1.621203899383545
Average loss 1.5997474810533356 for epoch 145, took 386.15594840049744 sec
Accuracy 0.0034331951202796244 for epoch 145, took 80.04150152206421 sec
Epoch 146
step 33689, loss 1.79783296585083
step 33699, loss 1.3504728078842163
step 33709, loss 1.9613336324691772
step 33719, loss 1.6208347082138062
step 33729, loss 1.6044572591781616
step 33739, loss 1.650017261505127
step 33749, loss 1.59381103515625
step 33759, loss 1.5494375228881836
step 33769, loss 1.6003260612487793
step 33779, loss 1.7157307863235474
step 33789, loss 1.6152623891830444
step 33799, loss 1.6441102027893066
step 33809, loss 1.591774344444275
step 33819, loss 1.6330645084381104
step 33829, loss 1.624079704284668
step 33839, loss 1.613168716430664
step 33849, loss 1.7089500427246094
step 33859, loss 1.6408238410949707
step 33869, loss 1.6074318885803223
step 33879, loss 1.5474728345870972
step 33889, loss 1.613218069076538
step 33899, loss 1.622987985610962
Average loss 1.6066724422730898 for epoch 146, took 385.9558424949646 sec
Accuracy 0.0034331951202796244 for epoch 146, took 80.05739068984985 sec
Epoch 147
step 33909, loss 1.7020683288574219
step 33919, loss 1.4319318532943726
step 33929, loss 1.4585068225860596
step 33939, loss 1.725430965423584
step 33949, loss 1.854104995727539
step 33959, loss 1.7136520147323608
step 33969, loss 1.621490478515625
step 33979, loss 1.5941534042358398
step 33989, loss 1.6040310859680176
step 33999, loss 1.5877668857574463
step 34009, loss 1.5774855613708496
step 34019, loss 1.6654331684112549
step 34029, loss 1.6229712963104248
step 34039, loss 1.6152671575546265
step 34049, loss 1.6164345741271973
step 34059, loss 1.618372917175293
step 34069, loss 1.598177433013916
step 34079, loss 1.5832200050354004
step 34089, loss 1.5780178308486938
step 34099, loss 1.6314045190811157
step 34109, loss 1.608474612236023
step 34119, loss 1.6152522563934326
step 34129, loss 1.6189875602722168
Average loss 1.6140313279210476 for epoch 147, took 386.18184518814087 sec
Accuracy 0.0034331951202796244 for epoch 147, took 80.20359802246094 sec
Epoch 148
step 34139, loss 1.36616849899292
step 34149, loss 1.4338250160217285
step 34159, loss 1.683179497718811
step 34169, loss 1.675812005996704
step 34179, loss 1.5830800533294678
step 34189, loss 1.565432071685791
step 34199, loss 1.6213958263397217
step 34209, loss 1.6053985357284546
step 34219, loss 1.565573811531067
step 34229, loss 1.6154705286026
step 34239, loss 1.6704998016357422
step 34249, loss 1.6182513236999512
step 34259, loss 1.554182529449463
step 34269, loss 1.6384499073028564
step 34279, loss 1.6372134685516357
step 34289, loss 1.6241592168807983
step 34299, loss 1.621040940284729
step 34309, loss 1.627238154411316
step 34319, loss 1.6333903074264526
step 34329, loss 1.5970468521118164
step 34339, loss 1.6284074783325195
step 34349, loss 1.786252498626709
step 34359, loss 1.6685270071029663
Average loss 1.6060947634671863 for epoch 148, took 386.1041932106018 sec
Accuracy 0.0034331951202796244 for epoch 148, took 80.00722479820251 sec
Epoch 149
step 34369, loss 1.3844878673553467
step 34379, loss 1.4829330444335938
step 34389, loss 1.4895623922348022
step 34399, loss 1.8369823694229126
step 34409, loss 1.82113778591156
step 34419, loss 1.7053221464157104
step 34429, loss 1.6271841526031494
step 34439, loss 1.524472951889038
step 34449, loss 1.6027135848999023
step 34459, loss 1.6160407066345215
step 34469, loss 1.6017534732818604
step 34479, loss 1.6241172552108765
step 34489, loss 1.554955244064331
step 34499, loss 1.6201071739196777
step 34509, loss 1.6722733974456787
step 34519, loss 1.596818208694458
step 34529, loss 1.5528442859649658
step 34539, loss 1.6231226921081543
step 34549, loss 1.6977512836456299
step 34559, loss 1.5312526226043701
step 34569, loss 1.6480826139450073
step 34579, loss 1.6417601108551025
step 34589, loss 1.6389555931091309
Average loss 1.6367111906670688 for epoch 149, took 386.0770924091339 sec
Accuracy 0.0034331951202796244 for epoch 149, took 80.04538059234619 sec
Epoch 150
step 34599, loss 1.3056434392929077
step 34609, loss 1.5070240497589111
step 34619, loss 1.8906090259552002
step 34629, loss 1.7519646883010864
step 34639, loss 1.6325628757476807
step 34649, loss 1.6028339862823486
step 34659, loss 1.6840126514434814
step 34669, loss 1.6204195022583008
step 34679, loss 1.6079453229904175
step 34689, loss 1.6143138408660889
step 34699, loss 1.673667073249817
step 34709, loss 1.5907679796218872
step 34719, loss 1.5858280658721924
step 34729, loss 1.6145219802856445
step 34739, loss 1.628296971321106
step 34749, loss 1.5713186264038086
step 34759, loss 1.5699620246887207
step 34769, loss 1.551391839981079
step 34779, loss 1.5945074558258057
step 34789, loss 1.568272352218628
step 34799, loss 1.593778371810913
step 34809, loss 1.6123757362365723
step 34819, loss 1.5953211784362793
Average loss 1.6066589758061527 for epoch 150, took 386.1795573234558 sec
Accuracy 0.0034331951202796244 for epoch 150, took 80.13217186927795 sec
Epoch 151
step 34829, loss 1.4501500129699707
step 34839, loss 1.460819959640503
step 34849, loss 1.6254119873046875
step 34859, loss 1.8940156698226929
step 34869, loss 1.6901795864105225
step 34879, loss 1.8100359439849854
step 34889, loss 1.5975815057754517
step 34899, loss 1.5811152458190918
step 34909, loss 1.6164953708648682
step 34919, loss 1.6407732963562012
step 34929, loss 1.6267935037612915
step 34939, loss 1.6826595067977905
step 34949, loss 1.6194294691085815
step 34959, loss 1.6402878761291504
step 34969, loss 1.6479616165161133
step 34979, loss 1.5916835069656372
step 34989, loss 1.6001598834991455
step 34999, loss 1.5842399597167969
step 35009, loss 1.6002814769744873
step 35019, loss 1.6750586032867432
step 35029, loss 1.585414171218872
step 35039, loss 1.594590187072754
Average loss 1.6178554322635919 for epoch 151, took 386.1792719364166 sec
Accuracy 0.0034331951202796244 for epoch 151, took 80.08377408981323 sec
Epoch 152
step 35049, loss 1.6324201822280884
step 35059, loss 1.5564013719558716
step 35069, loss 1.4039862155914307
step 35079, loss 2.0610921382904053
step 35089, loss 1.53104567527771
step 35099, loss 1.5960465669631958
step 35109, loss 1.6079776287078857
step 35119, loss 1.6136279106140137
step 35129, loss 1.6302456855773926
step 35139, loss 1.640047550201416
step 35149, loss 1.6215108633041382
step 35159, loss 1.5806128978729248
step 35169, loss 1.597959280014038
step 35179, loss 1.7103517055511475
step 35189, loss 1.6140334606170654
step 35199, loss 1.5899767875671387
step 35209, loss 1.6154351234436035
step 35219, loss 1.5751277208328247
step 35229, loss 1.6258567571640015
step 35239, loss 1.5713695287704468
step 35249, loss 1.6263108253479004
step 35259, loss 1.6053440570831299
step 35269, loss 1.6044518947601318
Average loss 1.6138611043754376 for epoch 152, took 386.1670298576355 sec
Accuracy 0.0034331951202796244 for epoch 152, took 80.22095227241516 sec
Epoch 153
step 35279, loss 1.4144339561462402
step 35289, loss 1.4043861627578735
step 35299, loss 1.5552434921264648
step 35309, loss 1.773486852645874
step 35319, loss 1.7661653757095337
step 35329, loss 1.5906754732131958
step 35339, loss 1.6015543937683105
step 35349, loss 1.5901871919631958
step 35359, loss 1.5799458026885986
step 35369, loss 1.607181191444397
step 35379, loss 1.6816167831420898
step 35389, loss 1.6049184799194336
step 35399, loss 1.542195200920105
step 35409, loss 1.6708416938781738
step 35419, loss 1.5998384952545166
step 35429, loss 1.617552638053894
step 35439, loss 1.6295127868652344
step 35449, loss 1.6124894618988037
step 35459, loss 1.6299793720245361
step 35469, loss 1.6121357679367065
step 35479, loss 1.6368592977523804
step 35489, loss 1.5435723066329956
step 35499, loss 1.6084119081497192
Average loss 1.6094837507657838 for epoch 153, took 386.0096185207367 sec
Accuracy 0.0034331951202796244 for epoch 153, took 80.10420417785645 sec
Epoch 154
step 35509, loss 1.4260554313659668
step 35519, loss 1.3806993961334229
step 35529, loss 1.6920719146728516
step 35539, loss 1.8531782627105713
step 35549, loss 1.678430199623108
step 35559, loss 1.6625770330429077
step 35569, loss 1.6147089004516602
step 35579, loss 1.5826464891433716
step 35589, loss 1.6388270854949951
step 35599, loss 1.6215393543243408
step 35609, loss 1.5733931064605713
step 35619, loss 1.7194643020629883
step 35629, loss 1.5727698802947998
step 35639, loss 1.6087523698806763
step 35649, loss 1.567647099494934
step 35659, loss 1.6749180555343628
step 35669, loss 1.6505622863769531
step 35679, loss 1.650608777999878
step 35689, loss 1.6184619665145874
step 35699, loss 1.6231480836868286
step 35709, loss 1.6449799537658691
step 35719, loss 1.669318675994873
step 35729, loss 1.5834436416625977
Average loss 1.6276371709087438 for epoch 154, took 386.00570487976074 sec
Accuracy 0.0034331951202796244 for epoch 154, took 80.15605425834656 sec
Epoch 155
step 35739, loss 1.5264793634414673
step 35749, loss 1.4196796417236328
step 35759, loss 1.6882140636444092
step 35769, loss 1.8035058975219727
step 35779, loss 1.6333866119384766
step 35789, loss 1.7274788618087769
step 35799, loss 1.6364355087280273
step 35809, loss 1.616513967514038
step 35819, loss 1.7011022567749023
step 35829, loss 1.6594111919403076
step 35839, loss 1.6754834651947021
step 35849, loss 1.5951204299926758
step 35859, loss 1.5543594360351562
step 35869, loss 1.609607458114624
step 35879, loss 1.5478839874267578
step 35889, loss 1.5802364349365234
step 35899, loss 1.6009944677352905
step 35909, loss 1.5932064056396484
step 35919, loss 1.6257450580596924
step 35929, loss 1.629440426826477
step 35939, loss 1.6051708459854126
step 35949, loss 1.6266710758209229
step 35959, loss 1.5805964469909668
Average loss 1.6281696652111255 for epoch 155, took 386.08808851242065 sec
Accuracy 0.0030765968747858267 for epoch 155, took 80.12940287590027 sec
Epoch 156
step 35969, loss 1.3816356658935547
step 35979, loss 1.3642302751541138
step 35989, loss 1.8569633960723877
step 35999, loss 1.5902912616729736
step 36009, loss 1.7122539281845093
step 36019, loss 1.6668455600738525
step 36029, loss 1.6769893169403076
step 36039, loss 1.6078412532806396
step 36049, loss 1.6346572637557983
step 36059, loss 1.6016870737075806
step 36069, loss 1.5912117958068848
step 36079, loss 1.5851532220840454
step 36089, loss 1.6257448196411133
step 36099, loss 1.5938137769699097
step 36109, loss 1.6269174814224243
step 36119, loss 1.6365630626678467
step 36129, loss 1.6240164041519165
step 36139, loss 1.6755564212799072
step 36149, loss 1.632649302482605
step 36159, loss 1.561617136001587
step 36169, loss 1.5899769067764282
step 36179, loss 1.6015143394470215
Average loss 1.6026651937710612 for epoch 156, took 386.0202980041504 sec
Accuracy 0.0030765968747858267 for epoch 156, took 80.28534507751465 sec
Epoch 157
step 36189, loss 1.5242364406585693
step 36199, loss 1.3857057094573975
step 36209, loss 1.367699146270752
step 36219, loss 1.5941247940063477
step 36229, loss 1.7376315593719482
step 36239, loss 1.7302861213684082
step 36249, loss 1.713484764099121
step 36259, loss 1.5873562097549438
step 36269, loss 1.582818627357483
step 36279, loss 1.6196401119232178
step 36289, loss 1.595834493637085
step 36299, loss 1.7072575092315674
step 36309, loss 1.5728445053100586
step 36319, loss 1.6063857078552246
step 36329, loss 1.6164543628692627
step 36339, loss 1.6225868463516235
step 36349, loss 1.6155951023101807
step 36359, loss 1.6227811574935913
step 36369, loss 1.5257245302200317
step 36379, loss 1.6910312175750732
step 36389, loss 1.599320650100708
step 36399, loss 1.5601450204849243
step 36409, loss 1.6151351928710938
Average loss 1.6027037187626487 for epoch 157, took 385.9503183364868 sec
Accuracy 0.0034331951202796244 for epoch 157, took 80.03080010414124 sec
Epoch 158
step 36419, loss 1.4717999696731567
step 36429, loss 1.4037981033325195
step 36439, loss 1.498291254043579
step 36449, loss 2.083251476287842
step 36459, loss 1.6575123071670532
step 36469, loss 1.6132557392120361
step 36479, loss 1.6856805086135864
step 36489, loss 1.6880757808685303
step 36499, loss 1.579615831375122
step 36509, loss 1.6620392799377441
step 36519, loss 1.6114132404327393
step 36529, loss 1.595249891281128
step 36539, loss 1.6056016683578491
step 36549, loss 1.6077265739440918
step 36559, loss 1.5833131074905396
step 36569, loss 1.5824439525604248
step 36579, loss 1.6535440683364868
step 36589, loss 1.6599652767181396
step 36599, loss 1.6928074359893799
step 36609, loss 1.6479339599609375
step 36619, loss 1.676399827003479
step 36629, loss 1.6191058158874512
step 36639, loss 1.7097327709197998
Average loss 1.6126251220703125 for epoch 158, took 386.2027862071991 sec
Accuracy 0.0034331951202796244 for epoch 158, took 79.9088671207428 sec
Epoch 159
step 36649, loss 1.4251325130462646
step 36659, loss 1.3859350681304932
step 36669, loss 1.5179206132888794
step 36679, loss 1.6933774948120117
step 36689, loss 1.648632287979126
step 36699, loss 1.6301770210266113
step 36709, loss 1.6850008964538574
step 36719, loss 1.6243350505828857
step 36729, loss 1.58402681350708
step 36739, loss 1.6036757230758667
step 36749, loss 1.557220458984375
step 36759, loss 1.6323580741882324
step 36769, loss 1.4964306354522705
step 36779, loss 1.5912060737609863
step 36789, loss 1.6198363304138184
step 36799, loss 1.6356291770935059
step 36809, loss 1.5695685148239136
step 36819, loss 1.6219961643218994
step 36829, loss 1.6122407913208008
step 36839, loss 1.6232662200927734
step 36849, loss 1.5895087718963623
step 36859, loss 1.6319528818130493
step 36869, loss 1.6116747856140137
Average loss 1.6090534366013711 for epoch 159, took 386.1546595096588 sec
Accuracy 0.0034331951202796244 for epoch 159, took 80.18483877182007 sec
Epoch 160
step 36879, loss 1.3872828483581543
step 36889, loss 1.4083315134048462
step 36899, loss 1.7659164667129517
step 36909, loss 1.5851612091064453
step 36919, loss 1.6461392641067505
step 36929, loss 1.7123969793319702
step 36939, loss 1.610932469367981
step 36949, loss 1.5773886442184448
step 36959, loss 1.6947128772735596
step 36969, loss 1.805717945098877
step 36979, loss 1.6300561428070068
step 36989, loss 1.5864070653915405
step 36999, loss 1.6177513599395752
step 37009, loss 1.603963851928711
step 37019, loss 1.656306266784668
step 37029, loss 1.537644386291504
step 37039, loss 1.6187200546264648
step 37049, loss 1.6350460052490234
step 37059, loss 1.6638662815093994
step 37069, loss 1.617715835571289
step 37079, loss 1.6290754079818726
step 37089, loss 1.6819429397583008
step 37099, loss 1.578635334968567
Average loss 1.6075823312265831 for epoch 160, took 386.60870885849 sec
Accuracy 0.0034331951202796244 for epoch 160, took 80.2937126159668 sec
Epoch 161
step 37109, loss 1.4010372161865234
step 37119, loss 1.413991093635559
step 37129, loss 1.6302337646484375
step 37139, loss 1.8804919719696045
step 37149, loss 1.6696279048919678
step 37159, loss 1.7280724048614502
step 37169, loss 1.608331322669983
step 37179, loss 1.6503877639770508
step 37189, loss 1.5939302444458008
step 37199, loss 1.5852230787277222
step 37209, loss 1.667765498161316
step 37219, loss 1.6005107164382935
step 37229, loss 1.6454315185546875
step 37239, loss 1.6474193334579468
step 37249, loss 1.5596661567687988
step 37259, loss 1.632362723350525
step 37269, loss 1.5409965515136719
step 37279, loss 1.6024627685546875
step 37289, loss 1.605966567993164
step 37299, loss 1.6478146314620972
step 37309, loss 1.6020135879516602
step 37319, loss 1.5659034252166748
Average loss 1.6138959051224224 for epoch 161, took 387.56269788742065 sec
Accuracy 0.0034331951202796244 for epoch 161, took 80.25806760787964 sec
Epoch 162
step 37329, loss 1.7788074016571045
step 37339, loss 1.310683250427246
step 37349, loss 1.365523338317871
step 37359, loss 1.68404221534729
step 37369, loss 1.8320841789245605
step 37379, loss 1.7695199251174927
step 37389, loss 1.617430567741394
step 37399, loss 1.5841678380966187
step 37409, loss 1.5722724199295044
step 37419, loss 1.608098030090332
step 37429, loss 1.6639924049377441
step 37439, loss 1.573124647140503
step 37449, loss 1.5630249977111816
step 37459, loss 1.6084909439086914
step 37469, loss 1.6294432878494263
step 37479, loss 1.6191904544830322
step 37489, loss 1.5600619316101074
step 37499, loss 1.6082158088684082
step 37509, loss 1.6491494178771973
step 37519, loss 1.5925606489181519
step 37529, loss 1.6212561130523682
step 37539, loss 1.6121244430541992
step 37549, loss 1.5878865718841553
Average loss 1.6086268200163256 for epoch 162, took 387.6193742752075 sec
Accuracy 0.0034331951202796244 for epoch 162, took 80.06798434257507 sec
Epoch 163
step 37559, loss 1.4336445331573486
step 37569, loss 1.3354156017303467
step 37579, loss 1.4029133319854736
step 37589, loss 2.0557100772857666
step 37599, loss 1.8393826484680176
step 37609, loss 1.7299635410308838
step 37619, loss 1.6227833032608032
step 37629, loss 1.5833096504211426
step 37639, loss 1.637477159500122
step 37649, loss 1.6340972185134888
step 37659, loss 1.6089513301849365
step 37669, loss 1.5796270370483398
step 37679, loss 1.587214469909668
step 37689, loss 1.6837234497070312
step 37699, loss 1.6311688423156738
step 37709, loss 1.5940189361572266
step 37719, loss 1.6601929664611816
step 37729, loss 1.6337828636169434
step 37739, loss 1.6347858905792236
step 37749, loss 1.5843664407730103
step 37759, loss 1.5992088317871094
step 37769, loss 1.6308586597442627
step 37779, loss 1.6217825412750244
Average loss 1.6106240848700206 for epoch 163, took 387.60094356536865 sec
Accuracy 0.0034331951202796244 for epoch 163, took 80.26083970069885 sec
Epoch 164
step 37789, loss 1.4313963651657104
step 37799, loss 1.3996596336364746
step 37809, loss 1.8303805589675903
step 37819, loss 1.9359170198440552
step 37829, loss 1.6154946088790894
step 37839, loss 1.6783522367477417
step 37849, loss 1.6376445293426514
step 37859, loss 1.6999468803405762
step 37869, loss 1.65118408203125
step 37879, loss 1.6268346309661865
step 37889, loss 1.624658226966858
step 37899, loss 1.618868350982666
step 37909, loss 1.701585054397583
step 37919, loss 1.605054259300232
step 37929, loss 1.5919773578643799
step 37939, loss 1.6113523244857788
step 37949, loss 1.5638794898986816
step 37959, loss 1.5735417604446411
step 37969, loss 1.6284030675888062
step 37979, loss 1.6151546239852905
step 37989, loss 1.644094705581665
step 37999, loss 1.618626356124878
step 38009, loss 1.6152900457382202
Average loss 1.61165956446999 for epoch 164, took 387.5794026851654 sec
Accuracy 0.0034021400178192037 for epoch 164, took 80.18972182273865 sec
Epoch 165
step 38019, loss 1.396188735961914
step 38029, loss 1.389112949371338
step 38039, loss 1.838240146636963
step 38049, loss 1.9285576343536377
step 38059, loss 1.6495449542999268
step 38069, loss 1.6625643968582153
step 38079, loss 1.632277011871338
step 38089, loss 1.5806024074554443
step 38099, loss 1.6697678565979004
step 38109, loss 1.5594360828399658
step 38119, loss 1.5834087133407593
step 38129, loss 1.5462439060211182
step 38139, loss 1.5833911895751953
step 38149, loss 1.5998961925506592
step 38159, loss 1.607572078704834
step 38169, loss 1.61537504196167
step 38179, loss 1.6178133487701416
step 38189, loss 1.5929949283599854
step 38199, loss 1.6142230033874512
step 38209, loss 1.621985912322998
step 38219, loss 1.6142067909240723
step 38229, loss 1.615556240081787
step 38239, loss 1.6586670875549316
Average loss 1.6022530114441587 for epoch 165, took 387.6100115776062 sec
Accuracy 0.0034331951202796244 for epoch 165, took 80.0647177696228 sec
Epoch 166
step 38249, loss 1.3300862312316895
step 38259, loss 1.383013129234314
step 38269, loss 2.1455745697021484
step 38279, loss 1.7810581922531128
step 38289, loss 1.8190689086914062
step 38299, loss 1.6883230209350586
step 38309, loss 1.629497766494751
step 38319, loss 1.6185917854309082
step 38329, loss 1.6062233448028564
step 38339, loss 1.550245761871338
step 38349, loss 1.5814270973205566
step 38359, loss 1.6009728908538818
step 38369, loss 1.6177997589111328
step 38379, loss 1.698285460472107
step 38389, loss 1.6186407804489136
step 38399, loss 1.601738452911377
step 38409, loss 1.615415334701538
step 38419, loss 1.5727968215942383
step 38429, loss 1.6029243469238281
step 38439, loss 1.5896364450454712
step 38449, loss 1.6212773323059082
step 38459, loss 1.5946266651153564
Average loss 1.6095731347276454 for epoch 166, took 387.5679306983948 sec
Accuracy 0.0034331951202796244 for epoch 166, took 80.19878387451172 sec
Epoch 167
step 38469, loss 1.6910314559936523
step 38479, loss 1.5102238655090332
step 38489, loss 1.4044506549835205
step 38499, loss 2.068209409713745
step 38509, loss 1.7284884452819824
step 38519, loss 1.6914628744125366
step 38529, loss 1.5908608436584473
step 38539, loss 1.5970183610916138
step 38549, loss 1.6347827911376953
step 38559, loss 1.6953035593032837
step 38569, loss 1.6105468273162842
step 38579, loss 1.562572717666626
step 38589, loss 1.5587501525878906
step 38599, loss 1.6127623319625854
step 38609, loss 1.638916254043579
step 38619, loss 1.6245354413986206
step 38629, loss 1.6069875955581665
step 38639, loss 1.6099145412445068
step 38649, loss 1.6088166236877441
step 38659, loss 1.5959742069244385
step 38669, loss 1.545536994934082
step 38679, loss 1.5725159645080566
step 38689, loss 1.65529465675354
Average loss 1.610661662461465 for epoch 167, took 387.6142466068268 sec
Accuracy 0.0034331951202796244 for epoch 167, took 80.1900999546051 sec
Epoch 168
step 38699, loss 1.5537350177764893
step 38709, loss 1.4160540103912354
step 38719, loss 1.4686553478240967
step 38729, loss 2.0737130641937256
step 38739, loss 2.0886104106903076
step 38749, loss 1.8714711666107178
step 38759, loss 1.6266727447509766
step 38769, loss 1.6056475639343262
step 38779, loss 1.5778131484985352
step 38789, loss 1.6217707395553589
step 38799, loss 1.6302871704101562
step 38809, loss 1.6380740404129028
step 38819, loss 1.6186549663543701
step 38829, loss 1.5890344381332397
step 38839, loss 1.6415300369262695
step 38849, loss 1.5897783041000366
step 38859, loss 1.5927627086639404
step 38869, loss 1.6020773649215698
step 38879, loss 1.6069235801696777
step 38889, loss 1.5661896467208862
step 38899, loss 1.569103717803955
step 38909, loss 1.6142501831054688
step 38919, loss 1.5937224626541138
Average loss 1.6279690710076116 for epoch 168, took 387.7094600200653 sec
Accuracy 0.0034021400178192037 for epoch 168, took 80.15017867088318 sec
Epoch 169
step 38929, loss 1.409116268157959
step 38939, loss 1.3754419088363647
step 38949, loss 1.585024356842041
step 38959, loss 1.774846076965332
step 38969, loss 1.7144172191619873
step 38979, loss 1.5974122285842896
step 38989, loss 1.6663626432418823
step 38999, loss 1.5800771713256836
step 39009, loss 1.6395783424377441
step 39019, loss 1.6471972465515137
step 39029, loss 1.6803860664367676
step 39039, loss 1.6419308185577393
step 39049, loss 1.6433539390563965
step 39059, loss 1.5898703336715698
step 39069, loss 1.640566110610962
step 39079, loss 1.5757511854171753
step 39089, loss 1.5328645706176758
step 39099, loss 1.6075007915496826
step 39109, loss 1.6339704990386963
step 39119, loss 1.5786570310592651
step 39129, loss 1.5634111166000366
step 39139, loss 1.651439905166626
step 39149, loss 1.6389554738998413
Average loss 1.6059400052355046 for epoch 169, took 387.60063767433167 sec
Accuracy 0.0034331951202796244 for epoch 169, took 80.30401349067688 sec
Epoch 170
step 39159, loss 1.3130507469177246
step 39169, loss 1.376233696937561
step 39179, loss 1.5877981185913086
step 39189, loss 1.9142436981201172
step 39199, loss 1.7424964904785156
step 39209, loss 1.6165306568145752
step 39219, loss 1.6553685665130615
step 39229, loss 1.6045842170715332
step 39239, loss 1.603516936302185
step 39249, loss 1.6316770315170288
step 39259, loss 1.5965139865875244
step 39269, loss 1.5642483234405518
step 39279, loss 1.6762490272521973
step 39289, loss 1.6538126468658447
step 39299, loss 1.598489761352539
step 39309, loss 1.6361727714538574
step 39319, loss 1.6426113843917847
step 39329, loss 1.5842925310134888
step 39339, loss 1.5854815244674683
step 39349, loss 1.6076278686523438
step 39359, loss 1.626299262046814
step 39369, loss 1.6005889177322388
step 39379, loss 1.6150587797164917
Average loss 1.6114483904420285 for epoch 170, took 387.6158902645111 sec
Accuracy 0.0030765968747858267 for epoch 170, took 79.9984917640686 sec
Epoch 171
step 39389, loss 1.4419652223587036
step 39399, loss 1.4398956298828125
step 39409, loss 1.7395888566970825