{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import os\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data_and_labels_from_many_files(data_folder, data_files):\n",
    "    \"\"\"\n",
    "    Loads sentences from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    # Load data from files\n",
    "    print(\"Loading data...\")\n",
    "    x_text = []\n",
    "    y = []\n",
    "\n",
    "    for i, data_file in enumerate(data_files):\n",
    "\n",
    "        sentences = list(open(data_folder + \"/\" + data_file, \"r\").readlines())\n",
    "        sentences = [s.strip() for s in sentences]\n",
    "        # Split by words\n",
    "        # sentences = [clean_str(s) for s in sentences]\n",
    "        sentences = [s.split() for s in sentences]\n",
    "        x_text += sentences\n",
    "        # Labels as numbers\n",
    "        labels = [i for s in sentences]\n",
    "        y += labels\n",
    "\n",
    "    # Generate one-hot labels\n",
    "    y = to_categorical(y, num_classes=len(data_files))\n",
    "\n",
    "    return x_text, y\n",
    "\n",
    "def load_data_and_labels_from_one_file(data_folder, data_file):\n",
    "    \"\"\"\n",
    "    Loads sentences from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    # Load data from files\n",
    "    print(\"Loading data...\")\n",
    "    x_text = []\n",
    "    y = []\n",
    "\n",
    "    lines = list(open(data_folder + \"/\" + data_file, \"r\").readlines())\n",
    "    print(lines)\n",
    "    print(l.split(\"\\t\") for l in lines)\n",
    "    sentences, labels = [l.split(\"\\t\") for l in lines]\n",
    "    print(sentences)\n",
    "    print(labels)\n",
    "    # Split by words\n",
    "    # sentences = [clean_str(s) for s in sentences]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    x_text += sentences\n",
    "    # Labels as numbers\n",
    "    y += labels\n",
    "\n",
    "    # Generate one-hot labels\n",
    "    y = to_categorical(y, num_classes=len(set(y)))\n",
    "\n",
    "    return x_text, y\n",
    "\n",
    "\n",
    "def pad_sentences(sentences, padding_word=\"\"):\n",
    "    \"\"\"\n",
    "    Pads all sentences to be the length of the longest sentence.\n",
    "    Returns padded sentences.\n",
    "    \"\"\"\n",
    "    print(\"Padding sentences...\")\n",
    "    sequence_length = max(len(x) for x in sentences)\n",
    "    padded_sentences = []\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        num_padding = sequence_length - len(sentence)\n",
    "        new_sentence = sentence + [padding_word] * num_padding\n",
    "        padded_sentences.append(new_sentence)\n",
    "        \n",
    "    return padded_sentences\n",
    "\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary mapping from token to index based on the sentences.\n",
    "    Returns vocabulary mapping and inverse vocabulary mapping.\n",
    "    \"\"\"\n",
    "    # Build vocabulary\n",
    "    print(\"Building word vocabulary...\")\n",
    "    word_counts = Counter(itertools.chain(*sentences))\n",
    "    \n",
    "    # Mapping from index to word\n",
    "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "    \n",
    "    # Mapping from word to index\n",
    "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "    \n",
    "    return vocabulary, vocabulary_inv\n",
    "\n",
    "\n",
    "def build_input_data(sentences, labels, vocabulary):\n",
    "    \"\"\"\n",
    "    Maps sentences and labels to vectors based on a vocabulary.\n",
    "    \"\"\"\n",
    "    print(\"Converting to ids...\")\n",
    "    x = np.array([\n",
    "            [vocabulary[word] for word in sentence]\n",
    "            for sentence in sentences])\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "50\n",
      "['tthdm', 'AlmsAjd', 'fy', \"synA'\", 'wAl>bAt$y', 'sxryp', 'mn', 'Alhjrp', 'Alnbwyp', 'fy', 'Aljrydp', 'Alrsmyp', 'lmA', 'mAdty', 'Altrbyp', 'Al<slAmyp', 'mn', 'AlmdArs']\n"
     ]
    }
   ],
   "source": [
    "data_folder = \"../data/vardial2017-sample\"\n",
    "data_files = [\"EGY\", \"GLF\", \"LAV\", \"MSA\", \"NOR\"]\n",
    "sentences, labels = load_data_and_labels_from_many_files(data_folder, data_files)\n",
    "print(len(sentences))\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "['AlkAmyrwn AlkAmlp\\tGLF\\n', '>ryd >n >sjl b>n AljmyE wAfq ElY h*A AlqrAr lmSlHp AlErAq\\tMSA\\n', \"mA fy$ mA fy$ kfAyp yEny kyf mmkn yEny Alywm >nt $Ayf mwqf AlqDAp wmjls AlqDA' Al>ElY mvlA lA nwAfq ElY Al<$rAf nAdy AlqDAp lw mwqf mtHfZ hnAk <$kAlyp Hwl h*A Aldwr wHwl mwqf AlqDAp Emlyp AlAstftA' ElY Aldstwr\\tGLF\\n\", 'tnTlq vAnwyp\\tGLF\\n', 'mA fy >y wAHd mn wSlwA <lY mrHlp AltElym yEny >kvr w<HnA nwASl AlmrHlp Al<EdAdyp\\tGLF\\n', 'k$f llm$Akl Aldynyp wAlvqAfyp kAml kAn yEy$ AltDArbAt yEny mn nAHyp mn nAHyp mn nAHyp AlxdmAt mn nAHyp HyAthA\\tNOR\\n', 'dktwr Ez Aldyn fy AlqAhrp br>yk mA Al*y syxtlf bynmA kAn Elyh AltEAwn >w AltEATy Al>myrky AlmSry fy Ehd mbArk wmA hw Elyh Al|n mA Al*y sybqY wmA Al*y syxtlf\\tMSA\\n', '<n h*A TbEA\\tGLF\\n', \"fy AlmTAlb Alm$rwEp bAlnsbp lkm lkn rAEwA <n fy nAs brDh m$ EAyz >qwl bywthA ElY w$k >nhA tqf Alty qAlt twqf Aln$AT AlryADy lkn >qwl lhm rEb Al$hdA' m$ HnnsY Ally HSl m$ HnnsY swA' lEybp swA' mjls <dArp swA' jmhwr swA' wkl HAjp lkn ll>ltrAs lkn\\tEGY\\n\", 'yjb >n nErf >n AlSHfyyn fy AlEAlm AlErby AlglAbp\\tLAV\\n']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-87b837747b3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdev_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"dev.words\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Step 1: Read in data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msentences_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data_and_labels_from_one_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0msentences_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data_and_labels_from_one_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences_train\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msentences_dev\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-9d4146ad7dfe>\u001b[0m in \u001b[0;36mload_data_and_labels_from_one_file\u001b[0;34m(data_folder, data_file)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_folder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "data_folder = \"../data/vardial2018-sample\"\n",
    "train_file = \"train.words\"\n",
    "dev_file = \"dev.words\"\n",
    "# Step 1: Read in data\n",
    "sentences_train, labels_train = load_data_and_labels_from_one_file(data_folder, train_file)\n",
    "sentences_dev, labels_dev = load_data_and_labels_from_one_file(data_folder, train_file)\n",
    "sentences = sentences_train + sentences_dev\n",
    "labels = labels_train + labels_dev\n",
    "print(len(sentences))\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
