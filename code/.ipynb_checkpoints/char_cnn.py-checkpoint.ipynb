{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#from tensorflow.python.framework import ops\n",
    "#ops.reset_default_graph()\n",
    "#sess = tf.Session()\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_folder = \"../data/vardial2017-sample\"\n",
    "data_files = [\"EGY\", \"GLF\", \"LAV\", \"MSA\", \"NOR\"]\n",
    "#train_file = \"train.words\"\n",
    "#dev_file = \"dev.words\"\n",
    "batch_size = 64\n",
    "\n",
    "# Model Hyperparameters\n",
    "# Embeddings\n",
    "vocab_size = 50000\n",
    "embed_size = 128\n",
    "\n",
    "# Convolutional layers\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 3\n",
    "dropout_keep_prob = 0.5\n",
    "l2_reg_lambda = 0.0\n",
    "l2_loss = 0.0\n",
    "\n",
    "# Training\n",
    "lr = 0.5\n",
    "training=False\n",
    "n_epochs = 15\n",
    "skip_step = 1\n",
    "n_test = 100\n",
    "global_step = tf.get_variable('global_step', initializer=tf.constant(0), trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Padding sentences...\n",
      "Building word vocabulary...\n",
      "Converting to ids...\n",
      "Train/Dev split: 45/5\n",
      "train shape: (45, 416)\n",
      "dev shape: (5, 416)\n",
      "vocab_size 1480\n",
      "sentence max words 416\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('data'):\n",
    "    train_data, test_data = utils.get_vardial_dataset_from_many_files(batch_size, data_folder, data_files)\n",
    "    #train_data, test_data = utils.get_vardial_dataset_from_train_dev_file(batch_size, data_folder, train_file, dev_file)\n",
    "    iterator = tf.data.Iterator.from_structure(train_data.output_types, \n",
    "                                           train_data.output_shapes)\n",
    "    sentence, label = iterator.get_next()\n",
    "    # shape = [batch_size, sentence_length],[batch_size, num_classes]\n",
    "\n",
    "    sentence_length = sentence.shape[1].value\n",
    "    num_classes = label.shape[1].value\n",
    "\n",
    "    train_init = iterator.make_initializer(train_data)  # initializer for train_data\n",
    "    test_init = iterator.make_initializer(test_data)    # initializer for train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Variable.eval of <tf.Variable 'char_embed/embed_matrix:0' shape=(50000, 128) dtype=float32_ref>>\n",
      "<bound method Tensor.eval of <tf.Tensor 'char_embed/embedding:0' shape=(?, 416, 128) dtype=float32>>\n",
      "<bound method Tensor.eval of <tf.Tensor 'char_embed/ExpandDims:0' shape=(?, 416, 128, 1) dtype=float32>>\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Step 2 + 3: define weights and embedding lookup\n",
    "\"\"\"\n",
    "with tf.variable_scope('char_embed'):\n",
    "    embed_matrix = tf.get_variable('embed_matrix', \n",
    "                                        shape=[vocab_size, embed_size],\n",
    "                                        initializer=tf.random_uniform_initializer())\n",
    "    embedded_chars = tf.nn.embedding_lookup(embed_matrix, sentence, name='embedding')\n",
    "    embedded_chars_expanded = tf.expand_dims(embedded_chars, -1)\n",
    "    \"\"\"sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    sess.run(train_init)\"\"\"\n",
    "    print(embed_matrix.eval)\n",
    "    print(embedded_chars.eval)\n",
    "    print(embedded_chars_expanded.eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Tensor.eval of <tf.Tensor 'conv-maxpool-3/conv/Relu:0' shape=(?, 416, 128, 3) dtype=float32>>\n",
      "<bound method Tensor.eval of <tf.Tensor 'conv-maxpool-3/pool/MaxPool:0' shape=(?, 1, 1, 3) dtype=float32>>\n",
      "<bound method Tensor.eval of <tf.Tensor 'conv-maxpool-4/conv/Relu:0' shape=(?, 416, 128, 3) dtype=float32>>\n",
      "<bound method Tensor.eval of <tf.Tensor 'conv-maxpool-4/pool/MaxPool:0' shape=(?, 1, 1, 3) dtype=float32>>\n",
      "<bound method Tensor.eval of <tf.Tensor 'conv-maxpool-5/conv/Relu:0' shape=(?, 416, 128, 3) dtype=float32>>\n",
      "<bound method Tensor.eval of <tf.Tensor 'conv-maxpool-5/pool/MaxPool:0' shape=(?, 1, 1, 3) dtype=float32>>\n"
     ]
    }
   ],
   "source": [
    "# Create a convolution + maxpool layer for each filter size\n",
    "pooled_outputs = []\n",
    "for i, filter_size in enumerate(filter_sizes):\n",
    "    with tf.variable_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "\n",
    "        # Convolution layer\n",
    "        conv = tf.layers.conv2d(inputs=embedded_chars_expanded,\n",
    "                          filters=num_filters,\n",
    "                          kernel_size=[filter_size, embed_size],\n",
    "                          padding='SAME',\n",
    "                          activation=tf.nn.relu,\n",
    "                          name='conv')\n",
    "        print(conv.eval)\n",
    "        # shape = [batch_size, sequence_length, embed_size, num_filters]\n",
    "        \n",
    "        # Maxpooling over the outputs\n",
    "        pooled = tf.layers.max_pooling2d(inputs=conv, \n",
    "                                        pool_size=(sentence.shape[1].value - filter_size + 1, 1), \n",
    "                                        strides=[filter_size, embed_size],\n",
    "                                        name='pool')\n",
    "        print(pooled.eval)\n",
    "        pooled_outputs.append(pooled)\n",
    "         # shape =  [batch_size, 1, 1, num_filters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "<bound method Tensor.eval of <tf.Tensor 'concat:0' shape=(?, 1, 1, 9) dtype=float32>>\n",
      "<bound method Tensor.eval of <tf.Tensor 'Reshape:0' shape=(?, 9) dtype=float32>>\n"
     ]
    }
   ],
   "source": [
    "num_filters_total = num_filters * len(filter_sizes)\n",
    "print(num_filters_total)\n",
    "h_pool = tf.concat(pooled_outputs, 3)\n",
    "h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])\n",
    "print(h_pool.eval)\n",
    "print(h_pool_flat.eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Tensor.eval of <tf.Tensor 'dropout/dropout/mul:0' shape=(?, 9) dtype=float32>>\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"dropout\"):\n",
    "   h_drop = tf.nn.dropout(h_pool_flat, dropout_keep_prob)\n",
    "print(h_drop.eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Tensor.eval of <tf.Tensor 'fully-connected/dense/BiasAdd:0' shape=(?, 5) dtype=float32>>\n",
      "<bound method Tensor.eval of <tf.Tensor 'fully-connected/predictions:0' shape=(?,) dtype=int64>>\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"fully-connected\"):\n",
    "    scores = tf.layers.dense(h_drop, num_classes,\n",
    "                kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    predictions = tf.argmax(scores, 1, name=\"predictions\")\n",
    "print(scores.eval)\n",
    "print(predictions.eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Tensor.eval of <tf.Tensor 'loss/Reshape_2:0' shape=(?,) dtype=float32>>\n",
      "<bound method Tensor.eval of <tf.Tensor 'loss/add:0' shape=() dtype=float32>>\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('loss'):\n",
    "    losses = tf.nn.softmax_cross_entropy_with_logits(logits=scores, labels=label)\n",
    "    loss = tf.reduce_mean(losses) + l2_reg_lambda * 0.1\n",
    "print(losses.eval)\n",
    "print(loss.eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Tensor.eval of <tf.Tensor 'accuracy/Equal:0' shape=(?,) dtype=bool>>\n",
      "<bound method Tensor.eval of <tf.Tensor 'accuracy/accuracy:0' shape=() dtype=float32>>\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_predictions = tf.equal(predictions, tf.argmax(label, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "print(correct_predictions.eval)\n",
    "print(accuracy.eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('optimizer'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(lr).minimize(loss, \n",
    "                                                      global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('summaries'):\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    tf.summary.histogram('histogram_loss', loss)\n",
    "    summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(sess, saver, init, writer, epoch, step):\n",
    "    start_time = time.time()\n",
    "    sess.run(init) \n",
    "    training = True\n",
    "    total_loss = 0\n",
    "    n_batches = 0\n",
    "    try:\n",
    "        while True:\n",
    "            _, l, summaries = sess.run([optimizer, loss, summary_op])\n",
    "            writer.add_summary(summaries, global_step=step)\n",
    "            if (step + 1) % skip_step == 0:\n",
    "                print('Loss at step {0}: {1}'.format(step, l))\n",
    "            step += 1\n",
    "            total_loss += l\n",
    "            n_batches += 1\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        pass\n",
    "    saver.save(sess, 'checkpoints/char-cnn', step)\n",
    "    print('Average loss at epoch {0}: {1}'.format(epoch, total_loss/n_batches))\n",
    "    print('Took: {0} seconds'.format(time.time() - start_time))\n",
    "    return step\n",
    "\n",
    "def eval_once(sess, init, writer, epoch, step):\n",
    "    start_time = time.time()\n",
    "    sess.run(init)\n",
    "    training = False\n",
    "    total_correct_preds = 0\n",
    "    try:\n",
    "        while True:\n",
    "            accuracy_batch, summaries = sess.run([accuracy, summary_op])\n",
    "            writer.add_summary(summaries, global_step=step)\n",
    "            total_correct_preds += accuracy_batch\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        pass\n",
    "\n",
    "    print('Accuracy at epoch {0}: {1} '.format(epoch, total_correct_preds/n_test))\n",
    "    print('Took: {0} seconds'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at epoch 0: 1.4981067180633545\n",
      "Took: 0.8831539154052734 seconds\n",
      "Accuracy at epoch 0: 4.000000059604645e-05 \n",
      "Took: 0.13344931602478027 seconds\n",
      "Average loss at epoch 1: 4.218474864959717\n",
      "Took: 0.45311450958251953 seconds\n",
      "Accuracy at epoch 1: 1.0000000149011612e-05 \n",
      "Took: 0.119171142578125 seconds\n",
      "Average loss at epoch 2: 2.8485333919525146\n",
      "Took: 0.4656839370727539 seconds\n",
      "Accuracy at epoch 2: 5e-05 \n",
      "Took: 0.13268709182739258 seconds\n",
      "Average loss at epoch 3: 1.532058835029602\n",
      "Took: 0.4485774040222168 seconds\n",
      "Accuracy at epoch 3: 5e-05 \n",
      "Took: 0.1291050910949707 seconds\n",
      "Average loss at epoch 4: 1.4919884204864502\n",
      "Took: 0.4705312252044678 seconds\n",
      "Accuracy at epoch 4: 5e-05 \n",
      "Took: 0.11514019966125488 seconds\n",
      "Average loss at epoch 5: 1.4610817432403564\n",
      "Took: 0.44898414611816406 seconds\n",
      "Accuracy at epoch 5: 5e-05 \n",
      "Took: 0.11464190483093262 seconds\n",
      "Average loss at epoch 6: 1.4374964237213135\n",
      "Took: 0.4296691417694092 seconds\n",
      "Accuracy at epoch 6: 5e-05 \n",
      "Took: 0.11474275588989258 seconds\n",
      "Average loss at epoch 7: 1.4196162223815918\n",
      "Took: 0.449634313583374 seconds\n",
      "Accuracy at epoch 7: 5e-05 \n",
      "Took: 0.13028979301452637 seconds\n",
      "Average loss at epoch 8: 1.4060986042022705\n",
      "Took: 0.44978976249694824 seconds\n",
      "Accuracy at epoch 8: 5e-05 \n",
      "Took: 0.12275862693786621 seconds\n",
      "Average loss at epoch 9: 1.3958743810653687\n",
      "Took: 0.4290635585784912 seconds\n",
      "Accuracy at epoch 9: 5e-05 \n",
      "Took: 0.12174272537231445 seconds\n",
      "Average loss at epoch 10: 1.3881160020828247\n",
      "Took: 0.4304478168487549 seconds\n",
      "Accuracy at epoch 10: 5e-05 \n",
      "Took: 0.1193082332611084 seconds\n",
      "Average loss at epoch 11: 1.3821966648101807\n",
      "Took: 0.499737024307251 seconds\n",
      "Accuracy at epoch 11: 5e-05 \n",
      "Took: 0.11594772338867188 seconds\n",
      "Average loss at epoch 12: 1.3776477575302124\n",
      "Took: 0.43953585624694824 seconds\n",
      "Accuracy at epoch 12: 5e-05 \n",
      "Took: 0.11755871772766113 seconds\n",
      "Average loss at epoch 13: 1.3741233348846436\n",
      "Took: 0.4327430725097656 seconds\n",
      "Accuracy at epoch 13: 5e-05 \n",
      "Took: 0.11529707908630371 seconds\n",
      "Average loss at epoch 14: 1.371367335319519\n",
      "Took: 0.4446372985839844 seconds\n",
      "Accuracy at epoch 14: 5e-05 \n",
      "Took: 0.11979150772094727 seconds\n"
     ]
    }
   ],
   "source": [
    "utils.safe_mkdir('checkpoints')\n",
    "utils.safe_mkdir('checkpoints/char-cnn')\n",
    "writer = tf.summary.FileWriter('./graphs/char-cnn', tf.get_default_graph())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/char-cnn/checkpoint'))\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "    step = global_step.eval()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        step = train_one_epoch(sess, saver, train_init, writer, epoch, step)\n",
    "        eval_once(sess, test_init, writer, epoch, step)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
